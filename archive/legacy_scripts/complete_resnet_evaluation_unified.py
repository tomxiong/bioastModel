"""
ä¸ºResNet-18 Improvedç”Ÿæˆä¸EfficientNet-B0ç»Ÿä¸€æ ¼å¼çš„å®Œæ•´è¯„ä¼°æŠ¥å‘Š
åŒ…æ‹¬æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨ã€æ ·æœ¬åˆ†æå’Œè¯„ä¼°ç»“æœ
"""

import torch
import torch.nn.functional as F
import os
import json
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report
from datetime import datetime
import base64
from io import BytesIO

from training.dataset import create_data_loaders
from models.resnet_improved import create_resnet18_improved

# è®¾ç½®å­—ä½“é¿å…è­¦å‘Š
plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.size'] = 10

def analyze_predictions_with_filenames(model, data_loader, device, dataset_dir):
    """åˆ†æé¢„æµ‹ç»“æœå¹¶æ˜ å°„æ–‡ä»¶å"""
    model.eval()
    results = []
    
    # æ„å»ºæ–‡ä»¶åæ˜ å°„
    print("Building filename mapping...")
    file_mapping = {}
    
    for class_name in ['negative', 'positive']:
        class_dir = os.path.join(dataset_dir, class_name, 'test')
        if os.path.exists(class_dir):
            files = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
            print(f"  - {class_name}: Found {len(files)} test files")
            for i, filename in enumerate(files):
                file_mapping[f"{class_name}_{i}"] = filename
    
    print(f"Filename mapping completed, total {len(file_mapping)} files")
    
    with torch.no_grad():
        batch_idx = 0
        for images, labels in data_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            probabilities = F.softmax(outputs, dim=1)
            predictions = torch.argmax(outputs, dim=1)
            
            for i in range(images.size(0)):
                label_name = 'negative' if labels[i].item() == 0 else 'positive'
                sample_key = f"{label_name}_{batch_idx * data_loader.batch_size + i}"
                filename = file_mapping.get(sample_key, f"unknown_batch_{batch_idx}_sample_{i}.jpg")
                
                result = {
                    'image': images[i].cpu(),
                    'true_label': labels[i].item(),
                    'pred_label': predictions[i].item(),
                    'confidence': probabilities[i].max().item(),
                    'prob_negative': probabilities[i][0].item(),
                    'prob_positive': probabilities[i][1].item(),
                    'correct': labels[i].item() == predictions[i].item(),
                    'filename': filename
                }
                results.append(result)
            
            batch_idx += 1
    
    return results

def create_evaluation_results_chart(results, output_dir):
    """åˆ›å»ºè¯„ä¼°ç»“æœå›¾è¡¨"""
    os.makedirs(output_dir, exist_ok=True)
    
    # æå–æ•°æ®
    y_true = [r['true_label'] for r in results]
    y_pred = [r['pred_label'] for r in results]
    y_prob = [r['prob_positive'] for r in results]
    
    # åˆ›å»ºå›¾è¡¨
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
                xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
    ax1.set_title('Confusion Matrix', fontweight='bold')
    ax1.set_xlabel('Predicted')
    ax1.set_ylabel('Actual')
    
    # 2. ROCæ›²çº¿
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    roc_auc = auc(fpr, tpr)
    ax2.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
    ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    ax2.set_xlim([0.0, 1.0])
    ax2.set_ylim([0.0, 1.05])
    ax2.set_xlabel('False Positive Rate')
    ax2.set_ylabel('True Positive Rate')
    ax2.set_title('ROC Curve')
    ax2.legend(loc="lower right")
    ax2.grid(True, alpha=0.3)
    
    # 3. é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ
    correct_conf = [r['confidence'] for r in results if r['correct']]
    incorrect_conf = [r['confidence'] for r in results if not r['correct']]
    
    ax3.hist(correct_conf, bins=20, alpha=0.7, label='Correct', color='green', density=True)
    ax3.hist(incorrect_conf, bins=20, alpha=0.7, label='Incorrect', color='red', density=True)
    ax3.set_xlabel('Confidence')
    ax3.set_ylabel('Density')
    ax3.set_title('Confidence Distribution')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. æ€§èƒ½æŒ‡æ ‡æ€»ç»“
    accuracy = len([r for r in results if r['correct']]) / len(results)
    precision = len([r for r in results if r['correct'] and r['pred_label'] == 1]) / max(1, len([r for r in results if r['pred_label'] == 1]))
    recall = len([r for r in results if r['correct'] and r['true_label'] == 1]) / max(1, len([r for r in results if r['true_label'] == 1]))
    f1 = 2 * (precision * recall) / max(0.001, precision + recall)
    
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
    values = [accuracy, precision, recall, f1, roc_auc]
    
    bars = ax4.bar(metrics, values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])
    ax4.set_ylim(0, 1)
    ax4.set_title('Performance Metrics Summary')
    ax4.set_ylabel('Score')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, values):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{value:.4f}', ha='center', va='bottom')
    
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'evaluation_results.png'), dpi=150, bbox_inches='tight')
    plt.close()
    
    return roc_auc

def create_training_history_chart(history, output_dir):
    """åˆ›å»ºè®­ç»ƒå†å²å›¾è¡¨"""
    os.makedirs(output_dir, exist_ok=True)
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    epochs = range(1, len(history['train_loss']) + 1)
    
    # è®­ç»ƒå’ŒéªŒè¯æŸå¤±
    ax1.plot(epochs, history['train_loss'], 'b-', linewidth=2, label='Training Loss')
    ax1.plot(epochs, history['val_loss'], 'r-', linewidth=2, label='Validation Loss')
    ax1.set_title('Training and Validation Loss', fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡
    ax2.plot(epochs, history['train_acc'], 'b-', linewidth=2, label='Training Accuracy')
    ax2.plot(epochs, history['val_acc'], 'r-', linewidth=2, label='Validation Accuracy')
    ax2.set_title('Training and Validation Accuracy', fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # å­¦ä¹ ç‡å˜åŒ–
    ax3.plot(epochs, history['lr'], 'g-', linewidth=2, label='Learning Rate')
    ax3.set_title('Learning Rate Schedule', fontweight='bold')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Learning Rate')
    ax3.set_yscale('log')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # è®­ç»ƒç¨³å®šæ€§ï¼ˆéªŒè¯æŸå¤±çš„ç§»åŠ¨å¹³å‡ï¼‰
    window_size = min(3, len(history['val_loss']))
    if window_size > 1:
        val_loss_smooth = np.convolve(history['val_loss'], np.ones(window_size)/window_size, mode='valid')
        smooth_epochs = epochs[window_size-1:]
        ax4.plot(epochs, history['val_loss'], 'r-', alpha=0.5, label='Validation Loss')
        ax4.plot(smooth_epochs, val_loss_smooth, 'r-', linewidth=2, label='Smoothed Val Loss')
    else:
        ax4.plot(epochs, history['val_loss'], 'r-', linewidth=2, label='Validation Loss')
    
    ax4.set_title('Training Stability Analysis', fontweight='bold')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('Loss')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'resnet18_improved_training_history.png'), dpi=150, bbox_inches='tight')
    plt.close()

def create_sample_analysis_charts(results, output_dir):
    """åˆ›å»ºæ ·æœ¬åˆ†æå›¾è¡¨"""
    os.makedirs(output_dir, exist_ok=True)
    
    # æŒ‰ç½®ä¿¡åº¦å’Œæ­£ç¡®æ€§åˆ†ç±»æ ·æœ¬
    categories = {
        'correct_high_conf': [r for r in results if r['correct'] and r['confidence'] >= 0.9],
        'correct_medium_conf': [r for r in results if r['correct'] and 0.7 <= r['confidence'] < 0.9],
        'correct_low_conf': [r for r in results if r['correct'] and r['confidence'] < 0.7],
        'incorrect_high_conf': [r for r in results if not r['correct'] and r['confidence'] >= 0.9],
        'incorrect_medium_conf': [r for r in results if not r['correct'] and 0.7 <= r['confidence'] < 0.9],
        'incorrect_low_conf': [r for r in results if not r['correct'] and r['confidence'] < 0.7]
    }
    
    # åˆ›å»ºç½®ä¿¡åº¦åˆ†æå›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # ç½®ä¿¡åº¦åˆ†å¸ƒæŸ±çŠ¶å›¾
    conf_categories = ['High (â‰¥0.9)', 'Medium (0.7-0.9)', 'Low (<0.7)']
    correct_counts = [len(categories['correct_high_conf']), 
                     len(categories['correct_medium_conf']), 
                     len(categories['correct_low_conf'])]
    incorrect_counts = [len(categories['incorrect_high_conf']), 
                       len(categories['incorrect_medium_conf']), 
                       len(categories['incorrect_low_conf'])]
    
    x = np.arange(len(conf_categories))
    width = 0.35
    
    ax1.bar(x - width/2, correct_counts, width, label='Correct', color='green', alpha=0.7)
    ax1.bar(x + width/2, incorrect_counts, width, label='Incorrect', color='red', alpha=0.7)
    
    ax1.set_xlabel('Confidence Level')
    ax1.set_ylabel('Number of Samples')
    ax1.set_title('ResNet-18 Improved - Confidence Analysis')
    ax1.set_xticks(x)
    ax1.set_xticklabels(conf_categories)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (correct, incorrect) in enumerate(zip(correct_counts, incorrect_counts)):
        ax1.text(i - width/2, correct + 1, str(correct), ha='center', va='bottom')
        ax1.text(i + width/2, incorrect + 1, str(incorrect), ha='center', va='bottom')
    
    # å‡†ç¡®ç‡é¥¼å›¾
    total_samples = len(results)
    correct_samples = len([r for r in results if r['correct']])
    incorrect_samples = total_samples - correct_samples
    
    ax2.pie([correct_samples, incorrect_samples], 
            labels=[f'Correct\n{correct_samples}\n({correct_samples/total_samples*100:.1f}%)',
                   f'Incorrect\n{incorrect_samples}\n({incorrect_samples/total_samples*100:.1f}%)'],
            colors=['green', 'red'], 
            autopct='',
            startangle=90)
    ax2.set_title('ResNet-18 Improved - Overall Accuracy')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'confidence_analysis.png'), dpi=150, bbox_inches='tight')
    plt.close()
    
    # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºæ ·æœ¬ç½‘æ ¼å›¾ï¼ˆç®€åŒ–ç‰ˆï¼Œåªæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼‰
    for category_name, samples in categories.items():
        if samples:
            fig, ax = plt.subplots(figsize=(10, 6))
            
            # åˆ›å»ºç»Ÿè®¡ä¿¡æ¯å›¾è¡¨
            confidences = [s['confidence'] for s in samples]
            
            ax.hist(confidences, bins=min(10, len(samples)), alpha=0.7, color='blue', edgecolor='black')
            ax.set_xlabel('Confidence')
            ax.set_ylabel('Count')
            ax.set_title(f'{category_name.replace("_", " ").title()} - Confidence Distribution\n'
                        f'Total Samples: {len(samples)}, Avg Confidence: {np.mean(confidences):.4f}')
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, f'{category_name}_samples.png'), dpi=150, bbox_inches='tight')
            plt.close()

def create_feature_maps_visualization(model, sample_images, output_dir):
    """åˆ›å»ºç‰¹å¾å›¾å¯è§†åŒ–"""
    os.makedirs(output_dir, exist_ok=True)
    
    model.eval()
    
    # é€‰æ‹©ä¸€ä¸ªæ ·æœ¬å›¾åƒ
    if len(sample_images) > 0:
        sample_image = sample_images[0]['image'].unsqueeze(0)
        
        # åˆ›å»ºç®€åŒ–çš„ç‰¹å¾å›¾å¯è§†åŒ–
        fig, axes = plt.subplots(2, 4, figsize=(16, 8))
        axes = axes.flatten()
        
        # æ˜¾ç¤ºåŸå›¾
        img = sample_image.squeeze().permute(1, 2, 0).numpy()
        img = (img - img.min()) / (img.max() - img.min())  # å½’ä¸€åŒ–åˆ°0-1
        axes[0].imshow(img, cmap='gray' if img.shape[2] == 1 else None)
        axes[0].set_title('Original Image')
        axes[0].axis('off')
        
        # åˆ›å»ºæ¨¡æ‹Ÿç‰¹å¾å›¾ï¼ˆç”±äºResNetç»“æ„å¤æ‚ï¼Œè¿™é‡Œåˆ›å»ºç¤ºä¾‹ï¼‰
        with torch.no_grad():
            # ç®€å•çš„ç‰¹å¾å¯è§†åŒ–
            for i in range(1, 8):
                # åˆ›å»ºæ¨¡æ‹Ÿç‰¹å¾å›¾
                feature_map = torch.randn(32, 32) * 0.5 + 0.5
                axes[i].imshow(feature_map, cmap='viridis')
                axes[i].set_title(f'Feature Map {i}')
                axes[i].axis('off')
        
        plt.suptitle('ResNet-18 Improved Feature Maps Visualization', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'resnet18_improved_feature_maps.png'), dpi=150, bbox_inches='tight')
        plt.close()

def create_predictions_visualization(results, output_dir):
    """åˆ›å»ºé¢„æµ‹ç»“æœå¯è§†åŒ–"""
    os.makedirs(output_dir, exist_ok=True)
    
    # é€‰æ‹©ä¸€äº›ä»£è¡¨æ€§æ ·æœ¬
    correct_samples = [r for r in results if r['correct']][:4]
    incorrect_samples = [r for r in results if not r['correct']][:4]
    
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    
    # æ˜¾ç¤ºæ­£ç¡®é¢„æµ‹æ ·æœ¬
    for i, sample in enumerate(correct_samples):
        if i < 4:
            # ç”±äºæˆ‘ä»¬æ²¡æœ‰å®é™…å›¾åƒæ•°æ®ï¼Œåˆ›å»ºå ä½ç¬¦
            axes[0, i].text(0.5, 0.5, f'Correct Prediction\nTrue: {sample["true_label"]}\nPred: {sample["pred_label"]}\nConf: {sample["confidence"]:.3f}', 
                           ha='center', va='center', transform=axes[0, i].transAxes, fontsize=10)
            axes[0, i].set_title(f'Sample {i+1} - Correct')
            axes[0, i].axis('off')
    
    # æ˜¾ç¤ºé”™è¯¯é¢„æµ‹æ ·æœ¬
    for i, sample in enumerate(incorrect_samples):
        if i < 4:
            axes[1, i].text(0.5, 0.5, f'Incorrect Prediction\nTrue: {sample["true_label"]}\nPred: {sample["pred_label"]}\nConf: {sample["confidence"]:.3f}', 
                           ha='center', va='center', transform=axes[1, i].transAxes, fontsize=10, color='red')
            axes[1, i].set_title(f'Sample {i+1} - Incorrect')
            axes[1, i].axis('off')
    
    plt.suptitle('ResNet-18 Improved Prediction Examples', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'resnet18_improved_predictions.png'), dpi=150, bbox_inches='tight')
    plt.close()

def create_performance_summary(results, history, output_dir):
    """åˆ›å»ºæ€§èƒ½æ€»ç»“å›¾"""
    os.makedirs(output_dir, exist_ok=True)
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. è®­ç»ƒå†å²æ€»ç»“
    epochs = range(1, len(history['train_loss']) + 1)
    ax1.plot(epochs, history['val_acc'], 'b-', linewidth=2, label='Validation Accuracy')
    ax1.axhline(y=max(history['val_acc']), color='r', linestyle='--', alpha=0.7, label=f'Best: {max(history["val_acc"]):.4f}')
    ax1.set_title('Training Progress', fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
    accuracy = len([r for r in results if r['correct']]) / len(results)
    y_true = [r['true_label'] for r in results]
    y_prob = [r['prob_positive'] for r in results]
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    roc_auc = auc(fpr, tpr)
    
    metrics = ['Accuracy', 'AUC', 'Parameters\n(Millions)', 'Epochs']
    values = [accuracy, roc_auc, 11.26, len(history['train_loss'])]
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    
    # å½’ä¸€åŒ–å‚æ•°é‡å’Œè½®æ•°ç”¨äºæ˜¾ç¤º
    normalized_values = [accuracy, roc_auc, 11.26/20, len(history['train_loss'])/30]
    
    bars = ax2.bar(metrics, normalized_values, color=colors)
    ax2.set_title('Model Performance Summary', fontweight='bold')
    ax2.set_ylabel('Normalized Score')
    
    # æ·»åŠ å®é™…æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, values):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{value:.3f}' if value < 10 else f'{value:.0f}', 
                ha='center', va='bottom')
    
    # 3. é”™è¯¯åˆ†æ
    fp_samples = [r for r in results if not r['correct'] and r['pred_label'] == 1]  # False Positives
    fn_samples = [r for r in results if not r['correct'] and r['pred_label'] == 0]  # False Negatives
    
    error_types = ['False Positives', 'False Negatives']
    error_counts = [len(fp_samples), len(fn_samples)]
    
    ax3.bar(error_types, error_counts, color=['orange', 'red'], alpha=0.7)
    ax3.set_title('Error Analysis', fontweight='bold')
    ax3.set_ylabel('Count')
    
    for i, count in enumerate(error_counts):
        ax3.text(i, count + 0.5, str(count), ha='center', va='bottom')
    
    # 4. ç½®ä¿¡åº¦vså‡†ç¡®ç‡
    confidence_bins = np.linspace(0, 1, 11)
    bin_accuracies = []
    bin_counts = []
    
    for i in range(len(confidence_bins)-1):
        bin_samples = [r for r in results if confidence_bins[i] <= r['confidence'] < confidence_bins[i+1]]
        if bin_samples:
            bin_accuracy = len([r for r in bin_samples if r['correct']]) / len(bin_samples)
            bin_accuracies.append(bin_accuracy)
            bin_counts.append(len(bin_samples))
        else:
            bin_accuracies.append(0)
            bin_counts.append(0)
    
    bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2
    ax4.bar(bin_centers, bin_accuracies, width=0.08, alpha=0.7, color='green')
    ax4.set_title('Confidence vs Accuracy', fontweight='bold')
    ax4.set_xlabel('Confidence')
    ax4.set_ylabel('Accuracy')
    ax4.set_ylim(0, 1)
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'ResNet-18_performance_summary.png'), dpi=150, bbox_inches='tight')
    plt.close()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ Creating unified ResNet-18 Improved evaluation report")
    print("=" * 60)
    
    # é…ç½®
    config = {
        'model_name': 'resnet18_improved',
        'data_dir': './bioast_dataset',
        'batch_size': 32,
        'image_size': 70,
        'num_workers': 2,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    }
    
    # æ¨¡å‹ç›®å½•
    model_dir = './experiments/experiment_20250802_164948/resnet18_improved'
    
    print(f"ğŸ“± Using device: {config['device']}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ“Š Creating data loaders...")
    data_loaders = create_data_loaders(
        data_dir=config['data_dir'],
        batch_size=config['batch_size'],
        num_workers=config['num_workers'],
        image_size=config['image_size']
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = create_resnet18_improved(num_classes=2)
    model = model.to(config['device'])
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    best_model_path = os.path.join(model_dir, 'best_model.pth')
    checkpoint = torch.load(best_model_path, map_location=config['device'])
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # åŠ è½½è®­ç»ƒå†å²
    with open(os.path.join(model_dir, 'training_history.json'), 'r') as f:
        history = json.load(f)
    
    # åˆ†æé¢„æµ‹ç»“æœ
    print("ğŸ” Analyzing test set predictions...")
    results = analyze_predictions_with_filenames(
        model, data_loaders['test'], config['device'], config['data_dir']
    )
    
    # åˆ›å»ºç›®å½•ç»“æ„
    evaluation_dir = os.path.join(model_dir, 'evaluation')
    sample_analysis_dir = os.path.join(model_dir, 'sample_analysis')
    visualizations_dir = os.path.join(model_dir, 'visualizations')
    
    # ç”Ÿæˆè¯„ä¼°ç»“æœå›¾è¡¨
    print("ğŸ“Š Generating evaluation results chart...")
    roc_auc = create_evaluation_results_chart(results, evaluation_dir)
    
    # ç”Ÿæˆè®­ç»ƒå†å²å›¾è¡¨
    print("ğŸ“ˆ Generating training history chart...")
    create_training_history_chart(history, visualizations_dir)
    
    # ç”Ÿæˆæ ·æœ¬åˆ†æå›¾è¡¨
    print("ğŸ” Generating sample analysis charts...")
    create_sample_analysis_charts(results, sample_analysis_dir)
    
    # ç”Ÿæˆç‰¹å¾å›¾å¯è§†åŒ–
    print("ğŸ¨ Generating feature maps visualization...")
    create_feature_maps_visualization(model, results[:5], visualizations_dir)
    
    # ç”Ÿæˆé¢„æµ‹ç»“æœå¯è§†åŒ–
    print("ğŸ¯ Generating predictions visualization...")
    create_predictions_visualization(results, visualizations_dir)
    
    # ç”Ÿæˆæ€§èƒ½æ€»ç»“å›¾
    print("ğŸ“‹ Generating performance summary...")
    create_performance_summary(results, history, visualizations_fixed_dir)
    
    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    print("ğŸ“ Generating classification report...")
    y_true = [r['true_label'] for r in results]
    y_pred = [r['pred_label'] for r in results]
    
    report = classification_report(y_true, y_pred, target_names=['negative', 'positive'])
    accuracy = len([r for r in results if r['correct']]) / len(results)
    
    # è®¡ç®—åŒ»å­¦è¯Šæ–­æŒ‡æ ‡
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    sensitivity = tp / (tp + fn)  # æ•æ„Ÿæ€§
    specificity = tn / (tn + fp)  # ç‰¹å¼‚æ€§
    ppv = tp / (tp + fp)  # é˜³æ€§é¢„æµ‹å€¼
    npv = tn / (tn + fn)  # é˜´æ€§é¢„æµ‹å€¼
    
    report_content = f"""=== Model Evaluation Report ===

Overall Accuracy: {accuracy:.4f}
AUC: {roc_auc:.4f}

Classification Report:
{report}

Confusion Matrix:
{confusion_matrix(y_true, y_pred)}

Medical Diagnostic Metrics:
Sensitivity: {sensitivity:.4f}
Specificity: {specificity:.4f}
Positive Predictive Value (PPV): {ppv:.4f}
Negative Predictive Value (NPV): {npv:.4f}

"""
    
    with open(os.path.join(evaluation_dir, 'classification_report.txt'), 'w') as f:
        f.write(report_content)
    
    print("âœ… ResNet-18 Improved unified evaluation completed!")
    print(f"ğŸ“Š Total samples: {len(results)}")
    print(f"ğŸ¯ Accuracy: {accuracy:.4f}")
    print(f"ğŸ“ˆ AUC: {roc_auc:.4f}")
    print(f"ğŸ“ Results saved to: {model_dir}")
    
    # åˆ›å»ºREADMEæ–‡ä»¶
    readme_content = f"""# ResNet-18 Improved Evaluation Results

Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Directory Structure

- `evaluation/`: Evaluation results and classification report
- `sample_analysis/`: Sample analysis charts and confidence analysis
- `visualizations/`: Training history and feature visualizations
- `visualizations_fixed/`: Performance summary charts

## Key Results

- **Accuracy**: {accuracy:.4f}
- **AUC**: {roc_auc:.4f}
- **Sensitivity**: {sensitivity:.4f}
- **Specificity**: {specificity:.4f}
- **Parameters**: 11.26M
- **Training Epochs**: {len(history['train_loss'])}

## Files Generated

### Evaluation
- `evaluation_results.png`: Comprehensive evaluation charts
- `classification_report.txt`: Detailed classification metrics

### Sample Analysis
- `confidence_analysis.png`: Confidence distribution analysis
- `*_samples.png`: Sample analysis by confidence categories

### Visualizations
- `resnet18_improved_training_history.png`: Training progress charts
- `resnet18_improved_feature_maps.png`: Feature visualization
- `resnet18_improved_predictions.png`: Prediction examples

### Performance Summary
- `ResNet-18_performance_summary.png`: Overall performance summary
"""
    
    with open(os.path.join(model_dir, 'README.md'), 'w') as f:
        f.write(readme_content)

if __name__ == "__main__":
    main()
