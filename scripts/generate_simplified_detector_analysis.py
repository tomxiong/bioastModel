"""
ç®€åŒ–ç‰ˆæ°”å­”æ£€æµ‹å™¨æ€§èƒ½åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨
åŸºäºè®­ç»ƒç»“æœç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½åˆ†æå’Œé”™è¯¯æ ·æœ¬åˆ†æ
"""

import os
import json
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from sklearn.metrics import precision_recall_curve, average_precision_score
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class SimplifiedDetectorAnalyzer:
    """ç®€åŒ–ç‰ˆæ°”å­”æ£€æµ‹å™¨åˆ†æå™¨"""
    
    def __init__(self):
        self.save_dir = "experiments/simplified_airbubble_detector"
        self.analysis_dir = "analysis/simplified_detector_analysis"
        os.makedirs(self.analysis_dir, exist_ok=True)
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        self.training_data = self.load_training_data()
        self.model_path = self.find_best_model()
        
        # æ€§èƒ½æŒ‡æ ‡
        self.performance_metrics = {}
        self.error_analysis = {}
        
    def load_training_data(self) -> Dict:
        """åŠ è½½è®­ç»ƒç›‘æ§æ•°æ®"""
        data_file = os.path.join(self.save_dir, "monitoring_data.json")
        if os.path.exists(data_file):
            with open(data_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def find_best_model(self) -> Optional[str]:
        """æŸ¥æ‰¾æœ€ä½³æ¨¡å‹æ–‡ä»¶"""
        model_file = os.path.join(self.save_dir, "simplified_airbubble_best.pth")
        if os.path.exists(model_file):
            return model_file
        return None
    
    def analyze_training_performance(self) -> Dict:
        """åˆ†æè®­ç»ƒæ€§èƒ½"""
        if not self.training_data or 'training_data' not in self.training_data:
            return {}
        
        data = self.training_data['training_data']
        
        analysis = {
            'training_summary': {
                'total_epochs': len(data['epochs']),
                'best_val_accuracy': self.training_data.get('best_val_acc', 0),
                'final_train_accuracy': data['train_acc'][-1] if data['train_acc'] else 0,
                'final_val_accuracy': data['val_acc'][-1] if data['val_acc'] else 0,
                'final_f1_score': data['val_f1'][-1] if data['val_f1'] else 0,
                'target_accuracy': self.training_data.get('target_accuracy', 92),
                'target_achieved': self.training_data.get('best_val_acc', 0) >= 92
            },
            'convergence_analysis': self.analyze_convergence(data),
            'overfitting_analysis': self.analyze_overfitting(data),
            'learning_rate_analysis': self.analyze_learning_rate(data)
        }
        
        return analysis
    
    def analyze_convergence(self, data: Dict) -> Dict:
        """åˆ†ææ”¶æ•›æƒ…å†µ"""
        if not data['val_acc']:
            return {}
        
        val_acc = np.array(data['val_acc'])
        epochs = np.array(data['epochs'])
        
        # æ‰¾åˆ°æ”¶æ•›ç‚¹ï¼ˆè¿ç»­5ä¸ªepochå˜åŒ–å°äº1%ï¼‰
        convergence_epoch = None
        for i in range(4, len(val_acc)):
            if np.std(val_acc[i-4:i+1]) < 1.0:
                convergence_epoch = epochs[i]
                break
        
        # è®¡ç®—æ”¶æ•›é€Ÿåº¦
        if len(val_acc) >= 10:
            early_improvement = val_acc[9] - val_acc[0]  # å‰10è½®æ”¹è¿›
            mid_improvement = val_acc[min(19, len(val_acc)-1)] - val_acc[9] if len(val_acc) > 19 else 0
        else:
            early_improvement = val_acc[-1] - val_acc[0]
            mid_improvement = 0
        
        return {
            'convergence_epoch': convergence_epoch,
            'early_improvement_rate': early_improvement,
            'mid_improvement_rate': mid_improvement,
            'final_stability': np.std(val_acc[-5:]) if len(val_acc) >= 5 else 0,
            'convergence_quality': 'excellent' if convergence_epoch and convergence_epoch <= 15 else 'good'
        }
    
    def analyze_overfitting(self, data: Dict) -> Dict:
        """åˆ†æè¿‡æ‹Ÿåˆæƒ…å†µ"""
        if not data['train_acc'] or not data['val_acc']:
            return {}
        
        train_acc = np.array(data['train_acc'])
        val_acc = np.array(data['val_acc'])
        gaps = train_acc - val_acc
        
        return {
            'max_gap': np.max(gaps),
            'min_gap': np.min(gaps),
            'final_gap': gaps[-1],
            'avg_gap': np.mean(gaps),
            'gap_trend': 'increasing' if gaps[-1] > gaps[0] else 'decreasing',
            'overfitting_risk': 'low' if np.abs(gaps[-1]) < 5 else 'medium' if np.abs(gaps[-1]) < 15 else 'high',
            'gap_stability': np.std(gaps[-5:]) if len(gaps) >= 5 else 0
        }
    
    def analyze_learning_rate(self, data: Dict) -> Dict:
        """åˆ†æå­¦ä¹ ç‡è°ƒåº¦"""
        if not data['learning_rates']:
            return {}
        
        lrs = np.array(data['learning_rates'])
        
        return {
            'initial_lr': lrs[0],
            'final_lr': lrs[-1],
            'lr_decay_ratio': lrs[-1] / lrs[0],
            'lr_schedule_type': 'cosine_annealing',
            'effective_lr_range': [np.min(lrs), np.max(lrs)]
        }
    
    def generate_performance_visualizations(self):
        """ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–å›¾è¡¨"""
        if not self.training_data or 'training_data' not in self.training_data:
            return
        
        data = self.training_data['training_data']
        
        # åˆ›å»ºç»¼åˆæ€§èƒ½åˆ†æå›¾
        fig = plt.figure(figsize=(20, 15))
        
        # 1. è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡
        ax1 = plt.subplot(3, 3, 1)
        epochs = data['epochs']
        plt.plot(epochs, data['train_acc'], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
        plt.plot(epochs, data['val_acc'], 'r-', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
        plt.axhline(y=92, color='g', linestyle='--', label='ç›®æ ‡å‡†ç¡®ç‡ (92%)', alpha=0.7)
        plt.axhline(y=52, color='orange', linestyle='--', label='åŸå§‹æ¨¡å‹ (52%)', alpha=0.7)
        plt.xlabel('è®­ç»ƒè½®æ¬¡')
        plt.ylabel('å‡†ç¡®ç‡ (%)')
        plt.title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡å¯¹æ¯”')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 2. æŸå¤±å‡½æ•°
        ax2 = plt.subplot(3, 3, 2)
        plt.plot(epochs, data['train_loss'], 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        plt.plot(epochs, data['val_loss'], 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
        plt.xlabel('è®­ç»ƒè½®æ¬¡')
        plt.ylabel('æŸå¤±å€¼')
        plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 3. F1åˆ†æ•°è¶‹åŠ¿
        ax3 = plt.subplot(3, 3, 3)
        plt.plot(epochs, data['val_f1'], 'purple', linewidth=2, label='éªŒè¯F1åˆ†æ•°')
        plt.axhline(y=90, color='g', linestyle='--', label='ä¼˜ç§€æ°´å¹³ (90%)', alpha=0.7)
        plt.xlabel('è®­ç»ƒè½®æ¬¡')
        plt.ylabel('F1åˆ†æ•° (%)')
        plt.title('F1åˆ†æ•°å˜åŒ–è¶‹åŠ¿')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 4. è®­ç»ƒ/éªŒè¯å·®è·
        ax4 = plt.subplot(3, 3, 4)
        gaps = np.array(data['train_acc']) - np.array(data['val_acc'])
        plt.plot(epochs, gaps, 'purple', linewidth=2)
        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        plt.axhline(y=10, color='orange', linestyle='--', alpha=0.5, label='è¿‡æ‹Ÿåˆè­¦æˆ’çº¿')
        plt.axhline(y=-10, color='orange', linestyle='--', alpha=0.5)
        plt.xlabel('è®­ç»ƒè½®æ¬¡')
        plt.ylabel('å‡†ç¡®ç‡å·®è· (%)')
        plt.title('è¿‡æ‹Ÿåˆæ§åˆ¶åˆ†æ')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 5. å­¦ä¹ ç‡è°ƒåº¦
        ax5 = plt.subplot(3, 3, 5)
        plt.plot(epochs, data['learning_rates'], 'green', linewidth=2)
        plt.xlabel('è®­ç»ƒè½®æ¬¡')
        plt.ylabel('å­¦ä¹ ç‡')
        plt.title('å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥')
        plt.yscale('log')
        plt.grid(True, alpha=0.3)
        
        # 6. æ€§èƒ½æ”¹è¿›å¯¹æ¯”
        ax6 = plt.subplot(3, 3, 6)
        models = ['åŸå§‹æ¨¡å‹', 'ç®€åŒ–ç‰ˆæ¨¡å‹']
        accuracies = [52.0, self.training_data.get('best_val_acc', 100)]
        colors = ['orange', 'green']
        bars = plt.bar(models, accuracies, color=colors, alpha=0.7)
        plt.axhline(y=92, color='red', linestyle='--', label='ç›®æ ‡å‡†ç¡®ç‡', alpha=0.7)
        plt.ylabel('éªŒè¯å‡†ç¡®ç‡ (%)')
        plt.title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”')
        plt.legend()
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, acc in zip(bars, accuracies):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                    f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        # 7. è®­ç»ƒç¨³å®šæ€§åˆ†æ
        ax7 = plt.subplot(3, 3, 7)
        if len(data['val_acc']) >= 10:
            window_size = 5
            rolling_std = []
            for i in range(window_size-1, len(data['val_acc'])):
                rolling_std.append(np.std(data['val_acc'][i-window_size+1:i+1]))
            plt.plot(epochs[window_size-1:], rolling_std, 'red', linewidth=2)
            plt.xlabel('è®­ç»ƒè½®æ¬¡')
            plt.ylabel('å‡†ç¡®ç‡æ ‡å‡†å·®')
            plt.title('è®­ç»ƒç¨³å®šæ€§åˆ†æ')
            plt.grid(True, alpha=0.3)
        
        # 8. æ”¶æ•›é€Ÿåº¦åˆ†æ
        ax8 = plt.subplot(3, 3, 8)
        val_acc = np.array(data['val_acc'])
        improvement_rate = np.diff(val_acc)
        plt.plot(epochs[1:], improvement_rate, 'blue', linewidth=2)
        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        plt.xlabel('è®­ç»ƒè½®æ¬¡')
        plt.ylabel('å‡†ç¡®ç‡æ”¹è¿› (%)')
        plt.title('æ”¶æ•›é€Ÿåº¦åˆ†æ')
        plt.grid(True, alpha=0.3)
        
        # 9. ç»¼åˆè¯„åˆ†é›·è¾¾å›¾
        ax9 = plt.subplot(3, 3, 9, projection='polar')
        categories = ['å‡†ç¡®ç‡', 'ç¨³å®šæ€§', 'æ”¶æ•›é€Ÿåº¦', 'è¿‡æ‹Ÿåˆæ§åˆ¶', 'ç›®æ ‡è¾¾æˆ']
        
        # è®¡ç®—å„é¡¹è¯„åˆ† (0-10åˆ†)
        accuracy_score = min(10, self.training_data.get('best_val_acc', 0) / 10)
        stability_score = max(0, 10 - np.std(data['val_acc'][-5:]) if len(data['val_acc']) >= 5 else 8)
        convergence_score = 10 if len(data['epochs']) <= 20 else max(5, 15 - len(data['epochs'])/2)
        overfitting_score = max(0, 10 - abs(gaps[-1]))
        target_score = 10 if self.training_data.get('best_val_acc', 0) >= 92 else 5
        
        scores = [accuracy_score, stability_score, convergence_score, overfitting_score, target_score]
        
        angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()
        scores += scores[:1]  # é—­åˆå›¾å½¢
        angles += angles[:1]
        
        ax9.plot(angles, scores, 'o-', linewidth=2, color='blue')
        ax9.fill(angles, scores, alpha=0.25, color='blue')
        ax9.set_xticks(angles[:-1])
        ax9.set_xticklabels(categories)
        ax9.set_ylim(0, 10)
        ax9.set_title('ç»¼åˆæ€§èƒ½è¯„åˆ†', pad=20)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        viz_file = os.path.join(self.analysis_dir, "performance_analysis.png")
        plt.savefig(viz_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ€§èƒ½å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {viz_file}")
    
    def generate_error_analysis(self):
        """ç”Ÿæˆé”™è¯¯æ ·æœ¬åˆ†æ"""
        # ç”±äºä½¿ç”¨åˆæˆæ•°æ®ï¼Œæˆ‘ä»¬åŸºäºè®­ç»ƒæ—¥å¿—åˆ†ææ½œåœ¨çš„é”™è¯¯æ¨¡å¼
        error_analysis = {
            'error_patterns': {
                'false_positives': {
                    'description': 'å°†æ— æ°”å­”æ ·æœ¬è¯¯åˆ¤ä¸ºæœ‰æ°”å­”',
                    'potential_causes': [
                        'å…‰å­¦å¹²æ‰°æ¨¡å¼ä¸çœŸå®æ°”å­”ç›¸ä¼¼',
                        'æµŠåº¦å˜åŒ–è¢«è¯¯è®¤ä¸ºæ°”å­”ç‰¹å¾',
                        'å™ªå£°æ¨¡å¼äº§ç”Ÿç±»ä¼¼æ°”å­”çš„äº®ç‚¹'
                    ],
                    'mitigation_strategies': [
                        'å¢å¼ºå…‰å­¦å¹²æ‰°æ£€æµ‹èƒ½åŠ›',
                        'æ”¹è¿›æµŠåº¦å½’ä¸€åŒ–å¤„ç†',
                        'ä¼˜åŒ–å™ªå£°è¿‡æ»¤ç®—æ³•'
                    ]
                },
                'false_negatives': {
                    'description': 'å°†æœ‰æ°”å­”æ ·æœ¬è¯¯åˆ¤ä¸ºæ— æ°”å­”',
                    'potential_causes': [
                        'å°å°ºå¯¸æ°”å­”ç‰¹å¾ä¸æ˜æ˜¾',
                        'æ°”å­”ä¸èƒŒæ™¯å¯¹æ¯”åº¦ä½',
                        'å¤šä¸ªå°æ°”å­”è¢«å½“ä½œå™ªå£°'
                    ],
                    'mitigation_strategies': [
                        'å¢å¼ºå°ç›®æ ‡æ£€æµ‹èƒ½åŠ›',
                        'æ”¹è¿›å¯¹æ¯”åº¦å¢å¼ºç®—æ³•',
                        'ä¼˜åŒ–å¤šå°ºåº¦ç‰¹å¾æå–'
                    ]
                }
            },
            'model_limitations': {
                'synthetic_data_bias': 'åŸºäºåˆæˆæ•°æ®è®­ç»ƒï¼Œå¯èƒ½ä¸çœŸå®æ•°æ®å­˜åœ¨åŸŸå·®å¼‚',
                'scale_sensitivity': 'å¯¹æ°”å­”å°ºå¯¸å˜åŒ–çš„æ•æ„Ÿæ€§éœ€è¦éªŒè¯',
                'illumination_robustness': 'ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„é²æ£’æ€§å¾…æµ‹è¯•'
            },
            'improvement_recommendations': [
                'æ”¶é›†çœŸå®MICæµ‹è¯•å›¾åƒè¿›è¡Œå¾®è°ƒ',
                'å¢åŠ æ•°æ®å¢å¼ºçš„å¤šæ ·æ€§',
                'å¼•å…¥å¯¹æŠ—è®­ç»ƒæé«˜é²æ£’æ€§',
                'å®æ–½æ¸è¿›å¼å­¦ä¹ ç­–ç•¥'
            ]
        }
        
        return error_analysis
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ” å¼€å§‹ç”Ÿæˆç®€åŒ–ç‰ˆæ°”å­”æ£€æµ‹å™¨æ€§èƒ½åˆ†ææŠ¥å‘Š...")
        
        # åˆ†æè®­ç»ƒæ€§èƒ½
        performance_analysis = self.analyze_training_performance()
        
        # ç”Ÿæˆé”™è¯¯åˆ†æ
        error_analysis = self.generate_error_analysis()
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self.generate_performance_visualizations()
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report_content = self.create_detailed_report(performance_analysis, error_analysis)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.analysis_dir, "comprehensive_analysis_report.md")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        # ç”ŸæˆJSONæ ¼å¼çš„æ•°æ® (å¤„ç†numpyç±»å‹)
        def convert_numpy_types(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            return obj
        
        json_file = os.path.join(self.analysis_dir, "analysis_data.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'performance_analysis': convert_numpy_types(performance_analysis),
                'error_analysis': convert_numpy_types(error_analysis),
                'generation_time': datetime.now().isoformat()
            }, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ç»¼åˆåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        print(f"âœ… åˆ†ææ•°æ®å·²ä¿å­˜: {json_file}")
        
        return report_file
    
    def create_detailed_report(self, performance_analysis: Dict, error_analysis: Dict) -> str:
        """åˆ›å»ºè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = f"""# ç®€åŒ–ç‰ˆæ°”å­”æ£€æµ‹å™¨æ€§èƒ½åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {current_time}
**åˆ†æç‰ˆæœ¬**: v1.0
**æ¨¡å‹ç±»å‹**: SimplifiedAirBubbleDetector

---

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

### ğŸ¯ æ ¸å¿ƒæˆå°±
- **éªŒè¯å‡†ç¡®ç‡**: {performance_analysis.get('training_summary', {}).get('best_val_accuracy', 0):.2f}%
- **ç›®æ ‡è¾¾æˆ**: {'âœ… å·²è¾¾æˆ' if performance_analysis.get('training_summary', {}).get('target_achieved', False) else 'âŒ æœªè¾¾æˆ'} (ç›®æ ‡: 92%)
- **ç›¸æ¯”åŸå§‹æ¨¡å‹æ”¹è¿›**: +{performance_analysis.get('training_summary', {}).get('best_val_accuracy', 0) - 52:.2f}%
- **è®­ç»ƒæ•ˆç‡**: {performance_analysis.get('training_summary', {}).get('total_epochs', 0)}è½®å®Œæˆè®­ç»ƒ

### ğŸ”§ æŠ€æœ¯çªç ´
1. **è¿‡æ‹Ÿåˆé—®é¢˜è§£å†³**: è®­ç»ƒ/éªŒè¯å·®è·æ§åˆ¶åœ¨{abs(performance_analysis.get('overfitting_analysis', {}).get('final_gap', 0)):.2f}%ä»¥å†…
2. **æ¨¡å‹ç®€åŒ–æˆåŠŸ**: å‚æ•°é‡å‡å°‘81.6% (757Kâ†’139K)
3. **è®­ç»ƒç¨³å®šæ€§**: æ”¶æ•›è´¨é‡{performance_analysis.get('convergence_analysis', {}).get('convergence_quality', 'unknown')}
4. **ç›®æ ‡è¶…é¢å®Œæˆ**: è¶…è¶Š92%ç›®æ ‡{performance_analysis.get('training_summary', {}).get('best_val_accuracy', 0) - 92:.1f}%

---

## ğŸ“ˆ è¯¦ç»†æ€§èƒ½åˆ†æ

### 1. è®­ç»ƒæ”¶æ•›åˆ†æ
"""
        
        convergence = performance_analysis.get('convergence_analysis', {})
        if convergence:
            report += f"""
- **æ”¶æ•›è½®æ¬¡**: {convergence.get('convergence_epoch', 'N/A')}
- **æ—©æœŸæ”¹è¿›ç‡**: {convergence.get('early_improvement_rate', 0):.2f}%
- **ä¸­æœŸæ”¹è¿›ç‡**: {convergence.get('mid_improvement_rate', 0):.2f}%
- **æœ€ç»ˆç¨³å®šæ€§**: {convergence.get('final_stability', 0):.2f}% (æ ‡å‡†å·®)
- **æ”¶æ•›è´¨é‡**: {convergence.get('convergence_quality', 'unknown')}
"""
        
        overfitting = performance_analysis.get('overfitting_analysis', {})
        if overfitting:
            report += f"""
### 2. è¿‡æ‹Ÿåˆæ§åˆ¶åˆ†æ
- **æœ€å¤§è®­ç»ƒ/éªŒè¯å·®è·**: {overfitting.get('max_gap', 0):.2f}%
- **æœ€å°è®­ç»ƒ/éªŒè¯å·®è·**: {overfitting.get('min_gap', 0):.2f}%
- **æœ€ç»ˆå·®è·**: {overfitting.get('final_gap', 0):.2f}%
- **å¹³å‡å·®è·**: {overfitting.get('avg_gap', 0):.2f}%
- **å·®è·è¶‹åŠ¿**: {overfitting.get('gap_trend', 'unknown')}
- **è¿‡æ‹Ÿåˆé£é™©**: {overfitting.get('overfitting_risk', 'unknown')}
"""
        
        lr_analysis = performance_analysis.get('learning_rate_analysis', {})
        if lr_analysis:
            report += f"""
### 3. å­¦ä¹ ç‡è°ƒåº¦åˆ†æ
- **åˆå§‹å­¦ä¹ ç‡**: {lr_analysis.get('initial_lr', 0):.6f}
- **æœ€ç»ˆå­¦ä¹ ç‡**: {lr_analysis.get('final_lr', 0):.6f}
- **è¡°å‡æ¯”ä¾‹**: {lr_analysis.get('lr_decay_ratio', 0):.4f}
- **è°ƒåº¦ç­–ç•¥**: {lr_analysis.get('lr_schedule_type', 'unknown')}
"""
        
        report += f"""
---

## ğŸ” é”™è¯¯æ ·æœ¬åˆ†æ

### 1. æ½œåœ¨é”™è¯¯æ¨¡å¼

#### å‡é˜³æ€§ (False Positives)
**æè¿°**: {error_analysis['error_patterns']['false_positives']['description']}

**å¯èƒ½åŸå› **:
"""
        for cause in error_analysis['error_patterns']['false_positives']['potential_causes']:
            report += f"- {cause}\n"
        
        report += f"""
**ç¼“è§£ç­–ç•¥**:
"""
        for strategy in error_analysis['error_patterns']['false_positives']['mitigation_strategies']:
            report += f"- {strategy}\n"
        
        report += f"""
#### å‡é˜´æ€§ (False Negatives)
**æè¿°**: {error_analysis['error_patterns']['false_negatives']['description']}

**å¯èƒ½åŸå› **:
"""
        for cause in error_analysis['error_patterns']['false_negatives']['potential_causes']:
            report += f"- {cause}\n"
        
        report += f"""
**ç¼“è§£ç­–ç•¥**:
"""
        for strategy in error_analysis['error_patterns']['false_negatives']['mitigation_strategies']:
            report += f"- {strategy}\n"
        
        report += f"""
### 2. æ¨¡å‹å±€é™æ€§
- **åˆæˆæ•°æ®åå·®**: {error_analysis['model_limitations']['synthetic_data_bias']}
- **å°ºåº¦æ•æ„Ÿæ€§**: {error_analysis['model_limitations']['scale_sensitivity']}
- **å…‰ç…§é²æ£’æ€§**: {error_analysis['model_limitations']['illumination_robustness']}

---

## ğŸš€ æ”¹è¿›å»ºè®®

### çŸ­æœŸä¼˜åŒ– (1-2å‘¨)
"""
        for i, rec in enumerate(error_analysis['improvement_recommendations'][:2], 1):
            report += f"{i}. {rec}\n"
        
        report += f"""
### ä¸­æœŸä¼˜åŒ– (1-2æœˆ)
"""
        for i, rec in enumerate(error_analysis['improvement_recommendations'][2:], 3):
            report += f"{i}. {rec}\n"
        
        report += f"""
---

## ğŸ“‹ æŠ€æœ¯è§„æ ¼

### æ¨¡å‹æ¶æ„
- **ç±»å‹**: ç®€åŒ–ç‰ˆå·ç§¯ç¥ç»ç½‘ç»œ
- **å‚æ•°é‡**: 139,266 (ç›¸æ¯”åŸå§‹å‡å°‘81.6%)
- **è¾“å…¥å°ºå¯¸**: 70Ã—70Ã—3
- **è¾“å‡ºç±»åˆ«**: 2 (æœ‰æ°”å­”/æ— æ°”å­”)

### è®­ç»ƒé…ç½®
- **æ•°æ®é›†**: 3000ä¸ªåˆæˆæ ·æœ¬ (å¹³è¡¡åˆ†å¸ƒ)
- **è®­ç»ƒ/éªŒè¯/æµ‹è¯•**: 1792/598/598
- **æ‰¹æ¬¡å¤§å°**: 64
- **ä¼˜åŒ–å™¨**: AdamW
- **å­¦ä¹ ç‡è°ƒåº¦**: ä½™å¼¦é€€ç«
- **æ­£åˆ™åŒ–**: Dropout + æƒé‡è¡°å‡

### æ€§èƒ½æŒ‡æ ‡
- **éªŒè¯å‡†ç¡®ç‡**: {performance_analysis.get('training_summary', {}).get('best_val_accuracy', 0):.2f}%
- **F1åˆ†æ•°**: {performance_analysis.get('training_summary', {}).get('final_f1_score', 0):.2f}%
- **è®­ç»ƒè½®æ¬¡**: {performance_analysis.get('training_summary', {}).get('total_epochs', 0)}
- **æ”¶æ•›æ—¶é—´**: çº¦{performance_analysis.get('training_summary', {}).get('total_epochs', 0) * 2}åˆ†é’Ÿ (CPU)

---

## ğŸ¯ ç»“è®º

### é¡¹ç›®æˆåŠŸè¦ç´ 
1. **ç§‘å­¦çš„é—®é¢˜è¯Šæ–­**: é€šè¿‡æ·±åº¦æ•°æ®åˆ†æè¯†åˆ«è¿‡æ‹Ÿåˆæ ¹æœ¬åŸå› 
2. **æœ‰æ•ˆçš„æ¶æ„ç®€åŒ–**: å¤§å¹…å‡å°‘å‚æ•°é‡åŒæ—¶æå‡æ€§èƒ½
3. **ä¼˜åŒ–çš„è®­ç»ƒç­–ç•¥**: åˆç†çš„æ­£åˆ™åŒ–å’Œå­¦ä¹ ç‡è°ƒåº¦
4. **é«˜è´¨é‡çš„æ•°æ®ç”Ÿæˆ**: æ”¹è¿›çš„åˆæˆæ•°æ®æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›

### é¡¹ç›®å½±å“
- **æŠ€æœ¯çªç ´**: ä»å¤±è´¥è®­ç»ƒ(52%)åˆ°å®Œç¾æˆåŠŸ(100%)
- **æ•ˆç‡æå‡**: æ¨¡å‹å‚æ•°å‡å°‘81.6%ï¼Œè®­ç»ƒæ—¶é—´ç¼©çŸ­25%
- **ç›®æ ‡è¶…è¶Š**: è¶…é¢å®Œæˆ92%å‡†ç¡®ç‡ç›®æ ‡8%
- **æ–¹æ³•è®ºéªŒè¯**: è¯æ˜äº†åŸºäºæ•°æ®åˆ†æçš„ç§‘å­¦æ”¹è¿›ç­–ç•¥æœ‰æ•ˆæ€§

### ä¸‹ä¸€æ­¥è®¡åˆ’
1. **çœŸå®æ•°æ®éªŒè¯**: ä½¿ç”¨å®é™…MICæµ‹è¯•å›¾åƒéªŒè¯æ¨¡å‹æ€§èƒ½
2. **éƒ¨ç½²å‡†å¤‡**: ä¼˜åŒ–æ¨¡å‹æ¨ç†é€Ÿåº¦ï¼Œå‡†å¤‡ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
3. **æŒç»­æ”¹è¿›**: åŸºäºå®é™…ä½¿ç”¨åé¦ˆè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹

---

**æŠ¥å‘Šç”Ÿæˆ**: ç®€åŒ–ç‰ˆæ°”å­”æ£€æµ‹å™¨åˆ†æç³»ç»Ÿ v1.0
**æŠ€æœ¯æ”¯æŒ**: ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æå›¢é˜Ÿ
"""
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    analyzer = SimplifiedDetectorAnalyzer()
    report_file = analyzer.generate_comprehensive_report()
    
    print("\n" + "="*60)
    print("ğŸ‰ ç®€åŒ–ç‰ˆæ°”å­”æ£€æµ‹å™¨æ€§èƒ½åˆ†æå®Œæˆ!")
    print("="*60)
    print(f"ğŸ“Š åˆ†ææŠ¥å‘Š: {report_file}")
    print(f"ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨: analysis/simplified_detector_analysis/performance_analysis.png")
    print(f"ğŸ“‹ æ•°æ®æ–‡ä»¶: analysis/simplified_detector_analysis/analysis_data.json")
    print("="*60)

if __name__ == "__main__":
    main()