"""
å°†å‰©ä½™çš„æ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼
æ”¯æŒçš„æ¨¡å‹ï¼š
- coatnet
- convnext_tiny
- vit_tiny
- airbubble_hybrid_net
- mic_mobilenetv3
- micro_vit
- enhanced_airbubble_detector
"""

import os
import sys
import torch
import logging
import argparse
import numpy as np
import onnx
import onnxruntime as ort
from pathlib import Path
import importlib

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥é…ç½®
from core.config.model_configs import MODEL_CONFIGS

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

class ModelONNXConverter:
    def __init__(self, output_dir="deployment/onnx_models"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.output_dir = output_dir
        self.model_configs = MODEL_CONFIGS
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        
    def _get_checkpoint_path(self, model_name):
        """è·å–æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„"""
        # å°è¯•å¤šç§å¯èƒ½çš„è·¯å¾„
        possible_paths = [
            # æ¨¡å‹åç§°ç›®å½•ä¸‹çš„best_model.pth
            f"experiments/{model_name}/best_model.pth",
            # æ¨¡å‹åç§°ç›®å½•ä¸‹çš„æ¨¡å‹å_best.pth
            f"experiments/{model_name}/{model_name}_best.pth",
            # å®éªŒç›®å½•ä¸‹çš„æ¨¡å‹ç›®å½•ä¸‹çš„best_model.pth
            f"experiments/{self.model_configs[model_name].get('experiment_pattern', '')}/{model_name}/best_model.pth",
            # å®éªŒç›®å½•ä¸‹çš„æ¨¡å‹å_best.pth
            f"experiments/{self.model_configs[model_name].get('experiment_pattern', '')}/{model_name}_best.pth",
            # æ¨¡å‹åç§°ç›®å½•ä¸‹çš„æ¨¡å‹åŸºç¡€å_best.pth
            f"experiments/{model_name}/{model_name.split('_')[0]}_best.pth"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                return path
                
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›é»˜è®¤è·¯å¾„
        return f"experiments/{model_name}/{model_name}_best.pth"
    
    def load_model(self, model_name):
        """åŠ è½½æŒ‡å®šçš„æ¨¡å‹"""
        try:
            # è·å–æ¨¡å‹é…ç½®
            model_config = self.model_configs.get(model_name, {})
            
            if not model_config:
                raise ValueError(f"æœªæ‰¾åˆ°æ¨¡å‹é…ç½®: {model_name}")
            
            # è·å–æ¨¡å‹ç±»å’Œæ¨¡å—è·¯å¾„
            class_name = model_config.get('class_name')
            module_path = model_config.get('module_path')
            
            if not class_name or not module_path:
                raise ValueError(f"æ¨¡å‹é…ç½®ç¼ºå°‘class_nameæˆ–module_path: {model_name}")
            
            # åŠ¨æ€å¯¼å…¥æ¨¡å—
            module = importlib.import_module(module_path)
            
            # è·å–æ¨¡å‹åˆ›å»ºå‡½æ•°æˆ–ç±»
            model_class = None
            
            # å°è¯•è·å–create_æ¨¡å‹åå‡½æ•°
            create_func_name = f"create_{model_name}"
            if hasattr(module, create_func_name):
                model_class = getattr(module, create_func_name)
                model = model_class(num_classes=2)
            # å°è¯•è·å–ç±»å
            elif hasattr(module, class_name):
                model_class = getattr(module, class_name)
                model = model_class(num_classes=2)
            else:
                raise ValueError(f"æœªæ‰¾åˆ°æ¨¡å‹ç±»æˆ–åˆ›å»ºå‡½æ•°: {class_name}")
            
            # åŠ è½½æ¨¡å‹æƒé‡
            checkpoint_path = self._get_checkpoint_path(model_name)
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # å°è¯•åŠ è½½æ¨¡å‹æƒé‡
            try:
                model.load_state_dict(checkpoint['model_state_dict'])
            except Exception as e:
                # å¦‚æœç›´æ¥åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤„ç†æƒé‡é”®
                state_dict = checkpoint['model_state_dict']
                new_state_dict = {}
                for k, v in state_dict.items():
                    if k.startswith('module.'):
                        new_state_dict[k[7:]] = v
                    else:
                        new_state_dict[k] = v
                model.load_state_dict(new_state_dict)
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡: {checkpoint_path}")
            model.eval()
            return model
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ {model_name}: {str(e)}")
            return None
    
    def convert_to_onnx(self, model_name, model, opset_version=11):
        """å°†æ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼"""
        try:
            onnx_path = os.path.join(self.output_dir, f"{model_name}.onnx")
            
            # å‡†å¤‡è¾“å…¥å¼ é‡
            dummy_input = torch.randn(1, 3, 70, 70, device=self.device)
            
            # å¯¼å‡ºä¸ºONNX
            torch.onnx.export(
                model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=opset_version,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": onnx_path,
                "file_size": file_size
            }
            
        except Exception as e:
            logger.error(f"âŒ è½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return None
    
    def test_onnx_model(self, onnx_path):
        """æµ‹è¯•ONNXæ¨¡å‹"""
        try:
            # åŠ è½½ONNXæ¨¡å‹
            onnx_model = onnx.load(onnx_path)
            onnx.checker.check_model(onnx_model)
            
            # åˆ›å»ºæ¨ç†ä¼šè¯
            session = ort.InferenceSession(onnx_path)
            
            # å‡†å¤‡è¾“å…¥æ•°æ®
            input_name = session.get_inputs()[0].name
            input_shape = session.get_inputs()[0].shape
            dummy_input = np.random.randn(1, 3, 70, 70).astype(np.float32)
            
            # è¿è¡Œæ¨ç†
            outputs = session.run(None, {input_name: dummy_input})
            output = outputs[0]
            
            logger.info(f"âœ… ONNXæ¨¡å‹æµ‹è¯•æˆåŠŸ: {onnx_path}")
            logger.info(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
            logger.info(f"   è¾“å‡ºèŒƒå›´: [{output.min()} {output.max()}]")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ONNXæ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}")
            return False
    
    def convert_single_model(self, model_name):
        """è½¬æ¢å•ä¸ªæ¨¡å‹"""
        logger.info(f"\nğŸ“¦ æ­£åœ¨å¤„ç†æ¨¡å‹: {model_name}")
        logger.info(f"   æè¿°: {self.model_configs.get(model_name, {}).get('description', 'æœªçŸ¥')}")
        
        # åŠ è½½æ¨¡å‹
        model = self.load_model(model_name)
        if model is None:
            return False
        
        # è½¬æ¢ä¸ºONNX
        result = self.convert_to_onnx(model_name, model)
        if result is None:
            return False
        
        # æµ‹è¯•ONNXæ¨¡å‹
        if self.test_onnx_model(result["onnx_path"]):
            logger.info(f"âœ… æ¨¡å‹ {model_name} è½¬æ¢å¹¶æµ‹è¯•æˆåŠŸ!")
            return True
        else:
            return False
    
    def convert_all_models(self, model_names):
        """è½¬æ¢å¤šä¸ªæ¨¡å‹"""
        results = {}
        
        for model_name in model_names:
            logger.info(f"\nğŸ”„ å¼€å§‹è½¬æ¢æ¨¡å‹: {model_name}")
            success = self.convert_single_model(model_name)
            results[model_name] = success
        
        # æ‰“å°è½¬æ¢ç»“æœæ‘˜è¦
        logger.info("\nğŸ“Š è½¬æ¢ç»“æœæ‘˜è¦:")
        for model_name, success in results.items():
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
            logger.info(f"   {model_name}: {status}")
        
        # è®¡ç®—æˆåŠŸç‡
        success_count = sum(1 for success in results.values() if success)
        total_count = len(results)
        success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
        
        logger.info(f"\nğŸ“ˆ æ€»ä½“æˆåŠŸç‡: {success_rate:.1f}% ({success_count}/{total_count})")
        
        return results

def main():
    parser = argparse.ArgumentParser(description="å°†å¤šä¸ªæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼")
    parser.add_argument("--output_dir", type=str, default="deployment/onnx_models", 
                        help="ONNXæ¨¡å‹è¾“å‡ºç›®å½•")
    parser.add_argument("--models", type=str, nargs="+", 
                        default=["coatnet", "convnext_tiny", "vit_tiny", 
                                "airbubble_hybrid_net", "mic_mobilenetv3", 
                                "micro_vit", "enhanced_airbubble_detector"],
                        help="è¦è½¬æ¢çš„æ¨¡å‹åç§°åˆ—è¡¨")
    args = parser.parse_args()
    
    logger.info("ğŸš€ å¼€å§‹æ‰¹é‡è½¬æ¢æ¨¡å‹")
    logger.info(f"ğŸ“‹ å¾…è½¬æ¢æ¨¡å‹: {', '.join(args.models)}")
    
    converter = ModelONNXConverter(output_dir=args.output_dir)
    results = converter.convert_all_models(args.models)
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ¨¡å‹éƒ½è½¬æ¢æˆåŠŸ
    all_success = all(results.values())
    
    if all_success:
        logger.info("\nğŸ‰ æ‰€æœ‰æ¨¡å‹è½¬æ¢æˆåŠŸ!")
    else:
        logger.warning("\nâš ï¸ éƒ¨åˆ†æ¨¡å‹è½¬æ¢å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—äº†è§£è¯¦æƒ…")
    
    logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {converter.output_dir}")

if __name__ == "__main__":
    main()