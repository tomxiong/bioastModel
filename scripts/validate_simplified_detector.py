"""
ç®€åŒ–ç‰ˆæ°”å­”æ£€æµ‹å™¨éªŒè¯è„šæœ¬
ç”Ÿæˆè¯¦ç»†çš„éªŒè¯æŠ¥å‘Šï¼ŒåŒ…æ‹¬æ­£ç¡®å’Œé”™è¯¯æ ·æœ¬çš„åˆ†æ
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import json
import pandas as pd
import seaborn as sns
from datetime import datetime

from models.simplified_airbubble_detector import create_simplified_airbubble_detector
from training.dataset import create_data_loaders
from core.config import get_experiment_path, DATA_DIR, get_latest_experiment_path

def load_model(model_path):
    """åŠ è½½æ¨¡å‹"""
    print(f"ğŸ” åŠ è½½æ¨¡å‹: {model_path}")
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None
    
    try:
        checkpoint = torch.load(model_path, map_location=torch.device('cpu'))
        model = create_simplified_airbubble_detector()
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹")
        return model
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return None

def evaluate_model(model, data_loader, device):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print("ğŸ” è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    
    model.to(device)
    model.eval()
    
    all_predictions = []
    all_labels = []
    all_confidences = []
    all_images = []
    all_file_paths = []
    
    with torch.no_grad():
        for images, labels in data_loader:
            images, labels = images.to(device), labels.to(device)
            
            outputs = model(images)
            probabilities = torch.softmax(outputs, dim=1)
            confidences, predictions = torch.max(probabilities, dim=1)
            
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_confidences.extend(confidences.cpu().numpy())
            all_images.extend(images.cpu().numpy())
            
            # å¦‚æœæ•°æ®é›†æœ‰æ–‡ä»¶è·¯å¾„å±æ€§
            if hasattr(data_loader.dataset, 'samples'):
                batch_indices = list(range(len(all_labels) - len(labels), len(all_labels)))
                batch_paths = [data_loader.dataset.samples[i][0] for i in batch_indices]
                all_file_paths.extend(batch_paths)
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(all_labels, all_predictions)
    precision = precision_score(all_labels, all_predictions, average='weighted')
    recall = recall_score(all_labels, all_predictions, average='weighted')
    f1 = f1_score(all_labels, all_predictions, average='weighted')
    
    print(f"ğŸ“Š å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"ğŸ“Š ç²¾ç¡®ç‡: {precision:.4f}")
    print(f"ğŸ“Š å¬å›ç‡: {recall:.4f}")
    print(f"ğŸ“Š F1åˆ†æ•°: {f1:.4f}")
    
    # è¿”å›ç»“æœ
    return {
        'predictions': np.array(all_predictions),
        'labels': np.array(all_labels),
        'confidences': np.array(all_confidences),
        'images': np.array(all_images),
        'file_paths': all_file_paths,
        'metrics': {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1': float(f1)
        }
    }

def plot_confusion_matrix(labels, predictions, save_path):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    print("ğŸ” ç»˜åˆ¶æ··æ·†çŸ©é˜µ...")
    
    cm = confusion_matrix(labels, predictions)
    plt.figure(figsize=(10, 8))
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['æ— æ°”å­”', 'æœ‰æ°”å­”'],
                yticklabels=['æ— æ°”å­”', 'æœ‰æ°”å­”'])
    
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()
    
    print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {save_path}")

def plot_confidence_distribution(confidences, labels, predictions, save_path):
    """ç»˜åˆ¶ç½®ä¿¡åº¦åˆ†å¸ƒ"""
    print("ğŸ” ç»˜åˆ¶ç½®ä¿¡åº¦åˆ†å¸ƒ...")
    
    plt.figure(figsize=(12, 8))
    
    # æ­£ç¡®é¢„æµ‹çš„ç½®ä¿¡åº¦
    correct = confidences[predictions == labels]
    # é”™è¯¯é¢„æµ‹çš„ç½®ä¿¡åº¦
    incorrect = confidences[predictions != labels]
    
    plt.hist(correct, bins=20, alpha=0.7, label='æ­£ç¡®é¢„æµ‹', color='green')
    plt.hist(incorrect, bins=20, alpha=0.7, label='é”™è¯¯é¢„æµ‹', color='red')
    
    plt.title('é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
    plt.xlabel('ç½®ä¿¡åº¦')
    plt.ylabel('æ ·æœ¬æ•°é‡')
    plt.legend()
    plt.grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()
    
    print(f"âœ… ç½®ä¿¡åº¦åˆ†å¸ƒå·²ä¿å­˜åˆ°: {save_path}")

def visualize_samples(images, labels, predictions, confidences, save_dir, category, num_samples=10):
    """å¯è§†åŒ–æ ·æœ¬"""
    print(f"ğŸ” å¯è§†åŒ–{category}æ ·æœ¬...")
    
    if category == 'correct':
        indices = np.where(predictions == labels)[0]
    elif category == 'incorrect':
        indices = np.where(predictions != labels)[0]
    else:
        indices = np.arange(len(labels))
    
    if len(indices) == 0:
        print(f"âš ï¸ æ²¡æœ‰{category}æ ·æœ¬å¯ä¾›å¯è§†åŒ–")
        return
    
    # é€‰æ‹©æ ·æœ¬
    if len(indices) > num_samples:
        indices = np.random.choice(indices, num_samples, replace=False)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    
    # å¯è§†åŒ–æ¯ä¸ªæ ·æœ¬
    for i, idx in enumerate(indices):
        image = images[idx].transpose(1, 2, 0)  # è½¬æ¢ä¸º(H, W, C)æ ¼å¼
        
        # åå½’ä¸€åŒ–
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        image = std * image + mean
        image = np.clip(image, 0, 1)
        
        plt.figure(figsize=(8, 8))
        plt.imshow(image)
        
        true_label = "æœ‰æ°”å­”" if labels[idx] == 1 else "æ— æ°”å­”"
        pred_label = "æœ‰æ°”å­”" if predictions[idx] == 1 else "æ— æ°”å­”"
        
        title = f"çœŸå®: {true_label}, é¢„æµ‹: {pred_label}\nç½®ä¿¡åº¦: {confidences[idx]:.4f}"
        plt.title(title)
        plt.axis('off')
        
        save_path = os.path.join(save_dir, f"{category}_sample_{i+1}.png")
        plt.savefig(save_path, dpi=300)
        plt.close()
    
    print(f"âœ… {category}æ ·æœ¬å·²ä¿å­˜åˆ°: {save_dir}")

def generate_error_list(file_paths, labels, predictions, confidences, save_path):
    """ç”Ÿæˆé”™è¯¯æ ·æœ¬åˆ—è¡¨"""
    print("ğŸ” ç”Ÿæˆé”™è¯¯æ ·æœ¬åˆ—è¡¨...")
    
    if len(file_paths) == 0:
        print("âš ï¸ æ²¡æœ‰æ–‡ä»¶è·¯å¾„ä¿¡æ¯ï¼Œæ— æ³•ç”Ÿæˆé”™è¯¯æ ·æœ¬åˆ—è¡¨")
        return
    
    # æ‰¾å‡ºé”™è¯¯é¢„æµ‹çš„æ ·æœ¬
    error_indices = np.where(predictions != labels)[0]
    
    if len(error_indices) == 0:
        print("âœ… æ²¡æœ‰é”™è¯¯é¢„æµ‹çš„æ ·æœ¬")
        return
    
    # åˆ›å»ºé”™è¯¯æ ·æœ¬åˆ—è¡¨
    error_list = []
    for idx in error_indices:
        file_path = file_paths[idx]
        file_name = os.path.basename(file_path)
        true_label = "æœ‰æ°”å­”" if labels[idx] == 1 else "æ— æ°”å­”"
        pred_label = "æœ‰æ°”å­”" if predictions[idx] == 1 else "æ— æ°”å­”"
        
        error_list.append({
            'file_name': file_name,
            'file_path': file_path,
            'true_label': true_label,
            'predicted_label': pred_label,
            'confidence': float(confidences[idx])
        })
    
    # æŒ‰ç½®ä¿¡åº¦æ’åº
    error_list.sort(key=lambda x: x['confidence'], reverse=True)
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(error_list, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… é”™è¯¯æ ·æœ¬åˆ—è¡¨å·²ä¿å­˜åˆ°: {save_path}")
    print(f"   å…±{len(error_list)}ä¸ªé”™è¯¯æ ·æœ¬")

def generate_validation_report(results, save_path):
    """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
    print("ğŸ” ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
    
    # è®¡ç®—ç±»åˆ«æŒ‡æ ‡
    class_report = classification_report(
        results['labels'], 
        results['predictions'], 
        target_names=['æ— æ°”å­”', 'æœ‰æ°”å­”'],
        output_dict=True
    )
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(results['labels'], results['predictions'])
    
    # åˆ›å»ºæŠ¥å‘Š
    report = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'metrics': results['metrics'],
        'class_report': class_report,
        'confusion_matrix': cm.tolist(),
        'sample_count': len(results['labels']),
        'correct_count': int(np.sum(results['predictions'] == results['labels'])),
        'error_count': int(np.sum(results['predictions'] != results['labels'])),
        'confidence_stats': {
            'mean': float(np.mean(results['confidences'])),
            'median': float(np.median(results['confidences'])),
            'min': float(np.min(results['confidences'])),
            'max': float(np.max(results['confidences'])),
            'correct_mean': float(np.mean(results['confidences'][results['predictions'] == results['labels']])),
            'incorrect_mean': float(np.mean(results['confidences'][results['predictions'] != results['labels']])) if np.any(results['predictions'] != results['labels']) else None
        }
    }
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")

def generate_markdown_report(results, save_path):
    """ç”ŸæˆMarkdownæ ¼å¼çš„éªŒè¯æŠ¥å‘Š"""
    print("ğŸ” ç”ŸæˆMarkdownæ ¼å¼çš„éªŒè¯æŠ¥å‘Š...")
    
    # è®¡ç®—ç±»åˆ«æŒ‡æ ‡
    class_report = classification_report(
        results['labels'], 
        results['predictions'], 
        target_names=['æ— æ°”å­”', 'æœ‰æ°”å­”'],
        output_dict=True
    )
    
    # åˆ›å»ºæŠ¥å‘Šå†…å®¹
    report = f"""# ç®€åŒ–ç‰ˆæ°”å­”æ£€æµ‹å™¨éªŒè¯æŠ¥å‘Š

## éªŒè¯æ—¶é—´
{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ€»ä½“æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| å‡†ç¡®ç‡ | {results['metrics']['accuracy']:.4f} |
| ç²¾ç¡®ç‡ | {results['metrics']['precision']:.4f} |
| å¬å›ç‡ | {results['metrics']['recall']:.4f} |
| F1åˆ†æ•° | {results['metrics']['f1']:.4f} |
| æ ·æœ¬æ€»æ•° | {len(results['labels'])} |
| æ­£ç¡®é¢„æµ‹æ•° | {int(np.sum(results['predictions'] == results['labels']))} |
| é”™è¯¯é¢„æµ‹æ•° | {int(np.sum(results['predictions'] != results['labels']))} |

## ç±»åˆ«æ€§èƒ½æŒ‡æ ‡

### æ— æ°”å­”ç±»åˆ«

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| ç²¾ç¡®ç‡ | {class_report['æ— æ°”å­”']['precision']:.4f} |
| å¬å›ç‡ | {class_report['æ— æ°”å­”']['recall']:.4f} |
| F1åˆ†æ•° | {class_report['æ— æ°”å­”']['f1-score']:.4f} |
| æ”¯æŒåº¦ | {class_report['æ— æ°”å­”']['support']} |

### æœ‰æ°”å­”ç±»åˆ«

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| ç²¾ç¡®ç‡ | {class_report['æœ‰æ°”å­”']['precision']:.4f} |
| å¬å›ç‡ | {class_report['æœ‰æ°”å­”']['recall']:.4f} |
| F1åˆ†æ•° | {class_report['æœ‰æ°”å­”']['f1-score']:.4f} |
| æ”¯æŒåº¦ | {class_report['æœ‰æ°”å­”']['support']} |

## ç½®ä¿¡åº¦åˆ†æ

| ç»Ÿè®¡é‡ | å€¼ |
|--------|-----|
| å¹³å‡ç½®ä¿¡åº¦ | {np.mean(results['confidences']):.4f} |
| ä¸­ä½æ•°ç½®ä¿¡åº¦ | {np.median(results['confidences']):.4f} |
| æœ€å°ç½®ä¿¡åº¦ | {np.min(results['confidences']):.4f} |
| æœ€å¤§ç½®ä¿¡åº¦ | {np.max(results['confidences']):.4f} |
| æ­£ç¡®é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦ | {np.mean(results['confidences'][results['predictions'] == results['labels']]):.4f} |
"""

    # å¦‚æœæœ‰é”™è¯¯é¢„æµ‹ï¼Œæ·»åŠ é”™è¯¯é¢„æµ‹çš„å¹³å‡ç½®ä¿¡åº¦
    if np.any(results['predictions'] != results['labels']):
        report += f"| é”™è¯¯é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦ | {np.mean(results['confidences'][results['predictions'] != results['labels']]):.4f} |\n"
    
    # æ·»åŠ æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(results['labels'], results['predictions'])
    report += f"""
## æ··æ·†çŸ©é˜µ

|  | é¢„æµ‹: æ— æ°”å­” | é¢„æµ‹: æœ‰æ°”å­” |
|-----------------|--------------|--------------|
| **çœŸå®: æ— æ°”å­”** | {cm[0][0]} | {cm[0][1]} |
| **çœŸå®: æœ‰æ°”å­”** | {cm[1][0]} | {cm[1][1]} |

## é”™è¯¯åˆ†æ

### å‡é˜³æ€§æ ·æœ¬ (é¢„æµ‹ä¸ºæœ‰æ°”å­”ï¼Œå®é™…æ— æ°”å­”)
- æ•°é‡: {cm[0][1]}
- å¯èƒ½åŸå› : å›¾åƒä¸­çš„å™ªå£°æˆ–å…‰ç…§å˜åŒ–è¢«è¯¯è¯†åˆ«ä¸ºæ°”å­”

### å‡é˜´æ€§æ ·æœ¬ (é¢„æµ‹ä¸ºæ— æ°”å­”ï¼Œå®é™…æœ‰æ°”å­”)
- æ•°é‡: {cm[1][0]}
- å¯èƒ½åŸå› : æ°”å­”å¤ªå°æˆ–å¯¹æ¯”åº¦ä¸è¶³ï¼Œå¯¼è‡´ç‰¹å¾ä¸æ˜æ˜¾

## ç»“è®ºä¸å»ºè®®

- æ¨¡å‹æ€»ä½“è¡¨ç°: {"ä¼˜ç§€" if results['metrics']['accuracy'] > 0.95 else "è‰¯å¥½" if results['metrics']['accuracy'] > 0.85 else "ä¸€èˆ¬"}
- ä¸»è¦é—®é¢˜: {"å‡é˜³æ€§è¾ƒå¤š" if cm[0][1] > cm[1][0] else "å‡é˜´æ€§è¾ƒå¤š" if cm[1][0] > cm[0][1] else "å‡é˜³æ€§å’Œå‡é˜´æ€§å‡è¡¡"}
- æ”¹è¿›å»ºè®®:
  - {"å¢å¼ºæ•°æ®å¢å¼ºä»¥å‡å°‘å‡é˜³æ€§" if cm[0][1] > cm[1][0] else "å¢å¼ºå¯¹å°æ°”å­”çš„æ£€æµ‹èƒ½åŠ›" if cm[1][0] > cm[0][1] else "å¹³è¡¡æ¨¡å‹çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡"}
  - è€ƒè™‘è°ƒæ•´æ¨¡å‹ç»“æ„æˆ–å‚æ•°ä»¥æé«˜æ€§èƒ½
  - å¢åŠ æ›´å¤šçš„è®­ç»ƒæ ·æœ¬ï¼Œç‰¹åˆ«æ˜¯éš¾ä»¥åˆ†ç±»çš„è¾¹ç¼˜æ¡ˆä¾‹
"""
    
    # ä¿å­˜ä¸ºMarkdownæ–‡ä»¶
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… Markdownæ ¼å¼çš„éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” ç®€åŒ–ç‰ˆæ°”å­”æ£€æµ‹å™¨éªŒè¯")
    print("=" * 50)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # è·å–æœ€æ–°å®éªŒè·¯å¾„
    model_name = 'simplified_airbubble_detector'
    experiment_path = get_latest_experiment_path(model_name)
    
    if experiment_path is None:
        print(f"âŒ æœªæ‰¾åˆ°{model_name}çš„å®éªŒç›®å½•")
        return
    
    print(f"ğŸ“ å®éªŒè·¯å¾„: {experiment_path}")
    
    # åŠ è½½æ¨¡å‹
    model_path = os.path.join(experiment_path, 'best_model.pth')
    model = load_model(model_path)
    
    if model is None:
        return
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
    data_loaders = create_data_loaders(
        str(DATA_DIR),
        batch_size=32,
        num_workers=2
    )
    
    # è¯„ä¼°æ¨¡å‹
    print("ğŸ§ª åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
    results = evaluate_model(model, data_loaders['test'], device)
    
    # åˆ›å»ºéªŒè¯ç›®å½•
    validation_dir = os.path.join(experiment_path, 'validation')
    os.makedirs(validation_dir, exist_ok=True)
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plot_confusion_matrix(
        results['labels'],
        results['predictions'],
        os.path.join(validation_dir, 'confusion_matrix.png')
    )
    
    # ç»˜åˆ¶ç½®ä¿¡åº¦åˆ†å¸ƒ
    plot_confidence_distribution(
        results['confidences'],
        results['labels'],
        results['predictions'],
        os.path.join(validation_dir, 'confidence_distribution.png')
    )
    
    # å¯è§†åŒ–æ­£ç¡®æ ·æœ¬
    visualize_samples(
        results['images'],
        results['labels'],
        results['predictions'],
        results['confidences'],
        os.path.join(validation_dir, 'correct_samples'),
        'correct',
        num_samples=10
    )
    
    # å¯è§†åŒ–é”™è¯¯æ ·æœ¬
    visualize_samples(
        results['images'],
        results['labels'],
        results['predictions'],
        results['confidences'],
        os.path.join(validation_dir, 'incorrect_samples'),
        'incorrect',
        num_samples=10
    )
    
    # ç”Ÿæˆé”™è¯¯æ ·æœ¬åˆ—è¡¨
    generate_error_list(
        results['file_paths'],
        results['labels'],
        results['predictions'],
        results['confidences'],
        os.path.join(validation_dir, 'error_list.json')
    )
    
    # ç”ŸæˆéªŒè¯æŠ¥å‘Š
    generate_validation_report(
        results,
        os.path.join(validation_dir, 'validation_report.json')
    )
    
    # ç”ŸæˆMarkdownæ ¼å¼çš„éªŒè¯æŠ¥å‘Š
    generate_markdown_report(
        results,
        os.path.join(validation_dir, 'validation_report.md')
    )
    
    print("\nâœ… éªŒè¯å®Œæˆ")
    print(f"ğŸ“ éªŒè¯ç»“æœä¿å­˜åˆ°: {validation_dir}")

if __name__ == "__main__":
    main()