"""
é€šç”¨æ¨¡å‹è½¬æ¢è„šæœ¬ - å°†ä»»æ„æ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼
æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹åŠ è½½å’Œè½¬æ¢é€»è¾‘
"""

import os
import sys
import torch
import logging
import argparse
import numpy as np
import onnx
import onnxruntime as ort
from pathlib import Path
import importlib

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

class ModelConverter:
    def __init__(self, output_dir="deployment/onnx_models"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.output_dir = output_dir
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
    
    def load_model(self, model_path, model_class, model_args=None):
        """
        åŠ è½½æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
            model_class: æ¨¡å‹ç±»æˆ–åˆ›å»ºå‡½æ•°
            model_args: æ¨¡å‹åˆå§‹åŒ–å‚æ•°
        
        Returns:
            åŠ è½½çš„æ¨¡å‹
        """
        try:
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            if model_args is None:
                model_args = {"num_classes": 2}
            
            model = model_class(**model_args)
            
            # åŠ è½½æ¨¡å‹æƒé‡
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # å°è¯•åŠ è½½æ¨¡å‹æƒé‡
            try:
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)
            except Exception as e:
                # å¦‚æœç›´æ¥åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤„ç†æƒé‡é”®
                state_dict = checkpoint.get('model_state_dict', checkpoint)
                new_state_dict = {}
                for k, v in state_dict.items():
                    if k.startswith('module.'):
                        new_state_dict[k[7:]] = v
                    else:
                        new_state_dict[k] = v
                model.load_state_dict(new_state_dict)
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡: {model_path}")
            model.eval()
            return model
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
            return None
    
    def convert_to_onnx(self, model, output_path, input_shape=(1, 3, 70, 70), opset_version=11):
        """
        å°†æ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼
        
        Args:
            model: PyTorchæ¨¡å‹
            output_path: ONNXæ¨¡å‹è¾“å‡ºè·¯å¾„
            input_shape: è¾“å…¥å¼ é‡å½¢çŠ¶
            opset_version: ONNXæ“ä½œé›†ç‰ˆæœ¬
        
        Returns:
            è½¬æ¢ç»“æœ
        """
        try:
            # å‡†å¤‡è¾“å…¥å¼ é‡
            dummy_input = torch.randn(*input_shape, device=self.device)
            
            # å¯¼å‡ºä¸ºONNX
            torch.onnx.export(
                model,
                dummy_input,
                output_path,
                export_params=True,
                opset_version=opset_version,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢æ¨¡å‹ -> {output_path} ({file_size:.2f} MB)")
            
            return {
                "onnx_path": output_path,
                "file_size": file_size
            }
            
        except Exception as e:
            logger.error(f"âŒ è½¬æ¢å¤±è´¥: {str(e)}")
            return None
    
    def test_onnx_model(self, onnx_path, input_shape=(1, 3, 70, 70)):
        """
        æµ‹è¯•ONNXæ¨¡å‹
        
        Args:
            onnx_path: ONNXæ¨¡å‹è·¯å¾„
            input_shape: è¾“å…¥å¼ é‡å½¢çŠ¶
        
        Returns:
            æµ‹è¯•ç»“æœ
        """
        try:
            # åŠ è½½ONNXæ¨¡å‹
            onnx_model = onnx.load(onnx_path)
            onnx.checker.check_model(onnx_model)
            
            # åˆ›å»ºæ¨ç†ä¼šè¯
            session = ort.InferenceSession(onnx_path)
            
            # å‡†å¤‡è¾“å…¥æ•°æ®
            input_name = session.get_inputs()[0].name
            dummy_input = np.random.randn(*input_shape).astype(np.float32)
            
            # è¿è¡Œæ¨ç†
            outputs = session.run(None, {input_name: dummy_input})
            output = outputs[0]
            
            logger.info(f"âœ… ONNXæ¨¡å‹æµ‹è¯•æˆåŠŸ: {onnx_path}")
            logger.info(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
            logger.info(f"   è¾“å‡ºèŒƒå›´: [{output.min()} {output.max()}]")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ONNXæ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}")
            return False
    
    def convert_model(self, model_name, model_class, model_path, model_args=None, input_shape=(1, 3, 70, 70)):
        """
        æ‰§è¡Œå®Œæ•´çš„æ¨¡å‹è½¬æ¢æµç¨‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            model_class: æ¨¡å‹ç±»æˆ–åˆ›å»ºå‡½æ•°
            model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
            model_args: æ¨¡å‹åˆå§‹åŒ–å‚æ•°
            input_shape: è¾“å…¥å¼ é‡å½¢çŠ¶
        
        Returns:
            è½¬æ¢ç»“æœ
        """
        logger.info(f"ğŸš€ å¼€å§‹è½¬æ¢æ¨¡å‹: {model_name}")
        
        # åŠ è½½æ¨¡å‹
        model = self.load_model(model_path, model_class, model_args)
        if model is None:
            logger.error(f"âŒ æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥!")
            return False
        
        # è½¬æ¢ä¸ºONNX
        onnx_path = os.path.join(self.output_dir, f"{model_name}.onnx")
        result = self.convert_to_onnx(model, onnx_path, input_shape)
        if result is None:
            logger.error(f"âŒ æ¨¡å‹ {model_name} è½¬æ¢å¤±è´¥!")
            return False
        
        # æµ‹è¯•ONNXæ¨¡å‹
        if self.test_onnx_model(onnx_path, input_shape):
            logger.info(f"âœ… æ¨¡å‹ {model_name} è½¬æ¢å¹¶æµ‹è¯•æˆåŠŸ!")
            return True
        else:
            logger.error(f"âŒ æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥!")
            return False

def main():
    parser = argparse.ArgumentParser(description="å°†æ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼")
    parser.add_argument("--model_name", type=str, required=True, 
                        help="æ¨¡å‹åç§°")
    parser.add_argument("--model_module", type=str, required=True, 
                        help="æ¨¡å‹æ¨¡å—è·¯å¾„ (ä¾‹å¦‚: models.coatnet)")
    parser.add_argument("--model_class", type=str, required=True, 
                        help="æ¨¡å‹ç±»åæˆ–åˆ›å»ºå‡½æ•°å (ä¾‹å¦‚: CoAtNet æˆ– create_coatnet)")
    parser.add_argument("--model_path", type=str, required=True, 
                        help="æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="deployment/onnx_models", 
                        help="ONNXæ¨¡å‹è¾“å‡ºç›®å½•")
    parser.add_argument("--input_shape", type=int, nargs="+", default=[1, 3, 70, 70], 
                        help="è¾“å…¥å¼ é‡å½¢çŠ¶ (ä¾‹å¦‚: 1 3 70 70)")
    parser.add_argument("--num_classes", type=int, default=2, 
                        help="ç±»åˆ«æ•°é‡")
    args = parser.parse_args()
    
    # å¯¼å…¥æ¨¡å‹æ¨¡å—
    try:
        module = importlib.import_module(args.model_module)
        
        # è·å–æ¨¡å‹ç±»æˆ–åˆ›å»ºå‡½æ•°
        if hasattr(module, args.model_class):
            model_class = getattr(module, args.model_class)
        else:
            logger.error(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹ç±»æˆ–åˆ›å»ºå‡½æ•°: {args.model_class}")
            return
        
        # åˆ›å»ºè½¬æ¢å™¨
        converter = ModelConverter(output_dir=args.output_dir)
        
        # æ‰§è¡Œè½¬æ¢
        success = converter.convert_model(
            model_name=args.model_name,
            model_class=model_class,
            model_path=args.model_path,
            model_args={"num_classes": args.num_classes},
            input_shape=tuple(args.input_shape)
        )
        
        if success:
            logger.info(f"\nğŸ‰ æ¨¡å‹ {args.model_name} è½¬æ¢æˆåŠŸ!")
            logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {converter.output_dir}")
        else:
            logger.error(f"\nâŒ æ¨¡å‹ {args.model_name} è½¬æ¢å¤±è´¥!")
        
    except Exception as e:
        logger.error(f"âŒ è½¬æ¢è¿‡ç¨‹å‡ºé”™: {str(e)}")

if __name__ == "__main__":
    main()