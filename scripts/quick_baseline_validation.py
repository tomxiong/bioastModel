"""
å¿«é€ŸåŸºçº¿æ€§èƒ½éªŒè¯è„šæœ¬
è·å–å½“å‰æœ€ä½³æ¨¡å‹çš„åŸºçº¿æ€§èƒ½æ•°æ®
"""

import json
import os
from datetime import datetime

def analyze_existing_results():
    """åˆ†æç°æœ‰å®éªŒç»“æœï¼Œè·å–åŸºçº¿æ€§èƒ½"""
    print("ğŸ” åˆ†æç°æœ‰å®éªŒç»“æœ...")
    
    experiments_dir = "experiments"
    results = []
    
    # éå†æ‰€æœ‰å®éªŒç›®å½•
    for exp_dir in os.listdir(experiments_dir):
        exp_path = os.path.join(experiments_dir, exp_dir)
        if not os.path.isdir(exp_path):
            continue
            
        # æŸ¥æ‰¾æ¨¡å‹å­ç›®å½•
        for model_dir in os.listdir(exp_path):
            model_path = os.path.join(exp_path, model_dir)
            if not os.path.isdir(model_path):
                continue
                
            # è¯»å–æµ‹è¯•ç»“æœ
            test_results_path = os.path.join(model_path, "test_results.json")
            if os.path.exists(test_results_path):
                try:
                    with open(test_results_path, 'r', encoding='utf-8') as f:
                        test_data = json.load(f)
                    
                    result = {
                        'experiment': exp_dir,
                        'model': model_dir,
                        'accuracy': test_data.get('accuracy', 0) * 100,
                        'precision': test_data.get('precision', 0) * 100,
                        'recall': test_data.get('recall', 0) * 100,
                        'f1_score': test_data.get('f1_score', 0) * 100,
                        'path': model_path
                    }
                    results.append(result)
                    
                except Exception as e:
                    print(f"âš ï¸ è¯»å– {test_results_path} å¤±è´¥: {e}")
    
    return results

def generate_baseline_report(results):
    """ç”ŸæˆåŸºçº¿æ€§èƒ½æŠ¥å‘Š"""
    if not results:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å®éªŒç»“æœ")
        return
    
    # æŒ‰å‡†ç¡®ç‡æ’åº
    results.sort(key=lambda x: x['accuracy'], reverse=True)
    
    print("\nğŸ“Š å½“å‰æ¨¡å‹æ€§èƒ½åŸºçº¿åˆ†æ")
    print("=" * 60)
    
    # æ˜¾ç¤ºæœ€ä½³æ¨¡å‹
    best_model = results[0]
    print(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model['model']}")
    print(f"   å®éªŒ: {best_model['experiment']}")
    print(f"   å‡†ç¡®ç‡: {best_model['accuracy']:.2f}%")
    print(f"   ç²¾ç¡®ç‡: {best_model['precision']:.2f}%")
    print(f"   å¬å›ç‡: {best_model['recall']:.2f}%")
    print(f"   F1åˆ†æ•°: {best_model['f1_score']:.2f}%")
    
    # è®¡ç®—å‡é˜´æ€§ç‡ï¼ˆä¼°ç®—ï¼‰
    false_negative_rate = 100 - best_model['recall']
    print(f"   å‡é˜´æ€§ç‡: {false_negative_rate:.2f}%")
    
    print(f"\nğŸ“ˆ æ‰€æœ‰æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
    print("-" * 60)
    for i, result in enumerate(results[:8], 1):  # æ˜¾ç¤ºå‰8ä¸ª
        print(f"{i:2d}. {result['model']:<20} | å‡†ç¡®ç‡: {result['accuracy']:6.2f}% | F1: {result['f1_score']:6.2f}%")
    
    # æ°”å­”æ£€æµ‹å™¨æœ‰æ•ˆæ€§è¯„ä¼°
    print(f"\nğŸ¯ æ°”å­”æ£€æµ‹å™¨æœ‰æ•ˆæ€§è¯„ä¼°:")
    print("-" * 60)
    
    # è®¾å®šç›®æ ‡
    target_accuracy = 92.0
    target_precision = 90.0
    target_recall = 88.0
    target_f1 = 89.0
    max_false_negative_rate = 12.0
    
    # è¯„ä¼°æœ€ä½³æ¨¡å‹
    accuracy_gap = target_accuracy - best_model['accuracy']
    precision_gap = target_precision - best_model['precision']
    recall_gap = target_recall - best_model['recall']
    f1_gap = target_f1 - best_model['f1_score']
    fn_rate_gap = false_negative_rate - max_false_negative_rate
    
    print(f"å½“å‰æœ€ä½³æ¨¡å‹ vs ç›®æ ‡:")
    print(f"  å‡†ç¡®ç‡: {best_model['accuracy']:6.2f}% vs {target_accuracy:6.2f}% (å·®è·: {accuracy_gap:+6.2f}%)")
    print(f"  ç²¾ç¡®ç‡: {best_model['precision']:6.2f}% vs {target_precision:6.2f}% (å·®è·: {precision_gap:+6.2f}%)")
    print(f"  å¬å›ç‡: {best_model['recall']:6.2f}% vs {target_recall:6.2f}% (å·®è·: {recall_gap:+6.2f}%)")
    print(f"  F1åˆ†æ•°: {best_model['f1_score']:6.2f}% vs {target_f1:6.2f}% (å·®è·: {f1_gap:+6.2f}%)")
    print(f"  å‡é˜´æ€§ç‡: {false_negative_rate:6.2f}% vs {max_false_negative_rate:6.2f}% (å·®è·: {fn_rate_gap:+6.2f}%)")
    
    # æœ‰æ•ˆæ€§åˆ¤æ–­
    criteria_met = 0
    total_criteria = 5
    
    if best_model['accuracy'] >= target_accuracy:
        criteria_met += 1
        print(f"  âœ… å‡†ç¡®ç‡è¾¾æ ‡")
    else:
        print(f"  âŒ å‡†ç¡®ç‡æœªè¾¾æ ‡ (éœ€æå‡ {abs(accuracy_gap):.2f}%)")
    
    if best_model['precision'] >= target_precision:
        criteria_met += 1
        print(f"  âœ… ç²¾ç¡®ç‡è¾¾æ ‡")
    else:
        print(f"  âŒ ç²¾ç¡®ç‡æœªè¾¾æ ‡ (éœ€æå‡ {abs(precision_gap):.2f}%)")
    
    if best_model['recall'] >= target_recall:
        criteria_met += 1
        print(f"  âœ… å¬å›ç‡è¾¾æ ‡")
    else:
        print(f"  âŒ å¬å›ç‡æœªè¾¾æ ‡ (éœ€æå‡ {abs(recall_gap):.2f}%)")
    
    if best_model['f1_score'] >= target_f1:
        criteria_met += 1
        print(f"  âœ… F1åˆ†æ•°è¾¾æ ‡")
    else:
        print(f"  âŒ F1åˆ†æ•°æœªè¾¾æ ‡ (éœ€æå‡ {abs(f1_gap):.2f}%)")
    
    if false_negative_rate <= max_false_negative_rate:
        criteria_met += 1
        print(f"  âœ… å‡é˜´æ€§ç‡è¾¾æ ‡")
    else:
        print(f"  âŒ å‡é˜´æ€§ç‡è¿‡é«˜ (éœ€é™ä½ {abs(fn_rate_gap):.2f}%)")
    
    # ç»¼åˆè¯„ä¼°
    effectiveness_score = (criteria_met / total_criteria) * 100
    
    print(f"\nğŸ¯ ç»¼åˆæœ‰æ•ˆæ€§è¯„ä¼°:")
    print(f"  è¾¾æ ‡æŒ‡æ ‡: {criteria_met}/{total_criteria}")
    print(f"  æœ‰æ•ˆæ€§åˆ†æ•°: {effectiveness_score:.1f}/100")
    
    if effectiveness_score >= 80:
        status = "ğŸŸ¢ åŸºæœ¬æœ‰æ•ˆ"
        recommendation = "æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œå¯è¿›è¡Œå¾®è°ƒä¼˜åŒ–"
    elif effectiveness_score >= 60:
        status = "ğŸŸ¡ éƒ¨åˆ†æœ‰æ•ˆ"
        recommendation = "éœ€è¦é’ˆå¯¹æ€§æ”¹è¿›ï¼Œé‡ç‚¹æå‡æœªè¾¾æ ‡æŒ‡æ ‡"
    else:
        status = "ğŸ”´ éœ€è¦æ”¹è¿›"
        recommendation = "å»ºè®®é‡æ–°è®­ç»ƒæˆ–è°ƒæ•´æ¶æ„"
    
    print(f"  æœ‰æ•ˆæ€§çŠ¶æ€: {status}")
    print(f"  å»ºè®®: {recommendation}")
    
    # ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’
    print(f"\nğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’:")
    print("-" * 60)
    
    if accuracy_gap > 0:
        print(f"1. ğŸ¯ æå‡å‡†ç¡®ç‡: å½“å‰ {best_model['accuracy']:.2f}% â†’ ç›®æ ‡ {target_accuracy:.2f}%")
        print(f"   - ä½¿ç”¨å¢å¼ºå‹æ°”å­”æ£€æµ‹å™¨æ¶æ„")
        print(f"   - æ”¹è¿›æ•°æ®å¢å¼ºç­–ç•¥")
    
    if fn_rate_gap > 0:
        print(f"2. âš ï¸ é™ä½å‡é˜´æ€§ç‡: å½“å‰ {false_negative_rate:.2f}% â†’ ç›®æ ‡ â‰¤{max_false_negative_rate:.2f}%")
        print(f"   - ä¼˜åŒ–å‡é˜´æ€§æ§åˆ¶æœºåˆ¶")
        print(f"   - è°ƒæ•´åˆ†ç±»é˜ˆå€¼")
    
    if recall_gap > 0:
        print(f"3. ğŸ“ˆ æå‡å¬å›ç‡: å½“å‰ {best_model['recall']:.2f}% â†’ ç›®æ ‡ {target_recall:.2f}%")
        print(f"   - å¢å¼ºæµŠåº¦åˆ†ç±»èƒ½åŠ›")
        print(f"   - æ”¹è¿›å°æ°”å­”æ£€æµ‹æ•æ„Ÿæ€§")
    
    print(f"\nâœ… åŸºçº¿éªŒè¯å®Œæˆï¼æœ€ä½³æ¨¡å‹è·¯å¾„: {best_model['path']}")
    
    return best_model

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¿«é€ŸåŸºçº¿æ€§èƒ½éªŒè¯")
    print("=" * 50)
    
    # åˆ†æç°æœ‰ç»“æœ
    results = analyze_existing_results()
    
    if results:
        # ç”ŸæˆåŸºçº¿æŠ¥å‘Š
        best_model = generate_baseline_report(results)
        
        # ä¿å­˜åŸºçº¿æ•°æ®
        baseline_data = {
            'timestamp': datetime.now().isoformat(),
            'best_model': best_model,
            'all_results': results[:10]  # ä¿å­˜å‰10ä¸ªç»“æœ
        }
        
        os.makedirs('experiments/baseline_validation', exist_ok=True)
        with open('experiments/baseline_validation/baseline_performance.json', 'w', encoding='utf-8') as f:
            json.dump(baseline_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ åŸºçº¿æ•°æ®å·²ä¿å­˜: experiments/baseline_validation/baseline_performance.json")
    else:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å®éªŒç»“æœï¼Œè¯·å…ˆè¿è¡Œæ¨¡å‹è®­ç»ƒ")

if __name__ == "__main__":
    main()