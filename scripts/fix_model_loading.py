#!/usr/bin/env python3
"""
ä¿®å¤æ¨¡å‹åŠ è½½é—®é¢˜ - å¤„ç†å¸¦æœ‰base_modelå‰ç¼€çš„æƒé‡æ–‡ä»¶
"""

import os
import sys
import json
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, classification_report
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from training.dataset import BioastDataset
from models.mic_mobilenetv3 import MIC_MobileNetV3
from models.micro_vit import MicroViT
from models.airbubble_hybrid_net import AirBubbleHybridNet

class FixedModelTester:
    def __init__(self, data_dir="bioast_dataset"):
        self.data_dir = data_dir
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ•°æ®é¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize((70, 70)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # åŠ è½½æµ‹è¯•æ•°æ®é›†
        self.test_dataset = BioastDataset(
            data_dir=self.data_dir,
            split='test',
            transform=self.transform
        )
        self.test_loader = DataLoader(
            self.test_dataset,
            batch_size=32,
            shuffle=False,
            num_workers=4
        )
        
        print(f"æµ‹è¯•é›†å¤§å°: {len(self.test_dataset)}")
    
    def fix_state_dict_keys(self, state_dict, has_base_model_prefix=True):
        """ä¿®å¤çŠ¶æ€å­—å…¸çš„é”®å"""
        if has_base_model_prefix:
            # ç§»é™¤ base_model. å‰ç¼€
            new_state_dict = {}
            for key, value in state_dict.items():
                if key.startswith('base_model.'):
                    new_key = key[11:]  # ç§»é™¤ 'base_model.' å‰ç¼€
                    new_state_dict[new_key] = value
                else:
                    new_state_dict[key] = value
            return new_state_dict
        else:
            return state_dict
    
    def load_model_with_fix(self, model_name, model_path):
        """åŠ è½½æ¨¡å‹å¹¶ä¿®å¤æƒé‡é”®åé—®é¢˜"""
        print(f"åŠ è½½æ¨¡å‹: {model_name}")
        
        # æ ¹æ®æ¨¡å‹åç§°åˆ›å»ºæ¨¡å‹å®ä¾‹
        if model_name == 'mic_mobilenetv3':
            model = MIC_MobileNetV3(num_classes=2)
        elif model_name == 'micro_vit':
            model = MicroViT(num_classes=2)
        elif model_name == 'airbubble_hybrid_net':
            model = AirBubbleHybridNet(num_classes=2)
        else:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹åç§°: {model_name}")
        
        # åŠ è½½æ¨¡å‹æƒé‡
        checkpoint = torch.load(model_path, map_location=self.device)
        
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        else:
            state_dict = checkpoint
        
        # æ£€æŸ¥æ˜¯å¦æœ‰base_modelå‰ç¼€
        has_base_model_prefix = any(key.startswith('base_model.') for key in state_dict.keys())
        
        if has_base_model_prefix:
            print(f"æ£€æµ‹åˆ°base_modelå‰ç¼€ï¼Œæ­£åœ¨ä¿®å¤...")
            state_dict = self.fix_state_dict_keys(state_dict, True)
        
        # å°è¯•åŠ è½½æƒé‡
        try:
            model.load_state_dict(state_dict, strict=True)
            print(f"âœ… æˆåŠŸåŠ è½½æƒé‡")
        except RuntimeError as e:
            print(f"âŒ ä¸¥æ ¼æ¨¡å¼åŠ è½½å¤±è´¥: {e}")
            # å°è¯•éä¸¥æ ¼æ¨¡å¼
            try:
                missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
                print(f"âš ï¸ éä¸¥æ ¼æ¨¡å¼åŠ è½½æˆåŠŸ")
                if missing_keys:
                    print(f"ç¼ºå¤±çš„é”®: {len(missing_keys)} ä¸ª")
                if unexpected_keys:
                    print(f"æ„å¤–çš„é”®: {len(unexpected_keys)} ä¸ª")
            except Exception as e2:
                print(f"âŒ éä¸¥æ ¼æ¨¡å¼ä¹Ÿå¤±è´¥: {e2}")
                raise e2
        
        model = model.to(self.device)
        model.eval()
        
        return model
    
    def evaluate_model(self, model):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        all_preds = []
        all_labels = []
        all_probs = []
        
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(self.test_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                outputs = model(data)
                
                # å¤„ç†æ¨¡å‹è¾“å‡ºå¯èƒ½æ˜¯å­—å…¸çš„æƒ…å†µ
                if isinstance(outputs, dict):
                    print(f"æ¨¡å‹è¾“å‡ºæ˜¯å­—å…¸ï¼Œé”®: {list(outputs.keys())}")
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•è·å–logitsæˆ–ä¸»è¦è¾“å‡º
                    if 'logits' in outputs:
                        outputs = outputs['logits']
                    elif 'output' in outputs:
                        outputs = outputs['output']
                    elif 'classification' in outputs:
                        outputs = outputs['classification']
                    elif 'pred' in outputs:
                        outputs = outputs['pred']
                    else:
                        # å–å­—å…¸ä¸­ç¬¬ä¸€ä¸ªå¼ é‡å€¼
                        for key, value in outputs.items():
                            if isinstance(value, torch.Tensor) and value.dim() == 2:
                                outputs = value
                                print(f"ä½¿ç”¨é”® '{key}' çš„è¾“å‡º: {outputs.shape}")
                                break
                        else:
                            raise ValueError(f"æ— æ³•ä»å­—å…¸è¾“å‡ºä¸­æ‰¾åˆ°åˆé€‚çš„å¼ é‡: {outputs.keys()}")
                
                # ç¡®ä¿outputsæ˜¯å¼ é‡
                if not isinstance(outputs, torch.Tensor):
                    raise TypeError(f"æ¨¡å‹è¾“å‡ºä¸æ˜¯å¼ é‡: {type(outputs)}")
                
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)
                
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(target.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
                
                if batch_idx % 10 == 0:
                    print(f"å¤„ç†æ‰¹æ¬¡: {batch_idx}/{len(self.test_loader)}")
        
        # è®¡ç®—æŒ‡æ ‡
        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)
        all_probs = np.array(all_probs)
        
        accuracy = accuracy_score(all_labels, all_preds)
        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')
        precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)
        
        # AUCè®¡ç®—
        try:
            auc = roc_auc_score(all_labels, all_probs[:, 1])
        except:
            auc = 0.0
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_labels, all_preds)
        
        # æ•æ„Ÿæ€§å’Œç‰¹å¼‚æ€§
        tn, fp, fn, tp = cm.ravel()
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        
        # åˆ†ç±»æŠ¥å‘Š
        class_report = classification_report(all_labels, all_preds, target_names=['negative', 'positive'])
        
        results = {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1),
            'auc': float(auc),
            'sensitivity': float(sensitivity),
            'specificity': float(specificity),
            'confusion_matrix': cm.tolist(),
            'precision_per_class': precision_per_class.tolist(),
            'recall_per_class': recall_per_class.tolist(),
            'f1_per_class': f1_per_class.tolist(),
            'classification_report': class_report,
            'f1': float(f1)  # å…¼å®¹æ€§å­—æ®µ
        }
        
        return results
    
    def test_failed_models(self):
        """æµ‹è¯•ä¹‹å‰å¤±è´¥çš„æ¨¡å‹"""
        failed_experiments = [
            ('experiments/experiment_20250803_101438/mic_mobilenetv3', 'mic_mobilenetv3'),
            ('experiments/experiment_20250803_102845/micro_vit', 'micro_vit'),
            ('experiments/experiment_20250803_115344/airbubble_hybrid_net', 'airbubble_hybrid_net')
        ]
        
        success_count = 0
        
        for experiment_path, model_name in failed_experiments:
            print(f"\n{'='*60}")
            print(f"ä¿®å¤æµ‹è¯•: {experiment_path}")
            print(f"æ¨¡å‹: {model_name}")
            
            model_path = os.path.join(experiment_path, 'best_model.pth')
            results_path = os.path.join(experiment_path, 'test_results.json')
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æµ‹è¯•ç»“æœ
            if os.path.exists(results_path):
                print(f"æµ‹è¯•ç»“æœå·²å­˜åœ¨: {results_path}")
                success_count += 1
                continue
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(model_path):
                print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                continue
            
            try:
                # åŠ è½½å’Œæµ‹è¯•æ¨¡å‹
                model = self.load_model_with_fix(model_name, model_path)
                results = self.evaluate_model(model)
                
                # ä¿å­˜ç»“æœ
                with open(results_path, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=2, ensure_ascii=False)
                
                print(f"âœ… æµ‹è¯•å®Œæˆ! å‡†ç¡®ç‡: {results['accuracy']:.4f}")
                print(f"ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
                success_count += 1
                
            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
        
        print(f"\n{'='*60}")
        print(f"ä¿®å¤æµ‹è¯•å®Œæˆ! æˆåŠŸ: {success_count}/{len(failed_experiments)}")
        
        return success_count == len(failed_experiments)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ å¼€å§‹ä¿®å¤å¤±è´¥çš„æ¨¡å‹æµ‹è¯•...")
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    tester = FixedModelTester()
    success = tester.test_failed_models()
    
    if success:
        print("ğŸ‰ æ‰€æœ‰å¤±è´¥çš„æ¨¡å‹éƒ½å·²æˆåŠŸä¿®å¤!")
    else:
        print("âš ï¸ éƒ¨åˆ†æ¨¡å‹ä»ç„¶å­˜åœ¨é—®é¢˜")

if __name__ == "__main__":
    main()