"""
æµŠåº¦åˆ†ç±»ç²¾åº¦æå‡è„šæœ¬
ç›®æ ‡ï¼šå°†æµŠåº¦åˆ†ç±»å‡†ç¡®ç‡ä»88%æå‡è‡³92%+
é‡ç‚¹ï¼šå¤šå°ºåº¦ç‰¹å¾èåˆå’Œè‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torch.nn.functional as F
import numpy as np
import os
import json
from datetime import datetime
import logging
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import cv2
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.enhanced_airbubble_detector import EnhancedAirBubbleDetector
from core.data_loader import MICDataLoader

class MultiScaleFeatureFusion(nn.Module):
    """å¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å—"""
    
    def __init__(self, in_channels: int):
        super().__init__()
        
        # ä¸åŒå°ºåº¦çš„ç‰¹å¾æå–
        self.scale1 = nn.Sequential(
            nn.Conv2d(in_channels, 64, 1),  # 1x1å·ç§¯
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        
        self.scale2 = nn.Sequential(
            nn.Conv2d(in_channels, 64, 3, padding=1),  # 3x3å·ç§¯
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        
        self.scale3 = nn.Sequential(
            nn.Conv2d(in_channels, 64, 5, padding=2),  # 5x5å·ç§¯
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        
        # å…¨å±€å¹³å‡æ± åŒ–åˆ†æ”¯
        self.global_branch = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(in_channels, 64, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        
        # ç‰¹å¾èåˆ
        self.fusion = nn.Sequential(
            nn.Conv2d(256, 128, 1),  # 4ä¸ªåˆ†æ”¯ * 64 = 256
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, in_channels, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # è·å–è¾“å…¥å°ºå¯¸
        b, c, h, w = x.size()
        
        # å¤šå°ºåº¦ç‰¹å¾æå–
        feat1 = self.scale1(x)
        feat2 = self.scale2(x)
        feat3 = self.scale3(x)
        
        # å…¨å±€ç‰¹å¾
        global_feat = self.global_branch(x)
        global_feat = F.interpolate(global_feat, size=(h, w), mode='bilinear', align_corners=False)
        
        # ç‰¹å¾æ‹¼æ¥
        fused_features = torch.cat([feat1, feat2, feat3, global_feat], dim=1)
        
        # æ³¨æ„åŠ›æƒé‡
        attention = self.fusion(fused_features)
        
        # åº”ç”¨æ³¨æ„åŠ›
        enhanced_features = x * attention
        
        return enhanced_features

class AdaptiveThresholdModule(nn.Module):
    """è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´æ¨¡å—"""
    
    def __init__(self, num_classes: int = 5):
        super().__init__()
        self.num_classes = num_classes
        
        # é˜ˆå€¼é¢„æµ‹ç½‘ç»œ
        self.threshold_predictor = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, num_classes - 1),  # n-1ä¸ªé˜ˆå€¼
            nn.Sigmoid()
        )
        
        # åˆå§‹åŒ–é˜ˆå€¼
        self.register_buffer('base_thresholds', torch.linspace(0.2, 0.8, num_classes - 1))
    
    def forward(self, features, logits):
        # é¢„æµ‹é˜ˆå€¼è°ƒæ•´
        threshold_adjustments = self.threshold_predictor(features)
        
        # è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼
        adaptive_thresholds = self.base_thresholds + 0.2 * (threshold_adjustments - 0.5)
        adaptive_thresholds = torch.clamp(adaptive_thresholds, 0.1, 0.9)
        
        return adaptive_thresholds

class EnhancedTurbidityClassifier(nn.Module):
    """å¢å¼ºå‹æµŠåº¦åˆ†ç±»å™¨"""
    
    def __init__(self, base_model: nn.Module, num_classes: int = 5):
        super().__init__()
        self.base_model = base_model
        self.num_classes = num_classes
        
        # å¤šå°ºåº¦ç‰¹å¾èåˆ
        self.feature_fusion = MultiScaleFeatureFusion(256)
        
        # è‡ªé€‚åº”é˜ˆå€¼æ¨¡å—
        self.adaptive_threshold = AdaptiveThresholdModule(num_classes)
        
        # æµŠåº¦ç‰¹å¾å¢å¼ºå™¨
        self.turbidity_enhancer = nn.Sequential(
            nn.Conv2d(256, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten()
        )
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(32, num_classes)
        )
        
        # ç½®ä¿¡åº¦ä¼°è®¡
        self.confidence_estimator = nn.Sequential(
            nn.Linear(64, 16),
            nn.ReLU(inplace=True),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # åŸºç¡€ç‰¹å¾æå–
        if hasattr(self.base_model, 'backbone'):
            features = self.base_model.backbone(x)
        else:
            # å‡è®¾base_modelè¿”å›å­—å…¸
            outputs = self.base_model(x)
            features = outputs.get('features', x)
        
        # å¤šå°ºåº¦ç‰¹å¾èåˆ
        enhanced_features = self.feature_fusion(features)
        
        # æµŠåº¦ç‰¹å¾å¢å¼º
        turbidity_features = self.turbidity_enhancer(enhanced_features)
        
        # åˆ†ç±»é¢„æµ‹
        logits = self.classifier(turbidity_features)
        
        # è‡ªé€‚åº”é˜ˆå€¼
        adaptive_thresholds = self.adaptive_threshold(enhanced_features, logits)
        
        # ç½®ä¿¡åº¦ä¼°è®¡
        confidence = self.confidence_estimator(turbidity_features)
        
        return {
            'logits': logits,
            'adaptive_thresholds': adaptive_thresholds,
            'confidence': confidence,
            'features': turbidity_features
        }

class TurbidityClassificationTrainer:
    """æµŠåº¦åˆ†ç±»è®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.setup_model()
        
        # è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.setup_optimizer()
        self.setup_loss_functions()
        
        # æ•°æ®åŠ è½½å™¨
        self.setup_data_loaders()
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': [],
            'learning_rates': []
        }
        
        self.best_val_acc = 0.0
        self.patience_counter = 0
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = "experiments/turbidity_classification"
        os.makedirs(log_dir, exist_ok=True)
        
        log_file = os.path.join(log_dir, f"turbidity_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        # åŠ è½½é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹
        base_model = EnhancedAirBubbleDetector()
        
        # å¦‚æœæœ‰é¢„è®­ç»ƒæƒé‡ï¼ŒåŠ è½½å®ƒä»¬
        if 'pretrained_model_path' in self.config:
            pretrained_path = self.config['pretrained_model_path']
            if os.path.exists(pretrained_path):
                checkpoint = torch.load(pretrained_path, map_location=self.device)
                base_model.load_state_dict(checkpoint['model_state_dict'])
                self.logger.info(f"Loaded pretrained model from {pretrained_path}")
        
        # åˆ›å»ºå¢å¼ºå‹æµŠåº¦åˆ†ç±»å™¨
        self.model = EnhancedTurbidityClassifier(
            base_model=base_model,
            num_classes=self.config.get('num_classes', 5)
        ).to(self.device)
        
        self.logger.info(f"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}")
    
    def setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.get('learning_rate', 0.001),
            weight_decay=self.config.get('weight_decay', 1e-4)
        )
        
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=self.config.get('T_0', 10),
            T_mult=self.config.get('T_mult', 2),
            eta_min=self.config.get('eta_min', 1e-6)
        )
    
    def setup_loss_functions(self):
        """è®¾ç½®æŸå¤±å‡½æ•°"""
        # ä¸»è¦åˆ†ç±»æŸå¤±
        self.classification_loss = nn.CrossEntropyLoss(
            label_smoothing=self.config.get('label_smoothing', 0.1)
        )
        
        # ç½®ä¿¡åº¦æŸå¤±
        self.confidence_loss = nn.MSELoss()
        
        # é˜ˆå€¼ä¸€è‡´æ€§æŸå¤±
        self.threshold_consistency_loss = nn.MSELoss()
    
    def setup_data_loaders(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        data_loader = MICDataLoader()
        
        # è·å–æ•°æ®
        train_images, train_labels = data_loader.get_train_data()
        val_images, val_labels = data_loader.get_val_data()
        test_images, test_labels = data_loader.get_test_data()
        
        # è½¬æ¢ä¸ºæµŠåº¦åˆ†ç±»æ ‡ç­¾ï¼ˆå‡è®¾åŸå§‹æ ‡ç­¾éœ€è¦è½¬æ¢ï¼‰
        train_turbidity_labels = self.convert_to_turbidity_labels(train_labels)
        val_turbidity_labels = self.convert_to_turbidity_labels(val_labels)
        test_turbidity_labels = self.convert_to_turbidity_labels(test_labels)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TurbidityDataset(train_images, train_turbidity_labels, augment=True)
        val_dataset = TurbidityDataset(val_images, val_turbidity_labels, augment=False)
        test_dataset = TurbidityDataset(test_images, test_turbidity_labels, augment=False)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.get('batch_size', 32),
            shuffle=True,
            num_workers=4,
            pin_memory=True
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.get('batch_size', 32),
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        self.test_loader = DataLoader(
            test_dataset,
            batch_size=self.config.get('batch_size', 32),
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        self.logger.info(f"Train samples: {len(train_dataset)}")
        self.logger.info(f"Val samples: {len(val_dataset)}")
        self.logger.info(f"Test samples: {len(test_dataset)}")
    
    def convert_to_turbidity_labels(self, original_labels):
        """å°†åŸå§‹æ ‡ç­¾è½¬æ¢ä¸ºæµŠåº¦åˆ†ç±»æ ‡ç­¾"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µå®ç°æ ‡ç­¾è½¬æ¢é€»è¾‘
        # å‡è®¾æˆ‘ä»¬æœ‰5ä¸ªæµŠåº¦ç­‰çº§ï¼š0(æ¸…æ¾ˆ), 1(è½»å¾®), 2(ä¸­ç­‰), 3(æµ‘æµŠ), 4(éå¸¸æµ‘æµŠ)
        
        # ç®€åŒ–å®ç°ï¼šéšæœºåˆ†é…æµŠåº¦ç­‰çº§ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®å›¾åƒç‰¹å¾åˆ¤æ–­ï¼‰
        turbidity_labels = []
        for label in original_labels:
            # è¿™é‡Œåº”è¯¥æœ‰å®é™…çš„æµŠåº¦åˆ¤æ–­é€»è¾‘
            # æš‚æ—¶ä½¿ç”¨ç®€å•çš„æ˜ å°„
            if label == 0:  # å‡è®¾0è¡¨ç¤ºæ— æ°”å­”ï¼Œå¯èƒ½æµŠåº¦è¾ƒä½
                turbidity_level = np.random.choice([0, 1, 2], p=[0.5, 0.3, 0.2])
            else:  # æœ‰æ°”å­”ï¼Œå¯èƒ½æµŠåº¦è¾ƒé«˜
                turbidity_level = np.random.choice([2, 3, 4], p=[0.3, 0.4, 0.3])
            
            turbidity_labels.append(turbidity_level)
        
        return np.array(turbidity_labels)
    
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (images, labels) in enumerate(self.train_loader):
            images = images.to(self.device)
            labels = labels.to(self.device)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(images)
            logits = outputs['logits']
            confidence = outputs['confidence']
            
            # è®¡ç®—æŸå¤±
            cls_loss = self.classification_loss(logits, labels)
            
            # ç½®ä¿¡åº¦æŸå¤±ï¼ˆé«˜ç½®ä¿¡åº¦å¯¹åº”é«˜å‡†ç¡®ç‡ï¼‰
            _, predicted = torch.max(logits.data, 1)
            correct_predictions = (predicted == labels).float()
            conf_loss = self.confidence_loss(confidence.squeeze(), correct_predictions)
            
            # æ€»æŸå¤±
            total_loss_batch = cls_loss + 0.1 * conf_loss
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            total_loss_batch.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += total_loss_batch.item()
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            if batch_idx % 50 == 0:
                self.logger.info(
                    f'Batch {batch_idx}/{len(self.train_loader)}, '
                    f'Loss: {total_loss_batch.item():.4f}, '
                    f'Acc: {100.*correct/total:.2f}%'
                )
        
        avg_loss = total_loss / len(self.train_loader)
        accuracy = 100. * correct / total
        
        return {'loss': avg_loss, 'accuracy': accuracy}
    
    def validate_epoch(self) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        all_predictions = []
        all_labels = []
        all_confidences = []
        
        with torch.no_grad():
            for images, labels in self.val_loader:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(images)
                logits = outputs['logits']
                confidence = outputs['confidence']
                
                # è®¡ç®—æŸå¤±
                loss = self.classification_loss(logits, labels)
                total_loss += loss.item()
                
                # é¢„æµ‹
                _, predicted = torch.max(logits.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_confidences.extend(confidence.cpu().numpy())
        
        avg_loss = total_loss / len(self.val_loader)
        accuracy = 100. * correct / total
        
        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        precision = precision_score(all_labels, all_predictions, average='weighted') * 100
        recall = recall_score(all_labels, all_predictions, average='weighted') * 100
        f1 = f1_score(all_labels, all_predictions, average='weighted') * 100
        avg_confidence = np.mean(all_confidences) * 100
        
        return {
            'loss': avg_loss,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'confidence': avg_confidence
        }
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        self.logger.info("Starting Turbidity Classification Training...")
        self.logger.info(f"Target: Improve accuracy from 88% to 92%+")
        
        num_epochs = self.config.get('num_epochs', 100)
        patience = self.config.get('patience', 15)
        
        for epoch in range(num_epochs):
            self.logger.info(f"\nEpoch {epoch+1}/{num_epochs}")
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch()
            
            # éªŒè¯
            val_metrics = self.validate_epoch()
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•å†å²
            self.training_history['train_loss'].append(train_metrics['loss'])
            self.training_history['val_loss'].append(val_metrics['loss'])
            self.training_history['train_acc'].append(train_metrics['accuracy'])
            self.training_history['val_acc'].append(val_metrics['accuracy'])
            self.training_history['learning_rates'].append(current_lr)
            
            # æ—¥å¿—è¾“å‡º
            self.logger.info(
                f"Train - Loss: {train_metrics['loss']:.4f}, "
                f"Acc: {train_metrics['accuracy']:.2f}%"
            )
            self.logger.info(
                f"Val - Loss: {val_metrics['loss']:.4f}, "
                f"Acc: {val_metrics['accuracy']:.2f}%, "
                f"Precision: {val_metrics['precision']:.2f}%, "
                f"Recall: {val_metrics['recall']:.2f}%, "
                f"F1: {val_metrics['f1']:.2f}%, "
                f"Confidence: {val_metrics['confidence']:.2f}%"
            )
            self.logger.info(f"Learning Rate: {current_lr:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['accuracy'] > self.best_val_acc:
                self.best_val_acc = val_metrics['accuracy']
                self.patience_counter = 0
                self.save_best_model(epoch, val_metrics)
                self.logger.info(f"New best validation accuracy: {self.best_val_acc:.2f}%")
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                if val_metrics['accuracy'] >= 92.0:
                    self.logger.info("ğŸ‰ TARGET ACHIEVED! Accuracy >= 92%")
            else:
                self.patience_counter += 1
            
            # æ—©åœæ£€æŸ¥
            if self.patience_counter >= patience:
                self.logger.info(f"Early stopping triggered after {epoch+1} epochs")
                break
        
        # æœ€ç»ˆæµ‹è¯•
        self.final_evaluation()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_training_report()
    
    def save_best_model(self, epoch: int, metrics: Dict[str, float]):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        save_dir = "experiments/turbidity_classification"
        os.makedirs(save_dir, exist_ok=True)
        
        model_path = os.path.join(save_dir, "best_turbidity_classifier.pth")
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'metrics': metrics,
            'config': self.config,
            'training_history': self.training_history
        }
        
        torch.save(checkpoint, model_path)
        self.logger.info(f"Best model saved: {model_path}")
    
    def final_evaluation(self):
        """æœ€ç»ˆè¯„ä¼°"""
        self.logger.info("Performing final evaluation on test set...")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        model_path = "experiments/turbidity_classification/best_turbidity_classifier.pth"
        if os.path.exists(model_path):
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
        
        self.model.eval()
        correct = 0
        total = 0
        all_predictions = []
        all_labels = []
        all_confidences = []
        
        with torch.no_grad():
            for images, labels in self.test_loader:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(images)
                logits = outputs['logits']
                confidence = outputs['confidence']
                
                _, predicted = torch.max(logits.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_confidences.extend(confidence.cpu().numpy())
        
        test_accuracy = 100. * correct / total
        test_precision = precision_score(all_labels, all_predictions, average='weighted') * 100
        test_recall = recall_score(all_labels, all_predictions, average='weighted') * 100
        test_f1 = f1_score(all_labels, all_predictions, average='weighted') * 100
        avg_confidence = np.mean(all_confidences) * 100
        
        self.logger.info("=== FINAL TEST RESULTS ===")
        self.logger.info(f"Accuracy: {test_accuracy:.2f}%")
        self.logger.info(f"Precision: {test_precision:.2f}%")
        self.logger.info(f"Recall: {test_recall:.2f}%")
        self.logger.info(f"F1-Score: {test_f1:.2f}%")
        self.logger.info(f"Average Confidence: {avg_confidence:.2f}%")
        
        # ç›®æ ‡è¾¾æˆæ£€æŸ¥
        baseline_acc = 88.0
        target_acc = 92.0
        improvement = test_accuracy - baseline_acc
        
        self.logger.info(f"\n=== TARGET ACHIEVEMENT ===")
        self.logger.info(f"Baseline Accuracy: {baseline_acc:.2f}%")
        self.logger.info(f"Current Accuracy: {test_accuracy:.2f}%")
        self.logger.info(f"Improvement: +{improvement:.2f}%")
        self.logger.info(f"Target Accuracy: â‰¥ {target_acc:.2f}%")
        
        if test_accuracy >= target_acc:
            self.logger.info("âœ… TARGET ACHIEVED!")
        else:
            self.logger.info("âŒ Target not achieved, further optimization needed")
        
        return {
            'accuracy': test_accuracy,
            'precision': test_precision,
            'recall': test_recall,
            'f1': test_f1,
            'confidence': avg_confidence
        }
    
    def generate_training_report(self):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        report_dir = "experiments/turbidity_classification"
        os.makedirs(report_dir, exist_ok=True)
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # å‡†ç¡®ç‡æ›²çº¿
        axes[0, 0].plot(self.training_history['train_acc'], label='Train Accuracy', color='blue')
        axes[0, 0].plot(self.training_history['val_acc'], label='Val Accuracy', color='red')
        axes[0, 0].axhline(y=92.0, color='green', linestyle='--', label='Target (92%)')
        axes[0, 0].axhline(y=88.0, color='orange', linestyle='--', label='Baseline (88%)')
        axes[0, 0].set_title('Turbidity Classification Accuracy')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Accuracy (%)')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # æŸå¤±æ›²çº¿
        axes[0, 1].plot(self.training_history['train_loss'], label='Train Loss', color='blue')
        axes[0, 1].plot(self.training_history['val_loss'], label='Val Loss', color='red')
        axes[0, 1].set_title('Training and Validation Loss')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # å­¦ä¹ ç‡æ›²çº¿
        axes[1, 0].plot(self.training_history['learning_rates'])
        axes[1, 0].set_title('Learning Rate Schedule')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Learning Rate')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True)
        
        # æ€§èƒ½æå‡å¯¹æ¯”
        baseline_acc = 88.0
        current_best = max(self.training_history['val_acc'])
        improvement = current_best - baseline_acc
        
        axes[1, 1].bar(['Baseline', 'Enhanced'], [baseline_acc, current_best], 
                      color=['red', 'green'], alpha=0.7)
        axes[1, 1].set_title('Turbidity Classification Improvement')
        axes[1, 1].set_ylabel('Accuracy (%)')
        axes[1, 1].text(1, current_best + 0.5, f'+{improvement:.1f}%', 
                       ha='center', va='bottom', fontweight='bold')
        axes[1, 1].grid(True, axis='y')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        plot_path = os.path.join(report_dir, "turbidity_training_curves.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"Training curves saved: {plot_path}")

class TurbidityDataset(torch.utils.data.Dataset):
    """æµŠåº¦åˆ†ç±»æ•°æ®é›†"""
    
    def __init__(self, images, labels, augment=False):
        self.images = images
        self.labels = labels
        self.augment = augment
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]
        
        # è½¬æ¢ä¸ºtensor
        if not isinstance(image, torch.Tensor):
            image = torch.from_numpy(image).float()
        
        # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡®
        if len(image.shape) == 3 and image.shape[0] != 3:
            image = image.permute(2, 0, 1)
        
        # æ•°æ®å¢å¼º
        if self.augment:
            # éšæœºç¿»è½¬
            if np.random.random() > 0.5:
                image = torch.flip(image, [2])
            if np.random.random() > 0.5:
                image = torch.flip(image, [1])
            
            # éšæœºæ—‹è½¬
            if np.random.random() > 0.5:
                angle = np.random.uniform(-15, 15)
                image = self.rotate_image(image, angle)
            
            # äº®åº¦å’Œå¯¹æ¯”åº¦è°ƒæ•´
            if np.random.random() > 0.5:
                brightness = np.random.uniform(0.8, 1.2)
                contrast = np.random.uniform(0.8, 1.2)
                image = image * contrast + brightness - 1.0
                image = torch.clamp(image, 0, 1)
        
        # æ ‡å‡†åŒ–
        image = (image - image.mean()) / (image.std() + 1e-8)
        
        return image, torch.tensor(label, dtype=torch.long)
    
    def rotate_image(self, image, angle):
        """æ—‹è½¬å›¾åƒ"""
        # ç®€å•çš„æ—‹è½¬å®ç°
        if abs(angle) < 1:
            return image
        
        # è½¬æ¢ä¸ºnumpyè¿›è¡Œæ—‹è½¬
        img_np = image.cpu().numpy().transpose(1, 2, 0)
        h, w = img_np.shape[:2]
        center = (w // 2, h // 2)
        
        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
        rotated = cv2.warpAffine(img_np, rotation_matrix, (w, h))
        
        # è½¬æ¢å›tensor
        return torch.from_numpy(rotated.transpose(2, 0, 1))

def main():
    """ä¸»å‡½æ•°"""
    config = {
        'learning_rate': 0.001,
        'weight_decay': 1e-4,
        'batch_size': 32,
        'num_epochs': 100,
        'patience': 15,
        'num_classes': 5,
        'label_smoothing': 0.1,
        'T_0': 10,
        'T_mult': 2,
        'eta_min': 1e-6,
        'pretrained_model_path': 'experiments/false_negative_optimization/best_fn_optimized_model.pth'
    }
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = TurbidityClassificationTrainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()

if __name__ == "__main__":
    main()
"""
æµŠåº¦åˆ†ç±»ç²¾åº¦æå‡è„šæœ¬
ç›®æ ‡ï¼šå°†æµŠåº¦åˆ†ç±»å‡†ç¡®ç‡ä»88%æå‡è‡³92%+
é‡ç‚¹ï¼šå¤šå°ºåº¦ç‰¹å¾èåˆå’Œè‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torch.nn.functional as F
import numpy as np
import os
import json
from datetime import datetime
import logging
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import cv2
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.enhanced_airbubble_detector import EnhancedAirBubbleDetector
from core.data_loader import MICDataLoader

class MultiScaleFeatureFusion(nn.Module):
    """å¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å—"""
    
    def __init__(self, in_channels: int):
        super().__init__()
        
        # ä¸åŒå°ºåº¦çš„ç‰¹å¾æå–
        self.scale1 = nn.Sequential(
            nn.Conv2d(in_channels, 64, 1),  # 1x1å·ç§¯
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        
        self.scale2 = nn.Sequential(
            nn.Conv2d(in_channels, 64, 3, padding=1),  # 3x3å·ç§¯
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        
        self.scale3 = nn.Sequential(
            nn.Conv2d(in