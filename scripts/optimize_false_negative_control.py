"""
å‡é˜´æ€§æ§åˆ¶ä¼˜åŒ–è„šæœ¬
ç›®æ ‡ï¼šå°†å‡é˜´æ€§ç‡ä»2.43%é™è‡³1.5%ä»¥ä¸‹
é‡ç‚¹ï¼šå®æ–½ä¸å¹³è¡¡å­¦ä¹ ç­–ç•¥å’Œä¼˜åŒ–æŸå¤±å‡½æ•°æƒé‡
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, WeightedRandomSampler
import numpy as np
import os
import json
from datetime import datetime
import logging
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score, roc_curve
)
from sklearn.utils.class_weight import compute_class_weight
import seaborn as sns
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.enhanced_airbubble_detector import EnhancedAirBubbleDetector
from core.data_loader import MICDataLoader

class FalseNegativeOptimizer:
    """å‡é˜´æ€§æ§åˆ¶ä¼˜åŒ–å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = EnhancedAirBubbleDetector().to(self.device)
        
        # åŠ è½½é¢„è®­ç»ƒçš„æ°”å­”æ£€æµ‹å™¨
        if 'pretrained_model_path' in config:
            self.load_pretrained_model(config['pretrained_model_path'])
        
        # ä¼˜åŒ–å™¨é…ç½®
        self.setup_optimizer()
        
        # æŸå¤±å‡½æ•°é…ç½®
        self.setup_loss_functions()
        
        # æ•°æ®åŠ è½½å™¨
        self.setup_data_loaders()
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'train_fnr': [],
            'val_fnr': [],
            'train_fpr': [],
            'val_fpr': [],
            'train_acc': [],
            'val_acc': []
        }
        
        self.best_fnr = float('inf')
        self.patience_counter = 0
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = "experiments/false_negative_optimization"
        os.makedirs(log_dir, exist_ok=True)
        
        log_file = os.path.join(log_dir, f"fn_optimization_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_pretrained_model(self, model_path: str):
        """åŠ è½½é¢„è®­ç»ƒçš„æ°”å­”æ£€æµ‹å™¨"""
        if os.path.exists(model_path):
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.logger.info(f"Loaded pretrained model from {model_path}")
        else:
            self.logger.warning(f"Pretrained model not found at {model_path}")
    
    def setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        # ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒ
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.get('learning_rate', 0.0001),
            weight_decay=self.config.get('weight_decay', 1e-5)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            min_lr=1e-7
        )
    
    def setup_loss_functions(self):
        """è®¾ç½®æŸå¤±å‡½æ•°"""
        # è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†ä¸å¹³è¡¡æ•°æ®
        data_loader = MICDataLoader()
        train_images, train_labels = data_loader.get_train_data()
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(train_labels),
            y=train_labels
        )
        
        # é¢å¤–å¢åŠ æ­£ç±»ï¼ˆæœ‰æ°”å­”ï¼‰çš„æƒé‡ä»¥å‡å°‘å‡é˜´æ€§
        positive_class_boost = self.config.get('positive_class_boost', 2.0)
        if len(class_weights) > 1:
            class_weights[1] *= positive_class_boost  # å‡è®¾1æ˜¯æ­£ç±»
        
        class_weights_tensor = torch.FloatTensor(class_weights).to(self.device)
        
        self.logger.info(f"Class weights: {class_weights}")
        
        # ä¸»è¦æŸå¤±å‡½æ•°ï¼šåŠ æƒäº¤å‰ç†µ
        self.primary_loss = nn.CrossEntropyLoss(weight=class_weights_tensor)
        
        # è¾…åŠ©æŸå¤±å‡½æ•°ï¼šFocal Lossç”¨äºå¤„ç†å›°éš¾æ ·æœ¬
        self.focal_loss = FocalLoss(
            alpha=class_weights_tensor,
            gamma=self.config.get('focal_gamma', 2.0)
        )
        
        # å‡é˜´æ€§æƒ©ç½šæŸå¤±
        self.fn_penalty_loss = FalseNegativePenaltyLoss(
            penalty_weight=self.config.get('fn_penalty_weight', 5.0)
        )
    
    def setup_data_loaders(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        data_loader = MICDataLoader()
        
        # è·å–è®­ç»ƒæ•°æ®
        train_images, train_labels = data_loader.get_train_data()
        val_images, val_labels = data_loader.get_val_data()
        test_images, test_labels = data_loader.get_test_data()
        
        # åˆ›å»ºåŠ æƒé‡‡æ ·å™¨ä»¥å¹³è¡¡ç±»åˆ«
        train_class_counts = np.bincount(train_labels)
        train_weights = 1.0 / train_class_counts[train_labels]
        
        # å¢åŠ æ­£ç±»æ ·æœ¬çš„é‡‡æ ·æƒé‡
        positive_boost = self.config.get('positive_sampling_boost', 1.5)
        train_weights[train_labels == 1] *= positive_boost
        
        train_sampler = WeightedRandomSampler(
            weights=train_weights,
            num_samples=len(train_weights),
            replacement=True
        )
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = MICDataset(train_images, train_labels, augment=True)
        val_dataset = MICDataset(val_images, val_labels, augment=False)
        test_dataset = MICDataset(test_images, test_labels, augment=False)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.get('batch_size', 32),
            sampler=train_sampler,
            num_workers=4,
            pin_memory=True
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.get('batch_size', 32),
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        self.test_loader = DataLoader(
            test_dataset,
            batch_size=self.config.get('batch_size', 32),
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        self.logger.info(f"Train samples: {len(train_dataset)}")
        self.logger.info(f"Val samples: {len(val_dataset)}")
        self.logger.info(f"Test samples: {len(test_dataset)}")
    
    def compute_metrics(self, predictions: np.ndarray, labels: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—è¯¦ç»†æŒ‡æ ‡"""
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(labels, predictions)
        
        if cm.shape == (2, 2):
            tn, fp, fn, tp = cm.ravel()
            
            # è®¡ç®—å„ç§æŒ‡æ ‡
            accuracy = (tp + tn) / (tp + tn + fp + fn)
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            
            # å‡é˜´æ€§ç‡å’Œå‡é˜³æ€§ç‡
            fnr = fn / (fn + tp) if (fn + tp) > 0 else 0
            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
            
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        else:
            # å¤šç±»åˆ«æƒ…å†µ
            accuracy = accuracy_score(labels, predictions)
            precision = precision_score(labels, predictions, average='weighted')
            recall = recall_score(labels, predictions, average='weighted')
            f1 = f1_score(labels, predictions, average='weighted')
            fnr = 1 - recall  # ç®€åŒ–è®¡ç®—
            fpr = 0  # å¤šç±»åˆ«æƒ…å†µä¸‹ä¸æ˜“è®¡ç®—
            specificity = 0
        
        return {
            'accuracy': accuracy * 100,
            'precision': precision * 100,
            'recall': recall * 100,
            'specificity': specificity * 100,
            'f1': f1 * 100,
            'fnr': fnr * 100,
            'fpr': fpr * 100
        }
    
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        all_predictions = []
        all_labels = []
        
        for batch_idx, (images, labels) in enumerate(self.train_loader):
            images = images.to(self.device)
            labels = labels.to(self.device)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(images)
            logits = outputs['classification']
            
            # è®¡ç®—å¤šä¸ªæŸå¤±
            primary_loss = self.primary_loss(logits, labels)
            focal_loss = self.focal_loss(logits, labels)
            fn_penalty = self.fn_penalty_loss(logits, labels)
            
            # ç»„åˆæŸå¤±
            total_loss_batch = (
                self.config.get('primary_loss_weight', 1.0) * primary_loss +
                self.config.get('focal_loss_weight', 0.5) * focal_loss +
                self.config.get('fn_penalty_weight', 0.3) * fn_penalty
            )
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            total_loss_batch.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += total_loss_batch.item()
            _, predicted = torch.max(logits.data, 1)
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            if batch_idx % 50 == 0:
                self.logger.info(
                    f'Batch {batch_idx}/{len(self.train_loader)}, '
                    f'Loss: {total_loss_batch.item():.4f}'
                )
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self.compute_metrics(np.array(all_predictions), np.array(all_labels))
        metrics['loss'] = total_loss / len(self.train_loader)
        
        return metrics
    
    def validate_epoch(self) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        all_predictions = []
        all_labels = []
        all_probabilities = []
        
        with torch.no_grad():
            for images, labels in self.val_loader:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(images)
                logits = outputs['classification']
                
                # è®¡ç®—æŸå¤±
                loss = self.primary_loss(logits, labels)
                total_loss += loss.item()
                
                # é¢„æµ‹
                probabilities = torch.softmax(logits, dim=1)
                _, predicted = torch.max(logits.data, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self.compute_metrics(np.array(all_predictions), np.array(all_labels))
        metrics['loss'] = total_loss / len(self.val_loader)
        
        # è®¡ç®—AUCï¼ˆå¦‚æœæ˜¯äºŒåˆ†ç±»ï¼‰
        if len(np.unique(all_labels)) == 2:
            all_probabilities = np.array(all_probabilities)
            if all_probabilities.shape[1] == 2:
                auc = roc_auc_score(all_labels, all_probabilities[:, 1])
                metrics['auc'] = auc * 100
        
        return metrics
    
    def optimize(self):
        """æ‰§è¡Œå‡é˜´æ€§ä¼˜åŒ–"""
        self.logger.info("Starting False Negative Optimization...")
        self.logger.info(f"Target: Reduce FNR from 2.43% to < 1.5%")
        
        num_epochs = self.config.get('num_epochs', 50)
        patience = self.config.get('patience', 10)
        
        for epoch in range(num_epochs):
            self.logger.info(f"\nEpoch {epoch+1}/{num_epochs}")
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch()
            
            # éªŒè¯
            val_metrics = self.validate_epoch()
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step(val_metrics['fnr'])
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•å†å²
            self.training_history['train_loss'].append(train_metrics['loss'])
            self.training_history['val_loss'].append(val_metrics['loss'])
            self.training_history['train_fnr'].append(train_metrics['fnr'])
            self.training_history['val_fnr'].append(val_metrics['fnr'])
            self.training_history['train_fpr'].append(train_metrics['fpr'])
            self.training_history['val_fpr'].append(val_metrics['fpr'])
            self.training_history['train_acc'].append(train_metrics['accuracy'])
            self.training_history['val_acc'].append(val_metrics['accuracy'])
            
            # æ—¥å¿—è¾“å‡º
            self.logger.info(
                f"Train - Loss: {train_metrics['loss']:.4f}, "
                f"Acc: {train_metrics['accuracy']:.2f}%, "
                f"FNR: {train_metrics['fnr']:.2f}%, "
                f"FPR: {train_metrics['fpr']:.2f}%"
            )
            self.logger.info(
                f"Val - Loss: {val_metrics['loss']:.4f}, "
                f"Acc: {val_metrics['accuracy']:.2f}%, "
                f"FNR: {val_metrics['fnr']:.2f}%, "
                f"FPR: {val_metrics['fpr']:.2f}%"
            )
            if 'auc' in val_metrics:
                self.logger.info(f"Val AUC: {val_metrics['auc']:.2f}%")
            self.logger.info(f"Learning Rate: {current_lr:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºFNRï¼‰
            if val_metrics['fnr'] < self.best_fnr:
                self.best_fnr = val_metrics['fnr']
                self.patience_counter = 0
                self.save_best_model(epoch, val_metrics)
                self.logger.info(f"New best FNR: {self.best_fnr:.2f}%")
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                if val_metrics['fnr'] <= 1.5:
                    self.logger.info("ğŸ‰ TARGET ACHIEVED! FNR <= 1.5%")
            else:
                self.patience_counter += 1
            
            # æ—©åœæ£€æŸ¥
            if self.patience_counter >= patience:
                self.logger.info(f"Early stopping triggered after {epoch+1} epochs")
                break
        
        # æœ€ç»ˆæµ‹è¯•
        self.final_evaluation()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_optimization_report()
    
    def save_best_model(self, epoch: int, metrics: Dict[str, float]):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        save_dir = "experiments/false_negative_optimization"
        os.makedirs(save_dir, exist_ok=True)
        
        model_path = os.path.join(save_dir, "best_fn_optimized_model.pth")
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'metrics': metrics,
            'config': self.config,
            'training_history': self.training_history
        }
        
        torch.save(checkpoint, model_path)
        self.logger.info(f"Best model saved: {model_path}")
    
    def final_evaluation(self):
        """æœ€ç»ˆè¯„ä¼°"""
        self.logger.info("Performing final evaluation on test set...")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        model_path = "experiments/false_negative_optimization/best_fn_optimized_model.pth"
        if os.path.exists(model_path):
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
        
        self.model.eval()
        all_predictions = []
        all_labels = []
        all_probabilities = []
        
        with torch.no_grad():
            for images, labels in self.test_loader:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(images)
                logits = outputs['classification']
                probabilities = torch.softmax(logits, dim=1)
                _, predicted = torch.max(logits.data, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())
        
        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        final_metrics = self.compute_metrics(np.array(all_predictions), np.array(all_labels))
        
        self.logger.info("=== FINAL TEST RESULTS ===")
        self.logger.info(f"Accuracy: {final_metrics['accuracy']:.2f}%")
        self.logger.info(f"Precision: {final_metrics['precision']:.2f}%")
        self.logger.info(f"Recall: {final_metrics['recall']:.2f}%")
        self.logger.info(f"F1-Score: {final_metrics['f1']:.2f}%")
        self.logger.info(f"False Negative Rate: {final_metrics['fnr']:.2f}%")
        self.logger.info(f"False Positive Rate: {final_metrics['fpr']:.2f}%")
        
        # ç›®æ ‡è¾¾æˆæ£€æŸ¥
        baseline_fnr = 2.43
        target_fnr = 1.5
        improvement = baseline_fnr - final_metrics['fnr']
        
        self.logger.info(f"\n=== TARGET ACHIEVEMENT ===")
        self.logger.info(f"Baseline FNR: {baseline_fnr:.2f}%")
        self.logger.info(f"Current FNR: {final_metrics['fnr']:.2f}%")
        self.logger.info(f"Improvement: -{improvement:.2f}%")
        self.logger.info(f"Target FNR: â‰¤ {target_fnr:.2f}%")
        
        if final_metrics['fnr'] <= target_fnr:
            self.logger.info("âœ… TARGET ACHIEVED!")
        else:
            self.logger.info("âŒ Target not achieved, further optimization needed")
        
        return final_metrics
    
    def generate_optimization_report(self):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        report_dir = "experiments/false_negative_optimization"
        os.makedirs(report_dir, exist_ok=True)
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # FNRæ›²çº¿
        axes[0, 0].plot(self.training_history['train_fnr'], label='Train FNR', color='red')
        axes[0, 0].plot(self.training_history['val_fnr'], label='Val FNR', color='blue')
        axes[0, 0].axhline(y=1.5, color='green', linestyle='--', label='Target (1.5%)')
        axes[0, 0].axhline(y=2.43, color='orange', linestyle='--', label='Baseline (2.43%)')
        axes[0, 0].set_title('False Negative Rate Optimization')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('FNR (%)')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # FPRæ›²çº¿
        axes[0, 1].plot(self.training_history['train_fpr'], label='Train FPR', color='red')
        axes[0, 1].plot(self.training_history['val_fpr'], label='Val FPR', color='blue')
        axes[0, 1].set_title('False Positive Rate')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('FPR (%)')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        axes[1, 0].plot(self.training_history['train_acc'], label='Train Accuracy', color='red')
        axes[1, 0].plot(self.training_history['val_acc'], label='Val Accuracy', color='blue')
        axes[1, 0].set_title('Accuracy')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Accuracy (%)')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # æŸå¤±æ›²çº¿
        axes[1, 1].plot(self.training_history['train_loss'], label='Train Loss', color='red')
        axes[1, 1].plot(self.training_history['val_loss'], label='Val Loss', color='blue')
        axes[1, 1].set_title('Loss')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Loss')
        axes[1, 1].legend()
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plot_path = os.path.join(report_dir, "fn_optimization_curves.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"Optimization curves saved: {plot_path}")

class FocalLoss(nn.Module):
    """Focal Lossç”¨äºå¤„ç†å›°éš¾æ ·æœ¬"""
    
    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class FalseNegativePenaltyLoss(nn.Module):
    """å‡é˜´æ€§æƒ©ç½šæŸå¤±"""
    
    def __init__(self, penalty_weight=5.0):
        super().__init__()
        self.penalty_weight = penalty_weight
    
    def forward(self, inputs, targets):
        # è®¡ç®—é¢„æµ‹æ¦‚ç‡
        probs = torch.softmax(inputs, dim=1)
        
        # æ‰¾åˆ°çœŸæ­£ä¾‹ï¼ˆtargets == 1ï¼‰
        positive_mask = (targets == 1)
        
        if positive_mask.sum() == 0:
            return torch.tensor(0.0, device=inputs.device)
        
        # å¯¹äºæ­£æ ·æœ¬ï¼Œæƒ©ç½šé¢„æµ‹ä¸ºè´Ÿç±»çš„æ¦‚ç‡
        positive_probs = probs[positive_mask]
        negative_predictions = positive_probs[:, 0]  # é¢„æµ‹ä¸ºè´Ÿç±»çš„æ¦‚ç‡
        
        # æƒ©ç½šæŸå¤±ï¼šé¢„æµ‹ä¸ºè´Ÿç±»çš„æ¦‚ç‡è¶Šé«˜ï¼ŒæŸå¤±è¶Šå¤§
        penalty_loss = self.penalty_weight * negative_predictions.mean()
        
        return penalty_loss

class MICDataset(torch.utils.data.Dataset):
    """ç®€åŒ–çš„MICæ•°æ®é›†"""
    
    def __init__(self, images, labels, augment=False):
        self.images = images
        self.labels = labels
        self.augment = augment
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]
        
        # è½¬æ¢ä¸ºtensor
        if not isinstance(image, torch.Tensor):
            image = torch.from_numpy(image).float()
        
        # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡®
        if len(image.shape) == 3 and image.shape[0] != 3:
            image = image.permute(2, 0, 1)
        
        # ç®€å•çš„æ•°æ®å¢å¼º
        if self.augment:
            if np.random.random() > 0.5:
                image = torch.flip(image, [2])  # æ°´å¹³ç¿»è½¬
            if np.random.random() > 0.5:
                image = torch.flip(image, [1])  # å‚ç›´ç¿»è½¬
        
        # æ ‡å‡†åŒ–
        image = (image - image.mean()) / (image.std() + 1e-8)
        
        return image, torch.tensor(label, dtype=torch.long)

def main():
    """ä¸»å‡½æ•°"""
    config = {
        'learning_rate': 0.0001,
        'weight_decay': 1e-5,
        'batch_size': 32,
        'num_epochs': 50,
        'patience': 10,
        'positive_class_boost': 2.0,
        'positive_sampling_boost': 1.5,
        'focal_gamma': 2.0,
        'fn_penalty_weight': 5.0,
        'primary_loss_weight': 1.0,
        'focal_loss_weight': 0.5,
        'fn_penalty_weight': 0.3,
        'pretrained_model_path': 'experiments/enhanced_airbubble_detector/enhanced_airbubble_*_best.pth'
    }
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = FalseNegativeOptimizer(config)
    
    # å¼€å§‹ä¼˜åŒ–
    optimizer.optimize()

if __name__ == "__main__":
    main()