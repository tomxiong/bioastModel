#!/usr/bin/env python3
"""
å¿«é€Ÿåˆ†ææœ€åè®­ç»ƒçš„æ¨¡å‹ï¼Œç”Ÿæˆé”™è¯¯æ ·æœ¬æ¸…å•
ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸“é—¨ç”¨äºåˆ†ææœ€æ–°å®Œæˆçš„è®­ç»ƒ
"""

import sys
import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import argparse

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

from core.data_loader import MICDataLoader
from core.config.model_configs import get_model_config
import torch
from torch.utils.data import DataLoader

def find_latest_trained_model():
    """è‡ªåŠ¨æ‰¾åˆ°æœ€åä¸€ä¸ªè®­ç»ƒçš„æ¨¡å‹"""
    import glob
    
    # æŸ¥æ‰¾æ‰€æœ‰best_model.pthæ–‡ä»¶ï¼ŒæŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    model_files = glob.glob('experiments/*/best_model.pth') + glob.glob('experiments/*/*/best_model.pth')
    
    if not model_files:
        return None
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
    latest_model = max(model_files, key=os.path.getmtime)
    
    # ä»è·¯å¾„ä¸­æå–æ¨¡å‹ä¿¡æ¯
    path_parts = latest_model.split(os.sep)
    if len(path_parts) >= 3:
        model_name = path_parts[-2]  # æ¨¡å‹åç§°ä½œä¸ºç›®å½•å
        experiment_dir = os.path.dirname(latest_model)
        
        return {
            'name': model_name,
            'checkpoint': latest_model,
            'experiment_dir': experiment_dir
        }
    
    return None

def load_model(model_name, checkpoint_path):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    try:
        # è·å–æ¨¡å‹é…ç½®
        config = get_model_config(model_name)
        
        # åŠ¨æ€å¯¼å…¥æ¨¡å‹
        module_path = config['module_path']
        class_name = config['class_name']
        
        module = __import__(module_path, fromlist=[class_name])
        model_class = getattr(module, class_name)
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = model_class(num_classes=config['num_classes'])
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model.eval()
        return model, config
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return None, None

def quick_model_analysis(model_name, checkpoint_path, experiment_dir):
    """å¿«é€Ÿåˆ†ææ¨¡å‹æ€§èƒ½"""
    print(f"ğŸ” åˆ†ææ¨¡å‹: {model_name}")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {checkpoint_path}")
    
    # åŠ è½½æ•°æ®
    data_loader = MICDataLoader(data_dir='bioast_dataset')
    test_images, test_labels = data_loader.get_test_data()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
    from core.data_loader import MICDataset
    test_dataset = MICDataset(test_images, test_labels)
    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # åŠ è½½æ¨¡å‹
    model, config = load_model(model_name, checkpoint_path)
    if model is None:
        return None
    
    # åˆ†æé¢„æµ‹ç»“æœ
    model.eval()
    all_predictions = []
    all_labels = []
    all_confidences = []
    all_probs = []
    
    print("ğŸ“Š æ­£åœ¨åˆ†æé¢„æµ‹ç»“æœ...")
    
    with torch.no_grad():
        for images, labels in test_dataloader:
            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)
            confidences, predictions = torch.max(probs, dim=1)
            
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_confidences.extend(confidences.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    
    # è®¡ç®—å‡†ç¡®ç‡
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    all_confidences = np.array(all_confidences)
    
    accuracy = np.mean(all_predictions == all_labels)
    error_count = np.sum(all_predictions != all_labels)
    
    print(f"âœ… å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"âŒ é”™è¯¯æ ·æœ¬: {error_count} / {len(all_labels)}")
    
    # æ‰¾å‡ºé”™è¯¯æ ·æœ¬
    error_indices = np.where(all_predictions != all_labels)[0]
    
    # ç”Ÿæˆé”™è¯¯æ ·æœ¬æ¸…å•
    error_analysis = []
    for idx in error_indices:
        error_type = "False Positive" if all_labels[idx] == 0 else "False Negative"
        error_analysis.append({
            'index': idx,
            'true_label': int(all_labels[idx]),
            'predicted_label': int(all_predictions[idx]),
            'confidence': float(all_confidences[idx]),
            'prob_negative': float(all_probs[idx][0]),
            'prob_positive': float(all_probs[idx][1]),
            'error_type': error_type
        })
    
    # ç»Ÿè®¡é”™è¯¯ç±»å‹
    false_positives = len([e for e in error_analysis if e['error_type'] == 'False Positive'])
    false_negatives = len([e for e in error_analysis if e['error_type'] == 'False Negative'])
    
    print(f"ğŸ“Š é”™è¯¯ç±»å‹åˆ†å¸ƒ:")
    print(f"   - å‡é˜³æ€§ (False Positive): {false_positives}")
    print(f"   - å‡é˜´æ€§ (False Negative): {false_negatives}")
    
    # é«˜ç½®ä¿¡åº¦é”™è¯¯
    high_conf_errors = [e for e in error_analysis if e['confidence'] > 0.8]
    if high_conf_errors:
        print(f"âš ï¸  é«˜ç½®ä¿¡åº¦é”™è¯¯ (>0.8): {len(high_conf_errors)} ä¸ª")
    
    # ä¿å­˜ç»“æœ
    output_dir = os.path.join(experiment_dir, 'error_analysis')
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜é”™è¯¯æ ·æœ¬æ¸…å•
    error_df = pd.DataFrame(error_analysis)
    error_csv_path = os.path.join(output_dir, f'{model_name}_error_samples.csv')
    error_df.to_csv(error_csv_path, index=False)
    print(f"ğŸ“ é”™è¯¯æ ·æœ¬æ¸…å•: {error_csv_path}")
    
    # ä¿å­˜åˆ†ææŠ¥å‘Š
    report = {
        'model_name': model_name,
        'analysis_time': datetime.now().isoformat(),
        'total_samples': len(all_labels),
        'accuracy': float(accuracy),
        'error_count': int(error_count),
        'error_rate': float(error_count / len(all_labels)),
        'false_positives': false_positives,
        'false_negatives': false_negatives,
        'high_confidence_errors': len(high_conf_errors),
        'confidence_stats': {
            'mean': float(np.mean(all_confidences)),
            'std': float(np.std(all_confidences)),
            'min': float(np.min(all_confidences)),
            'max': float(np.max(all_confidences))
        }
    }
    
    report_path = os.path.join(output_dir, f'{model_name}_quick_analysis.json')
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“‹ åˆ†ææŠ¥å‘Š: {report_path}")
    
    # æ˜¾ç¤ºä¸€äº›å…¸å‹é”™è¯¯æ ·æœ¬
    if error_analysis:
        print(f"\nğŸ” å…¸å‹é”™è¯¯æ ·æœ¬:")
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        sorted_errors = sorted(error_analysis, key=lambda x: x['confidence'], reverse=True)
        for i, error in enumerate(sorted_errors[:5]):
            print(f"   {i+1}. æ ·æœ¬{error['index']}: {error['error_type']}, ç½®ä¿¡åº¦={error['confidence']:.3f}")
    
    return report

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¿«é€Ÿåˆ†ææœ€åè®­ç»ƒçš„æ¨¡å‹')
    parser.add_argument('--model', type=str, default=None,
                        help='æŒ‡å®šè¦åˆ†æçš„æ¨¡å‹åç§°')
    parser.add_argument('--checkpoint', type=str, default=None,
                        help='æŒ‡å®šæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¿«é€Ÿæ¨¡å‹åˆ†æå·¥å…·")
    print("=" * 50)
    
    if args.model and args.checkpoint:
        # åˆ†ææŒ‡å®šçš„æ¨¡å‹
        report = quick_model_analysis(args.model, args.checkpoint, 
                                    os.path.dirname(args.checkpoint))
    else:
        # è‡ªåŠ¨æ‰¾åˆ°æœ€åä¸€ä¸ªè®­ç»ƒçš„æ¨¡å‹
        latest_model = find_latest_trained_model()
        if latest_model:
            report = quick_model_analysis(
                latest_model['name'], 
                latest_model['checkpoint'], 
                latest_model['experiment_dir']
            )
        else:
            print("âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹")
            return
    
    if report:
        print(f"\nâœ… åˆ†æå®Œæˆ!")
        print(f"ğŸ“Š æ¨¡å‹ {report['model_name']} å‡†ç¡®ç‡: {report['accuracy']:.4f}")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {os.path.join(os.path.dirname(args.checkpoint) if args.checkpoint else 'experiments', 'error_analysis')}")

if __name__ == "__main__":
    main()