"""
Individual Model Deep Analysis Tool
Performs detailed performance analysis and error sample image analysis for each ONNX model
"""

import os
import sys
import torch
import numpy as np
import onnxruntime as ort
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import json
import base64
from PIL import Image, ImageDraw, ImageFont
import cv2
from sklearn.metrics import confusion_matrix, classification_report
import pandas as pd

# Add project root to system path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from training.dataset import BioastDataset
from torchvision import transforms
import importlib

class IndividualModelAnalyzer:
    """Deep analysis tool for individual ONNX models"""
    
    def __init__(self, model_name: str, model_config: Dict):
        self.model_name = model_name
        self.model_config = model_config
        self.input_shape = model_config['input_shape']
        self.output_dir = Path(f"reports/detailed_analysis/{model_name}")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Analysis results storage
        self.analysis_results = {
            'model_name': model_name,
            'config': model_config,
            'timestamp': datetime.now().isoformat()
        }
        
    def load_models(self) -> Tuple[Optional[torch.nn.Module], Optional[ort.InferenceSession]]:
        """Load both PyTorch and ONNX models"""
        print(f"Loading models for {self.model_name}...")
        
        pytorch_model = None
        onnx_session = None
        
        try:
            # Load PyTorch model
            module = importlib.import_module(self.model_config['module'])
            factory_func = getattr(module, self.model_config['factory_function'])
            pytorch_model = factory_func(num_classes=2)
            pytorch_model.eval()
            
            # Load checkpoint
            checkpoint_path = Path(self.model_config['checkpoint_path'])
            if checkpoint_path.exists():
                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
                if 'model_state_dict' in checkpoint:
                    state_dict = checkpoint['model_state_dict']
                else:
                    state_dict = checkpoint
                
                # Handle prefix removal for wrapped models
                if self.model_name == 'mic_mobilenetv3':
                    cleaned_state_dict = {}
                    for key, value in state_dict.items():
                        if key.startswith('base_model.'):
                            new_key = key.replace('base_model.', '')
                            cleaned_state_dict[new_key] = value
                        else:
                            cleaned_state_dict[key] = value
                    state_dict = cleaned_state_dict
                
                pytorch_model.load_state_dict(state_dict)
                print(f"PyTorch model loaded successfully")
            else:
                print(f"Checkpoint not found: {checkpoint_path}")
                return None, None
                
        except Exception as e:
            print(f"Failed to load PyTorch model: {e}")
            return None, None
        
        try:
            # Load ONNX model
            onnx_path = Path(f"onnx_models/{self.model_name}.onnx")
            if onnx_path.exists():
                onnx_session = ort.InferenceSession(str(onnx_path))
                print(f"ONNX model loaded successfully")
            else:
                print(f"ONNX model not found: {onnx_path}")
                return pytorch_model, None
                
        except Exception as e:
            print(f"Failed to load ONNX model: {e}")
            return pytorch_model, None
        
        return pytorch_model, onnx_session
    
    def prepare_comprehensive_test_data(self, num_samples: int = 500) -> Tuple[np.ndarray, np.ndarray, List]:
        """Prepare comprehensive test data with image metadata"""
        print(f"Preparing comprehensive test data for {self.model_name}...")
        
        try:
            # Define transforms based on input shape
            if self.input_shape[1:] == (224, 224):  # EfficientNet
                transform = transforms.Compose([
                    transforms.Resize((224, 224)),
                    transforms.ToTensor(),
                ])
            else:  # Others use 70x70
                transform = transforms.Compose([
                    transforms.Resize((70, 70)),
                    transforms.ToTensor(),
                ])
            
            dataset = BioastDataset(data_dir='bioast_dataset', split='test', transform=transform)
            print(f"Found {len(dataset)} samples in test dataset")
            
            # Collect all samples with metadata
            all_samples = []
            for i, (data, target) in enumerate(dataset):
                # Get original image path for analysis
                if hasattr(dataset, 'samples'):
                    image_path = dataset.samples[i][0] if i < len(dataset.samples) else f"sample_{i}"
                else:
                    image_path = f"sample_{i}"
                
                all_samples.append({
                    'data': data.numpy(),
                    'target': target,
                    'index': i,
                    'image_path': image_path
                })
            
            # Balanced sampling
            negative_samples = [s for s in all_samples if s['target'] == 0]
            positive_samples = [s for s in all_samples if s['target'] == 1]
            
            samples_per_class = min(num_samples // 2, len(negative_samples), len(positive_samples))
            
            # Select samples
            selected_samples = []
            selected_samples.extend(negative_samples[:samples_per_class])
            selected_samples.extend(positive_samples[:samples_per_class])
            
            # Extract arrays and metadata
            test_data = np.stack([s['data'] for s in selected_samples], axis=0)
            test_labels = np.array([s['target'] for s in selected_samples])
            sample_metadata = [{'index': s['index'], 'image_path': s['image_path'], 'target': s['target']} 
                             for s in selected_samples]
            
            print(f"Prepared balanced test data - Negative: {np.sum(test_labels == 0)}, Positive: {np.sum(test_labels == 1)}")
            
            return test_data, test_labels, sample_metadata
            
        except Exception as e:
            print(f"Failed to load real test data: {e}")
            # Fallback to random data
            test_data = np.random.randn(num_samples, *self.input_shape).astype(np.float32)
            test_labels = np.random.randint(0, 2, num_samples)
            sample_metadata = [{'index': i, 'image_path': f'random_{i}', 'target': test_labels[i]} 
                             for i in range(num_samples)]
            return test_data, test_labels, sample_metadata
    
    def run_detailed_inference(self, pytorch_model: torch.nn.Module, onnx_session: ort.InferenceSession,
                             test_data: np.ndarray, test_labels: np.ndarray, 
                             sample_metadata: List) -> Dict[str, Any]:
        """Run detailed inference analysis"""
        print(f"Running detailed inference analysis...")
        
        results = {
            'pytorch_predictions': [],
            'pytorch_probabilities': [],
            'pytorch_times': [],
            'onnx_predictions': [],
            'onnx_probabilities': [],
            'onnx_times': [],
            'sample_metadata': sample_metadata,
            'ground_truth': test_labels
        }
        
        # PyTorch inference
        if pytorch_model is not None:
            pytorch_model.eval()
            with torch.no_grad():
                for i, sample in enumerate(test_data):
                    input_tensor = torch.from_numpy(sample).unsqueeze(0)
                    
                    start_time = time.perf_counter()
                    try:
                        output = pytorch_model(input_tensor)
                        end_time = time.perf_counter()
                        
                        if isinstance(output, dict):
                            # Handle dictionary outputs
                            for key in ['classification', 'logits', 'output']:
                                if key in output:
                                    output = output[key]
                                    break
                            else:
                                output = list(output.values())[0]
                        elif isinstance(output, (tuple, list)):
                            output = output[0]
                        
                        probabilities = torch.softmax(output, dim=1).numpy()[0]
                        prediction = np.argmax(probabilities)
                        
                        results['pytorch_predictions'].append(prediction)
                        results['pytorch_probabilities'].append(probabilities)
                        results['pytorch_times'].append(end_time - start_time)
                        
                    except Exception as e:
                        print(f"PyTorch inference failed for sample {i}: {e}")
                        results['pytorch_predictions'].append(-1)
                        results['pytorch_probabilities'].append([0.5, 0.5])
                        results['pytorch_times'].append(0)
        
        # ONNX inference
        if onnx_session is not None:
            input_name = onnx_session.get_inputs()[0].name
            
            for i, sample in enumerate(test_data):
                input_batch = np.expand_dims(sample, axis=0)
                
                start_time = time.perf_counter()
                try:
                    output = onnx_session.run(None, {input_name: input_batch})[0]
                    end_time = time.perf_counter()
                    
                    if len(output.shape) > 2:
                        # Handle multi-dimensional outputs
                        output = output.reshape(output.shape[0], -1)
                        if output.shape[1] != 2:
                            output = output[:, :2]  # Take first 2 dimensions
                    
                    probabilities = self._softmax(output[0])
                    prediction = np.argmax(probabilities)
                    
                    results['onnx_predictions'].append(prediction)
                    results['onnx_probabilities'].append(probabilities)
                    results['onnx_times'].append(end_time - start_time)
                    
                except Exception as e:
                    print(f"ONNX inference failed for sample {i}: {e}")
                    results['onnx_predictions'].append(-1)
                    results['onnx_probabilities'].append([0.5, 0.5])
                    results['onnx_times'].append(0)
        
        return results
    
    def _softmax(self, x):
        """Safe softmax implementation"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)
    
    def analyze_errors_and_disagreements(self, inference_results: Dict) -> Dict[str, Any]:
        \"\"\"Analyze prediction errors and PyTorch/ONNX disagreements\"\"\"\n        print(f\"Analyzing errors and disagreements...\")\n        \n        pytorch_preds = np.array(inference_results['pytorch_predictions'])\n        onnx_preds = np.array(inference_results['onnx_predictions'])\n        ground_truth = np.array(inference_results['ground_truth'])\n        sample_metadata = inference_results['sample_metadata']\n        \n        analysis = {\n            'pytorch_errors': [],\n            'onnx_errors': [],\n            'model_disagreements': [],\n            'confidence_analysis': {},\n            'error_patterns': {}\n        }\n        \n        # Find PyTorch errors\n        pytorch_errors = np.where(pytorch_preds != ground_truth)[0]\n        for idx in pytorch_errors:\n            if idx < len(sample_metadata):\n                analysis['pytorch_errors'].append({\n                    'sample_index': int(idx),\n                    'predicted': int(pytorch_preds[idx]),\n                    'ground_truth': int(ground_truth[idx]),\n                    'confidence': float(np.max(inference_results['pytorch_probabilities'][idx])),\n                    'probabilities': [float(p) for p in inference_results['pytorch_probabilities'][idx]],\n                    'metadata': sample_metadata[idx]\n                })\n        \n        # Find ONNX errors\n        onnx_errors = np.where(onnx_preds != ground_truth)[0]\n        for idx in onnx_errors:\n            if idx < len(sample_metadata):\n                analysis['onnx_errors'].append({\n                    'sample_index': int(idx),\n                    'predicted': int(onnx_preds[idx]),\n                    'ground_truth': int(ground_truth[idx]),\n                    'confidence': float(np.max(inference_results['onnx_probabilities'][idx])),\n                    'probabilities': [float(p) for p in inference_results['onnx_probabilities'][idx]],\n                    'metadata': sample_metadata[idx]\n                })\n        \n        # Find model disagreements\n        disagreements = np.where(pytorch_preds != onnx_preds)[0]\n        for idx in disagreements:\n            if idx < len(sample_metadata):\n                analysis['model_disagreements'].append({\n                    'sample_index': int(idx),\n                    'pytorch_pred': int(pytorch_preds[idx]),\n                    'onnx_pred': int(onnx_preds[idx]),\n                    'ground_truth': int(ground_truth[idx]),\n                    'pytorch_confidence': float(np.max(inference_results['pytorch_probabilities'][idx])),\n                    'onnx_confidence': float(np.max(inference_results['onnx_probabilities'][idx])),\n                    'metadata': sample_metadata[idx]\n                })\n        \n        # Confidence analysis\n        pytorch_probs = np.array(inference_results['pytorch_probabilities'])\n        onnx_probs = np.array(inference_results['onnx_probabilities'])\n        \n        analysis['confidence_analysis'] = {\n            'pytorch_avg_confidence': float(np.mean([np.max(p) for p in pytorch_probs])),\n            'onnx_avg_confidence': float(np.mean([np.max(p) for p in onnx_probs])),\n            'low_confidence_threshold': 0.6,\n            'pytorch_low_confidence_samples': len([p for p in pytorch_probs if np.max(p) < 0.6]),\n            'onnx_low_confidence_samples': len([p for p in onnx_probs if np.max(p) < 0.6])\n        }\n        \n        # Error patterns by class\n        analysis['error_patterns'] = {\n            'pytorch': {\n                'false_positives': int(np.sum((pytorch_preds == 1) & (ground_truth == 0))),\n                'false_negatives': int(np.sum((pytorch_preds == 0) & (ground_truth == 1))),\n                'total_errors': len(pytorch_errors)\n            },\n            'onnx': {\n                'false_positives': int(np.sum((onnx_preds == 1) & (ground_truth == 0))),\n                'false_negatives': int(np.sum((onnx_preds == 0) & (ground_truth == 1))),\n                'total_errors': len(onnx_errors)\n            }\n        }\n        \n        return analysis\n    \n    def create_error_sample_visualizations(self, test_data: np.ndarray, \n                                         error_analysis: Dict) -> List[str]:\n        \"\"\"Create visualizations of error samples\"\"\"\n        print(f\"Creating error sample visualizations...\")\n        \n        visualization_paths = []\n        \n        # Create PyTorch error samples visualization\n        if error_analysis['pytorch_errors']:\n            fig_path = self._create_error_samples_grid(\n                test_data, error_analysis['pytorch_errors'][:12], \n                \"PyTorch Model Errors\", \"pytorch_errors\"\n            )\n            if fig_path:\n                visualization_paths.append(fig_path)\n        \n        # Create ONNX error samples visualization\n        if error_analysis['onnx_errors']:\n            fig_path = self._create_error_samples_grid(\n                test_data, error_analysis['onnx_errors'][:12], \n                \"ONNX Model Errors\", \"onnx_errors\"\n            )\n            if fig_path:\n                visualization_paths.append(fig_path)\n        \n        # Create model disagreement visualization\n        if error_analysis['model_disagreements']:\n            fig_path = self._create_disagreement_samples_grid(\n                test_data, error_analysis['model_disagreements'][:12]\n            )\n            if fig_path:\n                visualization_paths.append(fig_path)\n        \n        return visualization_paths\n    \n    def _create_error_samples_grid(self, test_data: np.ndarray, error_samples: List,\n                                 title: str, filename_prefix: str) -> Optional[str]:\n        \"\"\"Create a grid visualization of error samples\"\"\"\n        if not error_samples:\n            return None\n        \n        # Limit to 12 samples for visualization\n        samples_to_show = error_samples[:12]\n        n_samples = len(samples_to_show)\n        \n        # Calculate grid dimensions\n        cols = min(4, n_samples)\n        rows = (n_samples + cols - 1) // cols\n        \n        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n        if n_samples == 1:\n            axes = [axes]\n        elif rows == 1:\n            axes = [axes] if cols == 1 else axes\n        else:\n            axes = axes.flatten()\n        \n        fig.suptitle(title, fontsize=16, fontweight='bold')\n        \n        for i, error_sample in enumerate(samples_to_show):\n            if i >= len(axes):\n                break\n                \n            sample_idx = error_sample['sample_index']\n            \n            # Get image data\n            if sample_idx < len(test_data):\n                img_data = test_data[sample_idx]\n                \n                # Convert from tensor format (C, H, W) to image format (H, W, C)\n                if len(img_data.shape) == 3:\n                    img_data = np.transpose(img_data, (1, 2, 0))\n                \n                # Normalize to 0-1 range\n                img_data = (img_data - img_data.min()) / (img_data.max() - img_data.min())\n                \n                # Convert to grayscale if needed\n                if img_data.shape[2] == 3:\n                    img_data = np.mean(img_data, axis=2)\n                \n                axes[i].imshow(img_data, cmap='gray')\n                axes[i].set_title(\n                    f\"Sample {sample_idx}\\n\"\n                    f\"Pred: {error_sample['predicted']} | GT: {error_sample['ground_truth']}\\n\"\n                    f\"Conf: {error_sample['confidence']:.3f}\", \n                    fontsize=10\n                )\n                axes[i].axis('off')\n            else:\n                axes[i].text(0.5, 0.5, f'Sample {sample_idx}\\nData not available', \n                           ha='center', va='center')\n                axes[i].axis('off')\n        \n        # Hide empty subplots\n        for i in range(n_samples, len(axes)):\n            axes[i].axis('off')\n        \n        plt.tight_layout()\n        \n        # Save visualization\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        fig_path = self.output_dir / f\"{filename_prefix}_{timestamp}.png\"\n        plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        return str(fig_path)\n    \n    def _create_disagreement_samples_grid(self, test_data: np.ndarray, \n                                        disagreement_samples: List) -> Optional[str]:\n        \"\"\"Create visualization of samples where PyTorch and ONNX disagree\"\"\"\n        if not disagreement_samples:\n            return None\n        \n        samples_to_show = disagreement_samples[:12]\n        n_samples = len(samples_to_show)\n        \n        cols = min(4, n_samples)\n        rows = (n_samples + cols - 1) // cols\n        \n        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n        if n_samples == 1:\n            axes = [axes]\n        elif rows == 1:\n            axes = [axes] if cols == 1 else axes\n        else:\n            axes = axes.flatten()\n        \n        fig.suptitle('PyTorch vs ONNX Disagreements', fontsize=16, fontweight='bold')\n        \n        for i, disagreement in enumerate(samples_to_show):\n            if i >= len(axes):\n                break\n                \n            sample_idx = disagreement['sample_index']\n            \n            if sample_idx < len(test_data):\n                img_data = test_data[sample_idx]\n                \n                if len(img_data.shape) == 3:\n                    img_data = np.transpose(img_data, (1, 2, 0))\n                \n                img_data = (img_data - img_data.min()) / (img_data.max() - img_data.min())\n                \n                if img_data.shape[2] == 3:\n                    img_data = np.mean(img_data, axis=2)\n                \n                axes[i].imshow(img_data, cmap='gray')\n                axes[i].set_title(\n                    f\"Sample {sample_idx}\\n\"\n                    f\"PyTorch: {disagreement['pytorch_pred']} ({disagreement['pytorch_confidence']:.3f})\\n\"\n                    f\"ONNX: {disagreement['onnx_pred']} ({disagreement['onnx_confidence']:.3f})\\n\"\n                    f\"GT: {disagreement['ground_truth']}\", \n                    fontsize=9\n                )\n                axes[i].axis('off')\n            else:\n                axes[i].text(0.5, 0.5, f'Sample {sample_idx}\\nData not available', \n                           ha='center', va='center')\n                axes[i].axis('off')\n        \n        for i in range(n_samples, len(axes)):\n            axes[i].axis('off')\n        \n        plt.tight_layout()\n        \n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        fig_path = self.output_dir / f\"model_disagreements_{timestamp}.png\"\n        plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        return str(fig_path)\n    \n    def create_performance_analysis_charts(self, inference_results: Dict) -> List[str]:\n        \"\"\"Create detailed performance analysis charts\"\"\"\n        print(f\"Creating performance analysis charts...\")\n        \n        chart_paths = []\n        \n        # 1. Confidence distribution analysis\n        fig_path = self._create_confidence_distribution_chart(inference_results)\n        if fig_path:\n            chart_paths.append(fig_path)\n        \n        # 2. Prediction probability comparison\n        fig_path = self._create_probability_comparison_chart(inference_results)\n        if fig_path:\n            chart_paths.append(fig_path)\n        \n        # 3. Inference time analysis\n        fig_path = self._create_inference_time_analysis(inference_results)\n        if fig_path:\n            chart_paths.append(fig_path)\n            \n        return chart_paths\n    \n    def _create_confidence_distribution_chart(self, inference_results: Dict) -> Optional[str]:\n        \"\"\"Create confidence distribution comparison chart\"\"\"\n        pytorch_probs = np.array(inference_results['pytorch_probabilities'])\n        onnx_probs = np.array(inference_results['onnx_probabilities'])\n        \n        pytorch_confidences = [np.max(p) for p in pytorch_probs]\n        onnx_confidences = [np.max(p) for p in onnx_probs]\n        \n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n        \n        # Histogram comparison\n        ax1.hist(pytorch_confidences, alpha=0.7, label='PyTorch', bins=20, color='blue')\n        ax1.hist(onnx_confidences, alpha=0.7, label='ONNX', bins=20, color='orange')\n        ax1.set_xlabel('Confidence Score')\n        ax1.set_ylabel('Frequency')\n        ax1.set_title('Confidence Score Distribution')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # Box plot comparison\n        ax2.boxplot([pytorch_confidences, onnx_confidences], \n                   labels=['PyTorch', 'ONNX'])\n        ax2.set_ylabel('Confidence Score')\n        ax2.set_title('Confidence Score Comparison')\n        ax2.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        \n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        fig_path = self.output_dir / f\"confidence_analysis_{timestamp}.png\"\n        plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        return str(fig_path)\n    \n    def _create_probability_comparison_chart(self, inference_results: Dict) -> Optional[str]:\n        \"\"\"Create probability comparison scatter plot\"\"\"\n        pytorch_probs = np.array(inference_results['pytorch_probabilities'])\n        onnx_probs = np.array(inference_results['onnx_probabilities'])\n        \n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n        \n        # Class 0 probabilities\n        ax1.scatter(pytorch_probs[:, 0], onnx_probs[:, 0], alpha=0.6)\n        ax1.plot([0, 1], [0, 1], 'r--', alpha=0.8)\n        ax1.set_xlabel('PyTorch Class 0 Probability')\n        ax1.set_ylabel('ONNX Class 0 Probability')\n        ax1.set_title('Class 0 Probability Comparison')\n        ax1.grid(True, alpha=0.3)\n        \n        # Class 1 probabilities\n        ax2.scatter(pytorch_probs[:, 1], onnx_probs[:, 1], alpha=0.6)\n        ax2.plot([0, 1], [0, 1], 'r--', alpha=0.8)\n        ax2.set_xlabel('PyTorch Class 1 Probability')\n        ax2.set_ylabel('ONNX Class 1 Probability')\n        ax2.set_title('Class 1 Probability Comparison')\n        ax2.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        \n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        fig_path = self.output_dir / f\"probability_comparison_{timestamp}.png\"\n        plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        return str(fig_path)\n    \n    def _create_inference_time_analysis(self, inference_results: Dict) -> Optional[str]:\n        \"\"\"Create inference time analysis chart\"\"\"\n        pytorch_times = np.array(inference_results['pytorch_times']) * 1000  # Convert to ms\n        onnx_times = np.array(inference_results['onnx_times']) * 1000\n        \n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n        \n        # Time distribution\n        ax1.hist(pytorch_times, alpha=0.7, label='PyTorch', bins=20, color='blue')\n        ax1.hist(onnx_times, alpha=0.7, label='ONNX', bins=20, color='orange')\n        ax1.set_xlabel('Inference Time (ms)')\n        ax1.set_ylabel('Frequency')\n        ax1.set_title('Inference Time Distribution')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # Time comparison scatter\n        ax2.scatter(pytorch_times, onnx_times, alpha=0.6)\n        ax2.plot([pytorch_times.min(), pytorch_times.max()], \n                [pytorch_times.min(), pytorch_times.max()], 'r--', alpha=0.8)\n        ax2.set_xlabel('PyTorch Time (ms)')\n        ax2.set_ylabel('ONNX Time (ms)')\n        ax2.set_title('Inference Time Comparison')\n        ax2.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        \n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        fig_path = self.output_dir / f\"inference_time_analysis_{timestamp}.png\"\n        plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        return str(fig_path)\n    \n    def generate_improvement_recommendations(self, error_analysis: Dict, \n                                          inference_results: Dict) -> Dict[str, Any]:\n        \"\"\"Generate specific improvement recommendations for the model\"\"\"\n        print(f\"Generating improvement recommendations...\")\n        \n        recommendations = {\n            'model_name': self.model_name,\n            'architecture_type': self.model_config.get('architecture_type', 'Unknown'),\n            'priority_issues': [],\n            'performance_improvements': [],\n            'accuracy_improvements': [],\n            'onnx_optimization': [],\n            'data_quality_improvements': [],\n            'overall_assessment': ''\n        }\n        \n        # Analyze performance metrics\n        pytorch_times = np.array(inference_results['pytorch_times'])\n        onnx_times = np.array(inference_results['onnx_times'])\n        speedup = np.mean(pytorch_times) / np.mean(onnx_times) if np.mean(onnx_times) > 0 else 0\n        \n        pytorch_errors = len(error_analysis['pytorch_errors'])\n        onnx_errors = len(error_analysis['onnx_errors'])\n        disagreements = len(error_analysis['model_disagreements'])\n        total_samples = len(inference_results['ground_truth'])\n        \n        pytorch_accuracy = (total_samples - pytorch_errors) / total_samples * 100\n        onnx_accuracy = (total_samples - onnx_errors) / total_samples * 100\n        \n        # Priority issues identification\n        if disagreements > total_samples * 0.05:  # >5% disagreement\n            recommendations['priority_issues'].append({\n                'issue': 'High PyTorch-ONNX disagreement rate',\n                'severity': 'HIGH',\n                'description': f'{disagreements}/{total_samples} samples ({disagreements/total_samples*100:.1f}%) show different predictions',\n                'impact': 'Model reliability compromised in production'\n            })\n        \n        if speedup < 1.5:\n            recommendations['priority_issues'].append({\n                'issue': 'Poor ONNX performance optimization',\n                'severity': 'MEDIUM',\n                'description': f'ONNX speedup only {speedup:.2f}x, expected >2x',\n                'impact': 'Limited production performance benefits'\n            })\n        \n        if pytorch_accuracy < 95:\n            recommendations['priority_issues'].append({\n                'issue': 'Low model accuracy on test data',\n                'severity': 'HIGH',\n                'description': f'PyTorch accuracy {pytorch_accuracy:.1f}% below expected performance',\n                'impact': 'Poor model performance in production'\n            })\n        \n        # Performance improvement recommendations\n        if speedup < 2:\n            recommendations['performance_improvements'].append(\n                \"Consider quantization techniques (INT8) to improve ONNX inference speed\"\n            )\n            recommendations['performance_improvements'].append(\n                \"Explore ONNX Runtime optimization options (graph optimization, execution providers)\"\n            )\n        \n        if np.std(pytorch_times) > np.mean(pytorch_times) * 0.3:\n            recommendations['performance_improvements'].append(\n                \"High variance in inference times - investigate batch processing or model caching\"\n            )\n        \n        # Accuracy improvement recommendations\n        false_positives = error_analysis['error_patterns']['pytorch']['false_positives']\n        false_negatives = error_analysis['error_patterns']['pytorch']['false_negatives']\n        \n        if false_positives > false_negatives * 1.5:\n            recommendations['accuracy_improvements'].append(\n                \"High false positive rate - consider adjusting decision threshold or improving negative class representation\"\n            )\n        elif false_negatives > false_positives * 1.5:\n            recommendations['accuracy_improvements'].append(\n                \"High false negative rate - improve positive class detection through data augmentation or model architecture changes\"\n            )\n        \n        confidence_analysis = error_analysis['confidence_analysis']\n        if confidence_analysis['pytorch_low_confidence_samples'] > total_samples * 0.2:\n            recommendations['accuracy_improvements'].append(\n                \"High number of low-confidence predictions - consider ensemble methods or uncertainty quantification\"\n            )\n        \n        # ONNX optimization recommendations\n        if disagreements > 0:\n            recommendations['onnx_optimization'].append(\n                \"Investigate numerical precision differences between PyTorch and ONNX implementations\"\n            )\n            recommendations['onnx_optimization'].append(\n                \"Consider using higher precision (FP32) or dynamic quantization for better accuracy preservation\"\n            )\n        \n        if abs(pytorch_accuracy - onnx_accuracy) > 1:\n            recommendations['onnx_optimization'].append(\n                f\"Significant accuracy difference ({abs(pytorch_accuracy - onnx_accuracy):.1f}%) - review conversion parameters\"\n            )\n        \n        # Data quality improvements\n        if error_analysis['pytorch_errors']:\n            recommendations['data_quality_improvements'].append(\n                \"Analyze error samples for common patterns - may indicate data quality issues or missing edge cases\"\n            )\n        \n        # Overall assessment\n        if len(recommendations['priority_issues']) == 0 and speedup > 3 and pytorch_accuracy > 97:\n            recommendations['overall_assessment'] = \"EXCELLENT - Model performs well with good ONNX optimization\"\n        elif len(recommendations['priority_issues']) <= 1 and speedup > 2 and pytorch_accuracy > 95:\n            recommendations['overall_assessment'] = \"GOOD - Minor improvements needed\"\n        elif len(recommendations['priority_issues']) <= 2:\n            recommendations['overall_assessment'] = \"FAIR - Several improvements recommended\"\n        else:\n            recommendations['overall_assessment'] = \"NEEDS IMPROVEMENT - Critical issues need attention\"\n        \n        return recommendations\n    \n    def generate_detailed_html_report(self, inference_results: Dict, error_analysis: Dict,\n                                    recommendations: Dict, visualization_paths: List[str],\n                                    chart_paths: List[str]) -> str:\n        \"\"\"Generate comprehensive HTML report with embedded images\"\"\"\n        print(f\"Generating detailed HTML report...\")\n        \n        # Encode images to base64\n        encoded_images = {}\n        for path in visualization_paths + chart_paths:\n            try:\n                with open(path, 'rb') as f:\n                    encoded_images[Path(path).stem] = base64.b64encode(f.read()).decode('utf-8')\n            except:\n                continue\n        \n        # Calculate key metrics\n        total_samples = len(inference_results['ground_truth'])\n        pytorch_errors = len(error_analysis['pytorch_errors'])\n        onnx_errors = len(error_analysis['onnx_errors'])\n        disagreements = len(error_analysis['model_disagreements'])\n        \n        pytorch_accuracy = (total_samples - pytorch_errors) / total_samples * 100\n        onnx_accuracy = (total_samples - onnx_errors) / total_samples * 100\n        \n        pytorch_times = np.array(inference_results['pytorch_times']) * 1000\n        onnx_times = np.array(inference_results['onnx_times']) * 1000\n        speedup = np.mean(pytorch_times) / np.mean(onnx_times) if np.mean(onnx_times) > 0 else 0\n        \n        # Generate HTML content\n        html_content = f\"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{self.model_name} - Detailed Analysis Report</title>\n    <style>\n        body {{\n            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n            line-height: 1.6;\n            color: #333;\n            max-width: 1600px;\n            margin: 0 auto;\n            padding: 20px;\n            background-color: #f8f9fa;\n        }}\n        \n        .header {{\n            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n            color: white;\n            padding: 40px;\n            border-radius: 15px;\n            text-align: center;\n            margin-bottom: 30px;\n            box-shadow: 0 8px 16px rgba(0,0,0,0.1);\n        }}\n        \n        .header h1 {{\n            margin: 0;\n            font-size: 3em;\n            font-weight: 300;\n        }}\n        \n        .header .subtitle {{\n            margin-top: 15px;\n            opacity: 0.9;\n            font-size: 1.3em;\n        }}\n        \n        .metrics-grid {{\n            display: grid;\n            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n            gap: 20px;\n            margin-bottom: 30px;\n        }}\n        \n        .metric-card {{\n            background: white;\n            padding: 25px;\n            border-radius: 12px;\n            box-shadow: 0 4px 12px rgba(0,0,0,0.1);\n            text-align: center;\n        }}\n        \n        .metric-card h3 {{\n            margin: 0 0 15px 0;\n            color: #495057;\n            font-size: 1.1em;\n        }}\n        \n        .metric-card .value {{\n            font-size: 2.5em;\n            font-weight: 700;\n            margin-bottom: 10px;\n        }}\n        \n        .value-excellent {{ color: #28a745; }}\n        .value-good {{ color: #17a2b8; }}\n        .value-warning {{ color: #ffc107; }}\n        .value-danger {{ color: #dc3545; }}\n        \n        .card {{\n            background: white;\n            border-radius: 12px;\n            box-shadow: 0 4px 12px rgba(0,0,0,0.1);\n            margin-bottom: 25px;\n            overflow: hidden;\n        }}\n        \n        .card-header {{\n            background: #f8f9fa;\n            padding: 20px 25px;\n            border-bottom: 1px solid #dee2e6;\n            font-weight: 600;\n            font-size: 1.3em;\n            color: #495057;\n        }}\n        \n        .card-body {{\n            padding: 25px;\n        }}\n        \n        .image-container {{\n            text-align: center;\n            margin: 20px 0;\n        }}\n        \n        .image-container img {{\n            max-width: 100%;\n            height: auto;\n            border-radius: 8px;\n            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n        }}\n        \n        .priority-high {{\n            border-left: 4px solid #dc3545;\n            background: #f8d7da;\n            padding: 15px;\n            margin: 10px 0;\n            border-radius: 4px;\n        }}\n        \n        .priority-medium {{\n            border-left: 4px solid #ffc107;\n            background: #fff3cd;\n            padding: 15px;\n            margin: 10px 0;\n            border-radius: 4px;\n        }}\n        \n        .priority-low {{\n            border-left: 4px solid #17a2b8;\n            background: #d1ecf1;\n            padding: 15px;\n            margin: 10px 0;\n            border-radius: 4px;\n        }}\n        \n        .recommendation-list {{\n            list-style: none;\n            padding: 0;\n        }}\n        \n        .recommendation-list li {{\n            padding: 10px;\n            margin: 5px 0;\n            background: #f8f9fa;\n            border-radius: 6px;\n            border-left: 3px solid #007bff;\n        }}\n        \n        .assessment-excellent {{\n            background: #d4edda;\n            color: #155724;\n            padding: 20px;\n            border-radius: 8px;\n            text-align: center;\n            font-size: 1.2em;\n            font-weight: 600;\n        }}\n        \n        .assessment-good {{\n            background: #d1ecf1;\n            color: #0c5460;\n            padding: 20px;\n            border-radius: 8px;\n            text-align: center;\n            font-size: 1.2em;\n            font-weight: 600;\n        }}\n        \n        .assessment-fair {{\n            background: #fff3cd;\n            color: #856404;\n            padding: 20px;\n            border-radius: 8px;\n            text-align: center;\n            font-size: 1.2em;\n            font-weight: 600;\n        }}\n        \n        .assessment-poor {{\n            background: #f8d7da;\n            color: #721c24;\n            padding: 20px;\n            border-radius: 8px;\n            text-align: center;\n            font-size: 1.2em;\n            font-weight: 600;\n        }}\n        \n        table {{\n            width: 100%;\n            border-collapse: collapse;\n            margin-top: 15px;\n        }}\n        \n        th, td {{\n            padding: 12px;\n            text-align: left;\n            border-bottom: 1px solid #dee2e6;\n        }}\n        \n        th {{\n            background-color: #f8f9fa;\n            font-weight: 600;\n        }}\n        \n        .timestamp {{\n            color: #6c757d;\n            font-size: 0.9em;\n            text-align: center;\n            margin-top: 40px;\n            padding-top: 20px;\n            border-top: 1px solid #dee2e6;\n        }}\n    </style>\n</head>\n<body>\n    <div class=\"header\">\n        <h1>{self.model_name.replace('_', ' ').title()}</h1>\n        <div class=\"subtitle\">Detailed Performance Analysis Report</div>\n        <div class=\"subtitle\">{self.model_config.get('architecture_type', 'Unknown Architecture')}</div>\n        <div class=\"subtitle\">Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</div>\n    </div>\n    \n    <div class=\"metrics-grid\">\n        <div class=\"metric-card\">\n            <h3>PyTorch Accuracy</h3>\n            <div class=\"value {'value-excellent' if pytorch_accuracy > 97 else 'value-good' if pytorch_accuracy > 95 else 'value-warning' if pytorch_accuracy > 90 else 'value-danger'}\">\n                {pytorch_accuracy:.1f}%\n            </div>\n        </div>\n        <div class=\"metric-card\">\n            <h3>ONNX Accuracy</h3>\n            <div class=\"value {'value-excellent' if onnx_accuracy > 97 else 'value-good' if onnx_accuracy > 95 else 'value-warning' if onnx_accuracy > 90 else 'value-danger'}\">\n                {onnx_accuracy:.1f}%\n            </div>\n        </div>\n        <div class=\"metric-card\">\n            <h3>Performance Speedup</h3>\n            <div class=\"value {'value-excellent' if speedup > 4 else 'value-good' if speedup > 2 else 'value-warning' if speedup > 1 else 'value-danger'}\">\n                {speedup:.2f}x\n            </div>\n        </div>\n        <div class=\"metric-card\">\n            <h3>Model Disagreements</h3>\n            <div class=\"value {'value-excellent' if disagreements == 0 else 'value-good' if disagreements < total_samples*0.01 else 'value-warning' if disagreements < total_samples*0.05 else 'value-danger'}\">\n                {disagreements}\n            </div>\n        </div>\n    </div>\n    \n    <div class=\"card\">\n        <div class=\"card-header\">Overall Assessment</div>\n        <div class=\"card-body\">\n            <div class=\"assessment-{'excellent' if 'EXCELLENT' in recommendations['overall_assessment'] else 'good' if 'GOOD' in recommendations['overall_assessment'] else 'fair' if 'FAIR' in recommendations['overall_assessment'] else 'poor'}\">\n                {recommendations['overall_assessment']}\n            </div>\n        </div>\n    </div>\n\"\"\"\n        \n        # Add priority issues if any\n        if recommendations['priority_issues']:\n            html_content += \"\"\"\n    <div class=\"card\">\n        <div class=\"card-header\">Priority Issues</div>\n        <div class=\"card-body\">\n\"\"\"\n            for issue in recommendations['priority_issues']:\n                priority_class = f\"priority-{issue['severity'].lower()}\"\n                html_content += f\"\"\"\n            <div class=\"{priority_class}\">\n                <h4>{issue['issue']} ({issue['severity']})</h4>\n                <p><strong>Description:</strong> {issue['description']}</p>\n                <p><strong>Impact:</strong> {issue['impact']}</p>\n            </div>\n\"\"\"\n            html_content += \"\"\"\n        </div>\n    </div>\n\"\"\"\n        \n        # Add performance analysis charts\n        if chart_paths:\n            html_content += \"\"\"\n    <div class=\"card\">\n        <div class=\"card-header\">Performance Analysis</div>\n        <div class=\"card-body\">\n\"\"\"\n            for chart_path in chart_paths:\n                chart_name = Path(chart_path).stem\n                if chart_name in encoded_images:\n                    html_content += f\"\"\"\n            <div class=\"image-container\">\n                <img src=\"data:image/png;base64,{encoded_images[chart_name]}\" alt=\"{chart_name}\" />\n            </div>\n\"\"\"\n            html_content += \"\"\"\n        </div>\n    </div>\n\"\"\"\n        \n        # Add error sample visualizations\n        if visualization_paths:\n            html_content += \"\"\"\n    <div class=\"card\">\n        <div class=\"card-header\">Error Sample Analysis</div>\n        <div class=\"card-body\">\n\"\"\"\n            for viz_path in visualization_paths:\n                viz_name = Path(viz_path).stem\n                if viz_name in encoded_images:\n                    html_content += f\"\"\"\n            <div class=\"image-container\">\n                <img src=\"data:image/png;base64,{encoded_images[viz_name]}\" alt=\"{viz_name}\" />\n            </div>\n\"\"\"\n            html_content += \"\"\"\n        </div>\n    </div>\n\"\"\"\n        \n        # Add recommendations\n        html_content += \"\"\"\n    <div class=\"card\">\n        <div class=\"card-header\">Improvement Recommendations</div>\n        <div class=\"card-body\">\n\"\"\"\n        \n        if recommendations['performance_improvements']:\n            html_content += \"\"\"\n            <h4>Performance Improvements</h4>\n            <ul class=\"recommendation-list\">\n\"\"\"\n            for rec in recommendations['performance_improvements']:\n                html_content += f\"<li>{rec}</li>\"\n            html_content += \"</ul>\"\n        \n        if recommendations['accuracy_improvements']:\n            html_content += \"\"\"\n            <h4>Accuracy Improvements</h4>\n            <ul class=\"recommendation-list\">\n\"\"\"\n            for rec in recommendations['accuracy_improvements']:\n                html_content += f\"<li>{rec}</li>\"\n            html_content += \"</ul>\"\n        \n        if recommendations['onnx_optimization']:\n            html_content += \"\"\"\n            <h4>ONNX Optimization</h4>\n            <ul class=\"recommendation-list\">\n\"\"\"\n            for rec in recommendations['onnx_optimization']:\n                html_content += f\"<li>{rec}</li>\"\n            html_content += \"</ul>\"\n        \n        if recommendations['data_quality_improvements']:\n            html_content += \"\"\"\n            <h4>Data Quality Improvements</h4>\n            <ul class=\"recommendation-list\">\n\"\"\"\n            for rec in recommendations['data_quality_improvements']:\n                html_content += f\"<li>{rec}</li>\"\n            html_content += \"</ul>\"\n        \n        html_content += \"\"\"\n        </div>\n    </div>\n    \n    <div class=\"card\">\n        <div class=\"card-header\">Detailed Statistics</div>\n        <div class=\"card-body\">\n            <table>\n                <tr><th>Metric</th><th>PyTorch</th><th>ONNX</th></tr>\n                <tr><td>Total Samples</td><td>{total_samples}</td><td>{total_samples}</td></tr>\n                <tr><td>Correct Predictions</td><td>{total_samples - pytorch_errors}</td><td>{total_samples - onnx_errors}</td></tr>\n                <tr><td>Errors</td><td>{pytorch_errors}</td><td>{onnx_errors}</td></tr>\n                <tr><td>Accuracy</td><td>{pytorch_accuracy:.2f}%</td><td>{onnx_accuracy:.2f}%</td></tr>\n                <tr><td>Avg Inference Time</td><td>{np.mean(pytorch_times):.2f} ms</td><td>{np.mean(onnx_times):.2f} ms</td></tr>\n                <tr><td>Std Inference Time</td><td>{np.std(pytorch_times):.2f} ms</td><td>{np.std(onnx_times):.2f} ms</td></tr>\n                <tr><td>Avg Confidence</td><td>{error_analysis['confidence_analysis']['pytorch_avg_confidence']:.3f}</td><td>{error_analysis['confidence_analysis']['onnx_avg_confidence']:.3f}</td></tr>\n            </table>\n            \n            <h4>Error Pattern Analysis</h4>\n            <table>\n                <tr><th>Error Type</th><th>PyTorch</th><th>ONNX</th></tr>\n                <tr><td>False Positives</td><td>{error_analysis['error_patterns']['pytorch']['false_positives']}</td><td>{error_analysis['error_patterns']['onnx']['false_positives']}</td></tr>\n                <tr><td>False Negatives</td><td>{error_analysis['error_patterns']['pytorch']['false_negatives']}</td><td>{error_analysis['error_patterns']['onnx']['false_negatives']}</td></tr>\n                <tr><td>Model Disagreements</td><td colspan=\"2\">{disagreements} samples</td></tr>\n            </table>\n        </div>\n    </div>\n    \n    <div class=\"timestamp\">\n        Detailed analysis report generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n    </div>\n</body>\n</html>\n\"\"\"\n        \n        # Save HTML report\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        html_path = self.output_dir / f\"detailed_analysis_{timestamp}.html\"\n        \n        with open(html_path, 'w', encoding='utf-8') as f:\n            f.write(html_content)\n        \n        print(f\"Detailed HTML report saved: {html_path}\")\n        return str(html_path)\n    \n    def save_analysis_results(self, inference_results: Dict, error_analysis: Dict,\n                             recommendations: Dict) -> str:\n        \"\"\"Save complete analysis results to JSON\"\"\"\n        \n        # Prepare serializable data\n        self.analysis_results.update({\n            'performance_metrics': {\n                'total_samples': len(inference_results['ground_truth']),\n                'pytorch_accuracy': (len(inference_results['ground_truth']) - len(error_analysis['pytorch_errors'])) / len(inference_results['ground_truth']) * 100,\n                'onnx_accuracy': (len(inference_results['ground_truth']) - len(error_analysis['onnx_errors'])) / len(inference_results['ground_truth']) * 100,\n                'model_disagreements': len(error_analysis['model_disagreements']),\n                'speedup': np.mean(inference_results['pytorch_times']) / np.mean(inference_results['onnx_times']) if np.mean(inference_results['onnx_times']) > 0 else 0,\n                'pytorch_avg_time_ms': float(np.mean(inference_results['pytorch_times']) * 1000),\n                'onnx_avg_time_ms': float(np.mean(inference_results['onnx_times']) * 1000)\n            },\n            'error_analysis': {\n                'pytorch_errors_count': len(error_analysis['pytorch_errors']),\n                'onnx_errors_count': len(error_analysis['onnx_errors']),\n                'disagreements_count': len(error_analysis['model_disagreements']),\n                'confidence_analysis': error_analysis['confidence_analysis'],\n                'error_patterns': error_analysis['error_patterns']\n            },\n            'recommendations': recommendations\n        })\n        \n        # Save to JSON\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        json_path = self.output_dir / f\"analysis_results_{timestamp}.json\"\n        \n        with open(json_path, 'w', encoding='utf-8') as f:\n            json.dump(self.analysis_results, f, indent=2, ensure_ascii=False)\n        \n        print(f\"Analysis results saved: {json_path}\")\n        return str(json_path)\n    \n    def run_complete_analysis(self) -> Dict[str, Any]:\n        \"\"\"Run complete detailed analysis for the model\"\"\"\n        print(f\"\\nStarting complete analysis for {self.model_name}...\")\n        print(f\"{'='*60}\")\n        \n        try:\n            # 1. Load models\n            pytorch_model, onnx_session = self.load_models()\n            if pytorch_model is None:\n                return {'success': False, 'error': 'Failed to load PyTorch model'}\n            \n            # 2. Prepare test data\n            test_data, test_labels, sample_metadata = self.prepare_comprehensive_test_data(500)\n            \n            # 3. Run detailed inference\n            inference_results = self.run_detailed_inference(\n                pytorch_model, onnx_session, test_data, test_labels, sample_metadata\n            )\n            \n            # 4. Analyze errors and disagreements\n            error_analysis = self.analyze_errors_and_disagreements(inference_results)\n            \n            # 5. Create visualizations\n            visualization_paths = self.create_error_sample_visualizations(test_data, error_analysis)\n            chart_paths = self.create_performance_analysis_charts(inference_results)\n            \n            # 6. Generate recommendations\n            recommendations = self.generate_improvement_recommendations(error_analysis, inference_results)\n            \n            # 7. Generate reports\n            html_report_path = self.generate_detailed_html_report(\n                inference_results, error_analysis, recommendations, \n                visualization_paths, chart_paths\n            )\n            \n            json_report_path = self.save_analysis_results(\n                inference_results, error_analysis, recommendations\n            )\n            \n            print(f\"\\nAnalysis completed for {self.model_name}!\")\n            print(f\"HTML Report: {html_report_path}\")\n            print(f\"JSON Report: {json_report_path}\")\n            \n            return {\n                'success': True,\n                'model_name': self.model_name,\n                'html_report': html_report_path,\n                'json_report': json_report_path,\n                'visualizations': visualization_paths,\n                'charts': chart_paths,\n                'recommendations': recommendations,\n                'summary': {\n                    'pytorch_accuracy': (len(inference_results['ground_truth']) - len(error_analysis['pytorch_errors'])) / len(inference_results['ground_truth']) * 100,\n                    'onnx_accuracy': (len(inference_results['ground_truth']) - len(error_analysis['onnx_errors'])) / len(inference_results['ground_truth']) * 100,\n                    'speedup': np.mean(inference_results['pytorch_times']) / np.mean(inference_results['onnx_times']) if np.mean(inference_results['onnx_times']) > 0 else 0,\n                    'disagreements': len(error_analysis['model_disagreements']),\n                    'overall_assessment': recommendations['overall_assessment']\n                }\n            }\n            \n        except Exception as e:\n            print(f\"Error during analysis: {e}\")\n            import traceback\n            traceback.print_exc()\n            return {'success': False, 'error': str(e)}\n\ndef analyze_single_model(model_name: str, model_config: Dict) -> Dict[str, Any]:\n    \"\"\"Analyze a single model\"\"\"\n    analyzer = IndividualModelAnalyzer(model_name, model_config)\n    return analyzer.run_complete_analysis()\n\ndef main():\n    \"\"\"Main function to analyze all successful models\"\"\"\n    \n    # Model configurations (successful models from batch validation)\n    successful_models = {\n        'resnet18_improved': {\n            'module': 'models.resnet_improved',\n            'factory_function': 'create_resnet18_improved',\n            'input_shape': (3, 70, 70),\n            'architecture_type': 'CNN',\n            'checkpoint_path': 'experiments/experiment_20250802_164948/resnet18_improved/best_model.pth'\n        },\n        'micro_vit': {\n            'module': 'models.micro_vit',\n            'factory_function': 'create_micro_vit',\n            'input_shape': (3, 70, 70),\n            'architecture_type': 'Vision Transformer',\n            'checkpoint_path': 'experiments/experiment_20250803_102845/micro_vit/best_model.pth'\n        },\n        'vit_tiny': {\n            'module': 'models.vit_tiny',\n            'factory_function': 'create_vit_tiny',\n            'input_shape': (3, 70, 70),\n            'architecture_type': 'Vision Transformer',\n            'checkpoint_path': 'experiments/experiment_20250803_020217/vit_tiny/best_model.pth'\n        },\n        'coatnet': {\n            'module': 'models.coatnet',\n            'factory_function': 'create_coatnet',\n            'input_shape': (3, 70, 70),\n            'architecture_type': 'Hybrid CNN-Transformer',\n            'checkpoint_path': 'experiments/experiment_20250803_032628/coatnet/best_model.pth'\n        }\n    }\n    \n    print(\"Starting detailed analysis for all successful models...\")\n    \n    results = {}\n    for model_name, config in successful_models.items():\n        print(f\"\\nAnalyzing {model_name}...\")\n        results[model_name] = analyze_single_model(model_name, config)\n    \n    # Print summary\n    print(f\"\\n{'='*80}\")\n    print(\"DETAILED ANALYSIS SUMMARY\")\n    print(f\"{'='*80}\")\n    \n    for model_name, result in results.items():\n        if result['success']:\n            summary = result['summary']\n            print(f\"\\n{model_name.upper()}:\")\n            print(f\"  PyTorch Accuracy: {summary['pytorch_accuracy']:.1f}%\")\n            print(f\"  ONNX Accuracy: {summary['onnx_accuracy']:.1f}%\")\n            print(f\"  Speedup: {summary['speedup']:.2f}x\")\n            print(f\"  Disagreements: {summary['disagreements']}\")\n            print(f\"  Assessment: {summary['overall_assessment']}\")\n            print(f\"  Report: {result['html_report']}\")\n        else:\n            print(f\"\\n{model_name.upper()}: FAILED - {result['error']}\")\n\nif __name__ == \"__main__\":\n    main()