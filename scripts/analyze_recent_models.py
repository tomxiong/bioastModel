#!/usr/bin/env python3
"""
åˆ†æåˆšè®­ç»ƒå®Œæˆçš„æ¨¡å‹ï¼Œç”Ÿæˆé”™è¯¯æ ·æœ¬æ¸…å•å’Œæ€§èƒ½åˆ†ææŠ¥å‘Š
"""

import sys
import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

from core.data_loader import MICDataLoader
from core.config.model_configs import get_model_config
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

def load_model(model_name, checkpoint_path):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    try:
        # è·å–æ¨¡å‹é…ç½®
        config = get_model_config(model_name)
        
        # åŠ¨æ€å¯¼å…¥æ¨¡å‹
        module_path = config['module_path']
        class_name = config['class_name']
        
        module = __import__(module_path, fromlist=[class_name])
        model_class = getattr(module, class_name)
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = model_class(num_classes=config['num_classes'])
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        
        model.eval()
        return model, config
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return None, None

def analyze_model_predictions(model, data_loader, model_name, device='cpu'):
    """åˆ†ææ¨¡å‹é¢„æµ‹ç»“æœ"""
    model.eval()
    all_predictions = []
    all_labels = []
    all_confidences = []
    all_probs = []
    
    print(f"ğŸ” åˆ†æ {model_name} çš„é¢„æµ‹ç»“æœ...")
    
    with torch.no_grad():
        for images, labels in data_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)
            confidences, predictions = torch.max(probs, dim=1)
            
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_confidences.extend(confidences.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    all_confidences = np.array(all_confidences)
    all_probs = np.array(all_probs)
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = np.mean(all_predictions == all_labels)
    print(f"âœ… {model_name} å‡†ç¡®ç‡: {accuracy:.4f}")
    
    # æ‰¾å‡ºé”™è¯¯æ ·æœ¬
    error_indices = np.where(all_predictions != all_labels)[0]
    print(f"âŒ é”™è¯¯æ ·æœ¬æ•°é‡: {len(error_indices)}")
    
    # ç”Ÿæˆé”™è¯¯æ ·æœ¬æ¸…å•
    error_analysis = []
    for idx in error_indices:
        error_type = "False Positive" if all_labels[idx] == 0 else "False Negative"
        error_analysis.append({
            'index': idx,
            'true_label': int(all_labels[idx]),
            'predicted_label': int(all_predictions[idx]),
            'confidence': float(all_confidences[idx]),
            'prob_negative': float(all_probs[idx][0]),
            'prob_positive': float(all_probs[idx][1]),
            'error_type': error_type
        })
    
    return {
        'accuracy': accuracy,
        'error_count': len(error_indices),
        'total_samples': len(all_labels),
        'error_analysis': error_analysis,
        'all_predictions': all_predictions.tolist(),
        'all_labels': all_labels.tolist(),
        'all_confidences': all_confidences.tolist()
    }

def generate_error_report(analysis_results, model_name, output_dir):
    """ç”Ÿæˆé”™è¯¯åˆ†ææŠ¥å‘Š"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜é”™è¯¯æ ·æœ¬æ¸…å•
    error_df = pd.DataFrame(analysis_results['error_analysis'])
    error_csv_path = os.path.join(output_dir, f'{model_name}_error_samples.csv')
    error_df.to_csv(error_csv_path, index=False)
    print(f"ğŸ“Š é”™è¯¯æ ·æœ¬æ¸…å•å·²ä¿å­˜: {error_csv_path}")
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    report = {
        'model_name': model_name,
        'analysis_time': datetime.now().isoformat(),
        'output_dir': output_dir,
        'total_samples': analysis_results['total_samples'],
        'accuracy': analysis_results['accuracy'],
        'error_count': analysis_results['error_count'],
        'error_rate': analysis_results['error_count'] / analysis_results['total_samples'],
        'false_positives': len([e for e in analysis_results['error_analysis'] if e['error_type'] == 'False Positive']),
        'false_negatives': len([e for e in analysis_results['error_analysis'] if e['error_type'] == 'False Negative']),
        'confidence_stats': {
            'mean_confidence': np.mean(analysis_results['all_confidences']),
            'std_confidence': np.std(analysis_results['all_confidences']),
            'min_confidence': np.min(analysis_results['all_confidences']),
            'max_confidence': np.max(analysis_results['all_confidences'])
        }
    }
    
    # ä¿å­˜JSONæŠ¥å‘Š
    report_json_path = os.path.join(output_dir, f'{model_name}_analysis_report.json')
    with open(report_json_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“‹ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_json_path}")
    
    return error_df, report

def find_latest_trained_model():
    """è‡ªåŠ¨æ‰¾åˆ°æœ€åä¸€ä¸ªè®­ç»ƒçš„æ¨¡å‹"""
    import glob
    
    # æŸ¥æ‰¾æ‰€æœ‰best_model.pthæ–‡ä»¶ï¼ŒæŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    model_files = glob.glob('experiments/*/best_model.pth') + glob.glob('experiments/*/*/best_model.pth')
    
    if not model_files:
        return None
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
    latest_model = max(model_files, key=os.path.getmtime)
    
    # ä»è·¯å¾„ä¸­æå–æ¨¡å‹ä¿¡æ¯
    path_parts = latest_model.split(os.sep)
    if len(path_parts) >= 3:
        model_name = path_parts[-2]  # æ¨¡å‹åç§°ä½œä¸ºç›®å½•å
        experiment_dir = os.path.dirname(latest_model)
        
        return {
            'name': model_name,
            'checkpoint': latest_model,
            'experiment_dir': experiment_dir
        }
    
    return None

def analyze_recent_models(target_model=None):
    """åˆ†ææŒ‡å®šæ¨¡å‹æˆ–æœ€è¿‘è®­ç»ƒçš„æ¨¡å‹"""
    
    if target_model:
        # åˆ†ææŒ‡å®šçš„æ¨¡å‹
        model_configs = {
            'efficientnet_v2_s': {
                'name': 'efficientnet_v2_s',
                'checkpoint': 'experiments/experiment_20250805_220537/efficientnet_v2_s/best_model.pth',
                'experiment_dir': 'experiments/experiment_20250805_220537/efficientnet_v2_s'
            },
            'ghostnet': {
                'name': 'ghostnet',
                'checkpoint': 'experiments/experiment_20250805_221601/ghostnet/best_model.pth',
                'experiment_dir': 'experiments/experiment_20250805_221601/ghostnet'
            },
            'densenet121': {
                'name': 'densenet121',
                'checkpoint': 'experiments/experiment_20250805_222613/densenet121/best_model.pth',
                'experiment_dir': 'experiments/experiment_20250805_222613/densenet121'
            }
        }
        
        if target_model not in model_configs:
            print(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹: {target_model}")
            print(f"æ”¯æŒçš„æ¨¡å‹: {list(model_configs.keys())}")
            return {}
        
        models_to_analyze = [model_configs[target_model]]
        print(f"ğŸ¯ åˆ†ææŒ‡å®šæ¨¡å‹: {target_model}")
        
    else:
        # è‡ªåŠ¨æ‰¾åˆ°æœ€åä¸€ä¸ªè®­ç»ƒçš„æ¨¡å‹
        latest_model = find_latest_trained_model()
        if latest_model:
            models_to_analyze = [latest_model]
            print(f"ğŸ” è‡ªåŠ¨åˆ†ææœ€åè®­ç»ƒçš„æ¨¡å‹: {latest_model['name']}")
        else:
            print("âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹")
            return {}
    
    # å‡†å¤‡æ•°æ®
    data_loader = MICDataLoader(data_dir='bioast_dataset')
    train_images, train_labels = data_loader.get_train_data()
    val_images, val_labels = data_loader.get_val_data()
    test_images, test_labels = data_loader.get_test_data()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
    from core.data_loader import MICDataset
    test_dataset = MICDataset(test_images, test_labels)
    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # åˆ†ææ¯ä¸ªæ¨¡å‹
    all_results = {}
    
    for model_info in recent_models:
        model_name = model_info['name']
        checkpoint_path = model_info['checkpoint']
        experiment_dir = model_info['experiment_dir']
        
        print(f"\n{'='*60}")
        print(f"ğŸ” åˆ†ææ¨¡å‹: {model_name}")
        print(f"{'='*60}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(checkpoint_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            continue
        
        # åŠ è½½æ¨¡å‹
        model, config = load_model(model_name, checkpoint_path)
        if model is None:
            continue
        
        # åˆ†æé¢„æµ‹ç»“æœ
        analysis_results = analyze_model_predictions(model, test_dataloader, model_name)
        
        # ç”ŸæˆæŠ¥å‘Š
        output_dir = os.path.join(experiment_dir, 'error_analysis')
        error_df, report = generate_error_report(analysis_results, model_name, output_dir)
        
        all_results[model_name] = {
            'analysis': analysis_results,
            'report': report,
            'error_df': error_df
        }
    
    # ç”Ÿæˆç»¼åˆæ¯”è¾ƒæŠ¥å‘Š
    generate_comparison_report(all_results)
    
    return all_results

def generate_comparison_report(all_results):
    """ç”Ÿæˆæ¨¡å‹æ¯”è¾ƒæŠ¥å‘Š"""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¾ƒæŠ¥å‘Š")
    print(f"{'='*60}")
    
    comparison_data = []
    for model_name, results in all_results.items():
        report = results['report']
        comparison_data.append({
            'Model': model_name,
            'Accuracy': f"{report['accuracy']:.4f}",
            'Error Rate': f"{report['error_rate']:.4f}",
            'Error Count': report['error_count'],
            'False Positives': report['false_positives'],
            'False Negatives': report['false_negatives'],
            'Mean Confidence': f"{report['confidence_stats']['mean_confidence']:.4f}"
        })
    
    # åˆ›å»ºæ¯”è¾ƒè¡¨æ ¼
    comparison_df = pd.DataFrame(comparison_data)
    print("\nğŸ“‹ æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:")
    print(comparison_df.to_string(index=False))
    
    # ä¿å­˜æ¯”è¾ƒæŠ¥å‘Š
    comparison_path = 'recent_models_comparison_report.csv'
    comparison_df.to_csv(comparison_path, index=False)
    print(f"\nğŸ“Š æ¯”è¾ƒæŠ¥å‘Šå·²ä¿å­˜: {comparison_path}")
    
    # æ˜¾ç¤ºé”™è¯¯æ ·æœ¬ç±»å‹åˆ†å¸ƒ
    print(f"\nğŸ” é”™è¯¯æ ·æœ¬ç±»å‹åˆ†æ:")
    for model_name, results in all_results.items():
        report = results['report']
        print(f"\n{model_name}:")
        print(f"  - å‡é˜³æ€§ (False Positive): {report['false_positives']} ä¸ª")
        print(f"  - å‡é˜´æ€§ (False Negative): {report['false_negatives']} ä¸ª")
        
        # æ˜¾ç¤ºé«˜ç½®ä¿¡åº¦é”™è¯¯æ ·æœ¬
        error_df = results['error_df']
        high_conf_errors = error_df[error_df['confidence'] > 0.8]
        if len(high_conf_errors) > 0:
            print(f"  - é«˜ç½®ä¿¡åº¦é”™è¯¯ (>0.8): {len(high_conf_errors)} ä¸ª")
            print(f"    {high_conf_errors[['error_type', 'confidence']].head().to_string(index=False)}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='åˆ†æè®­ç»ƒå¥½çš„æ¨¡å‹æ€§èƒ½å¹¶ç”Ÿæˆé”™è¯¯æ ·æœ¬æ¸…å•')
    parser.add_argument('--model', type=str, default=None,
                        help='æŒ‡å®šè¦åˆ†æçš„æ¨¡å‹åç§° (efficientnet_v2_s, ghostnet, densenet121)')
    parser.add_argument('--latest', action='store_true',
                        help='åˆ†ææœ€åè®­ç»ƒçš„æ¨¡å‹ (é»˜è®¤è¡Œä¸º)')
    parser.add_argument('--list', action='store_true',
                        help='åˆ—å‡ºæ‰€æœ‰å¯åˆ†æçš„æ¨¡å‹')
    
    args = parser.parse_args()
    
    if args.list:
        # åˆ—å‡ºæ‰€æœ‰å¯åˆ†æçš„æ¨¡å‹
        print("ğŸ“‹ å¯åˆ†æçš„æ¨¡å‹:")
        available_models = ['efficientnet_v2_s', 'ghostnet', 'densenet121']
        for model in available_models:
            print(f"  - {model}")
        
        # æ˜¾ç¤ºæœ€æ–°çš„æ¨¡å‹
        latest = find_latest_trained_model()
        if latest:
            print(f"\nğŸ” æœ€åè®­ç»ƒçš„æ¨¡å‹: {latest['name']}")
        return
    
    print("ğŸš€ å¼€å§‹åˆ†ææ¨¡å‹...")
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    # ç¡®å®šåˆ†æç›®æ ‡
    target_model = args.model if args.model else None
    
    # åˆ†ææ¨¡å‹
    all_results = analyze_recent_models(target_model)
    
    if not all_results:
        print("âŒ æ¨¡å‹åˆ†æå¤±è´¥")
        return
    
    print(f"\n{'='*60}")
    print(f"âœ… æ¨¡å‹åˆ†æå®Œæˆ!")
    print(f"{'='*60}")
    
    print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    for model_name in all_results.keys():
        print(f"  - {all_results[model_name]['report'].get('output_dir', model_name + '/error_analysis')}/{model_name}_error_samples.csv")
        print(f"  - {all_results[model_name]['report'].get('output_dir', model_name + '/error_analysis')}/{model_name}_analysis_report.json")

if __name__ == "__main__":
    main()