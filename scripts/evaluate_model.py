"""
Unified model evaluation script for all supported models.

Usage:
    python scripts/evaluate_model.py --model efficientnet_b0
    python scripts/evaluate_model.py --model resnet18_improved --experiment experiment_20250802_164948
    python scripts/evaluate_model.py --model efficientnet_b0 --output custom_report
"""

import sys
import os
import argparse
import json
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

from core.config import (
    get_model_config,
    get_latest_experiment_path,
    get_model_report_path,
    EXPERIMENTS_DIR,
    REPORTS_DIR
)

def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='Evaluate a trained model with standardized reporting')
    
    parser.add_argument(
        '--model', 
        type=str, 
        required=True,
        choices=['efficientnet_b0', 'resnet18_improved'],
        help='Model name to evaluate'
    )
    
    parser.add_argument(
        '--experiment',
        type=str,
        default='latest',
        help='Experiment ID to evaluate (default: latest)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='Custom output directory name'
    )
    
    parser.add_argument(
        '--format',
        type=str,
        default='all',
        choices=['html', 'json', 'txt', 'all'],
        help='Output format for reports'
    )
    
    parser.add_argument(
        '--include-samples',
        action='store_true',
        help='Include sample analysis in the report'
    )
    
    parser.add_argument(
        '--include-visualizations',
        action='store_true',
        default=True,
        help='Include visualization charts in the report'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Show what would be evaluated without actually running'
    )
    
    return parser.parse_args()

def find_experiment_path(model_name, experiment_id):
    """Find the experiment path for evaluation."""
    if experiment_id == 'latest':
        experiment_path = get_latest_experiment_path(model_name)
        if experiment_path is None:
            raise FileNotFoundError(f"No experiments found for model: {model_name}")
        return experiment_path
    
    # Look for specific experiment
    experiment_path = EXPERIMENTS_DIR / experiment_id / model_name
    if not experiment_path.exists():
        raise FileNotFoundError(f"Experiment not found: {experiment_path}")
    
    return experiment_path

def validate_experiment(experiment_path):
def validate_experiment(experiment_path):
    """Validate that experiment has required files for evaluation."""
    required_files = [
        'best_model.pth',
        'training_history.json'
    ]
    
    # config.json is optional for backward compatibility
    optional_files = ['config.json']
    
    missing_files = []
    for file in required_files:
        if not (experiment_path / file).exists():
            missing_files.append(file)
    
    if missing_files:
        raise FileNotFoundError(f"Missing required files in {experiment_path}: {missing_files}")
    
    # Check for optional files and warn if missing
    missing_optional = []
    for file in optional_files:
        if not (experiment_path / file).exists():
            missing_optional.append(file)
    
    if missing_optional:
        print(f"‚ö†Ô∏è  Optional files missing (using defaults): {missing_optional}")
    
    return True

def create_evaluation_config(args, experiment_path):
def create_evaluation_config(args, experiment_path):
    """Create evaluation configuration."""
    # Load experiment config if available, otherwise use defaults
    experiment_config = {}
    config_file = experiment_path / 'config.json'
    if config_file.exists():
        with open(config_file, 'r') as f:
            experiment_config = json.load(f)
    else:
        # Create default config for backward compatibility
        experiment_config = {
            'model_name': args.model,
            'created_by': 'legacy_experiment',
            'note': 'Config reconstructed for backward compatibility'
        }
    
    # Get model config
    model_config = get_model_config(args.model)
    
    # Determine output path
    if args.output:
        output_path = REPORTS_DIR / 'individual' / args.output
    else:
        output_path = get_model_report_path(args.model)
    
    config = {
        'model': {
            'name': args.model,
            **model_config
        },
        'experiment': {
            'path': str(experiment_path),
            'id': experiment_path.parent.name,
            'config': experiment_config
        },
        'evaluation': {
            'output_path': str(output_path),
            'formats': [args.format] if args.format != 'all' else ['html', 'json', 'txt'],
            'include_samples': args.include_samples,
            'include_visualizations': args.include_visualizations
        },
        'metadata': {
            'created_by': 'scripts/evaluate_model.py',
            'command_line': ' '.join(sys.argv)
        }
    }
    
    return config

def run_evaluation(config):
    """Run the actual evaluation process."""
    model_name = config['model']['name']
    experiment_path = Path(config['experiment']['path'])
    output_path = Path(config['evaluation']['output_path'])
    
    print(f"üîç Starting evaluation for {model_name}...")
    print(f"üìÅ Experiment: {config['experiment']['id']}")
    print(f"üìä Output path: {output_path}")
    
    # Create output directory
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Save evaluation config
    with open(output_path / 'evaluation_config.json', 'w') as f:
        json.dump(config, f, indent=2)
    
    # Run model-specific evaluation
    if model_name == 'efficientnet_b0':
        run_efficientnet_evaluation(experiment_path, output_path, config)
    elif model_name == 'resnet18_improved':
        run_resnet_evaluation(experiment_path, output_path, config)
    else:
        raise ValueError(f"Evaluation not implemented for model: {model_name}")

def run_efficientnet_evaluation(experiment_path, output_path, config):
    """Run EfficientNet-specific evaluation."""
    print("üìä Running EfficientNet evaluation...")
    
    # Check if evaluation already exists in experiment
    if (experiment_path / 'evaluation').exists():
        print("‚úÖ Using existing evaluation results")
        # Copy existing results to output path
        import shutil
        shutil.copytree(experiment_path / 'evaluation', output_path / 'evaluation', dirs_exist_ok=True)
        if (experiment_path / 'sample_analysis').exists():
            shutil.copytree(experiment_path / 'sample_analysis', output_path / 'sample_analysis', dirs_exist_ok=True)
        if (experiment_path / 'visualizations').exists():
            shutil.copytree(experiment_path / 'visualizations', output_path / 'visualizations', dirs_exist_ok=True)
    else:
        print("üîÑ Running new evaluation...")
        # Run comprehensive evaluation (would need to implement)
        print("‚ö†Ô∏è  New evaluation not yet implemented - using existing experiment results")

def run_resnet_evaluation(experiment_path, output_path, config):
    """Run ResNet-specific evaluation."""
    print("üìä Running ResNet evaluation...")
    
    # Check if evaluation already exists in experiment
    if (experiment_path / 'evaluation').exists():
        print("‚úÖ Using existing evaluation results")
        # Copy existing results to output path
        import shutil
        shutil.copytree(experiment_path / 'evaluation', output_path / 'evaluation', dirs_exist_ok=True)
        if (experiment_path / 'sample_analysis').exists():
            shutil.copytree(experiment_path / 'sample_analysis', output_path / 'sample_analysis', dirs_exist_ok=True)
        if (experiment_path / 'visualizations').exists():
            shutil.copytree(experiment_path / 'visualizations', output_path / 'visualizations', dirs_exist_ok=True)
    else:
        print("üîÑ Running new evaluation...")
        # Use the unified evaluation script
        from complete_resnet_evaluation_unified import main as run_unified_evaluation
        print("‚ö†Ô∏è  Using existing unified evaluation script")

def generate_reports(config):
    """Generate evaluation reports in requested formats."""
    output_path = Path(config['evaluation']['output_path'])
    formats = config['evaluation']['formats']
    
    print(f"üìù Generating reports in formats: {formats}")
    
    for format_type in formats:
        if format_type == 'html':
            generate_html_report(config, output_path)
        elif format_type == 'json':
            generate_json_report(config, output_path)
        elif format_type == 'txt':
            generate_txt_report(config, output_path)

def generate_html_report(config, output_path):
    """Generate HTML evaluation report."""
    print("üìÑ Generating HTML report...")
    # Implementation would use existing HTML generation logic
    print("‚úÖ HTML report generated")

def generate_json_report(config, output_path):
    """Generate JSON evaluation report."""
    print("üìÑ Generating JSON report...")
    # Implementation would create structured JSON report
    print("‚úÖ JSON report generated")

def generate_txt_report(config, output_path):
    """Generate text evaluation report."""
    print("üìÑ Generating text report...")
    # Implementation would create readable text report
    print("‚úÖ Text report generated")

def main():
    """Main function."""
    args = parse_arguments()
    
    print("üîç Unified Model Evaluation Script")
    print("=" * 50)
    
    try:
        # Find experiment path
        experiment_path = find_experiment_path(args.model, args.experiment)
        print(f"üìÅ Found experiment: {experiment_path}")
        
        # Validate experiment
        validate_experiment(experiment_path)
        print("‚úÖ Experiment validation passed")
        
        # Create evaluation configuration
        config = create_evaluation_config(args, experiment_path)
        
        if args.dry_run:
            print("üìã Evaluation Configuration (Dry Run):")
            print(json.dumps(config, indent=2))
            return
        
        # Run evaluation
        run_evaluation(config)
        
        # Generate reports
        generate_reports(config)
        
        print(f"\n‚úÖ Evaluation completed successfully!")
        print(f"üìÅ Reports saved to: {config['evaluation']['output_path']}")
        
    except Exception as e:
        print(f"‚ùå Evaluation failed: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()