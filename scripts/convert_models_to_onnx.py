#!/usr/bin/env python3
"""
æ¨¡å‹ONNXè½¬æ¢è„šæœ¬
å°†æ‰€æœ‰è®­ç»ƒå¥½çš„PyTorchæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼ï¼Œç”¨äºC#é¡¹ç›®éƒ¨ç½²

æ›´æ–°ï¼šå®ç°äº†é’ˆå¯¹æ¯ä¸ªæ¨¡å‹çš„å•ç‹¬è½¬æ¢å‡½æ•°ï¼Œä»¥è§£å†³ä¸åŒæ¨¡å‹æ¶æ„çš„ç‰¹æ®Šéœ€æ±‚
"""

import os
import sys
import torch
import torch.onnx
import json
import logging
import argparse
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æ¨¡å‹ç±»
import torch.nn as nn

# å®šä¹‰SimplifiedAirBubbleDetectorç±»ï¼ˆä»è®­ç»ƒè„šæœ¬å¤åˆ¶ï¼‰
class SimplifiedAirBubbleDetector(nn.Module):
    """ç®€åŒ–ç‰ˆæ°”å­”æ£€æµ‹å™¨ - è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜"""
    
    def __init__(self, input_channels: int = 3, num_classes: int = 2):
        super().__init__()
        
        # å¤§å¹…ç®€åŒ–çš„ç‰¹å¾æå–å™¨ (ç›®æ ‡: <100kå‚æ•°)
        self.features = nn.Sequential(
            # ç¬¬ä¸€å±‚: ä¿æŒåˆ†è¾¨ç‡
            nn.Conv2d(input_channels, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.1),
            
            # ç¬¬äºŒå±‚: è½»å¾®ä¸‹é‡‡æ ·
            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 35x35
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.2),
            
            # ç¬¬ä¸‰å±‚: ç‰¹å¾æå–
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.2),
            
            # ç¬¬å››å±‚: è¿›ä¸€æ­¥ä¸‹é‡‡æ ·
            nn.Conv2d(64, 128, 3, stride=2, padding=1),  # 18x18
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.3),
            
            # å…¨å±€å¹³å‡æ± åŒ–
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # ç®€åŒ–çš„åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        features = self.features(x)
        features = features.view(features.size(0), -1)
        output = self.classifier(features)
        return output

# å¯¼å…¥å…¶ä»–æ¨¡å‹ç±»
try:
    from models.efficientnet_b0 import EfficientNetB0
except ImportError:
    EfficientNetB0 = None

try:
    from models.resnet18_improved import ResNet18Improved
except ImportError:
    ResNet18Improved = None

try:
    from models.coatnet import CoAtNet
except ImportError:
    CoAtNet = None

try:
    from models.convnext_tiny import ConvNextTiny
except ImportError:
    ConvNextTiny = None

try:
    from models.vit_tiny import ViTTiny
except ImportError:
    ViTTiny = None

try:
    from models.airbubble_hybrid_net import AirBubbleHybridNet
except ImportError:
    AirBubbleHybridNet = None

try:
    from models.mic_mobilenetv3 import MICMobileNetV3
except ImportError:
    MICMobileNetV3 = None

try:
    from models.micro_vit import MicroViT
except ImportError:
    MicroViT = None

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('onnx_conversion.log'),
        logging.StreamHandler(sys.stdout)  # ç¡®ä¿è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡º
    ]
)
logger = logging.getLogger(__name__)

class ONNXConverter:
    """ONNXæ¨¡å‹è½¬æ¢å™¨"""
    
    def __init__(self):
        self.input_size = (1, 3, 70, 70)  # MICæµ‹è¯•å›¾åƒå°ºå¯¸
        self.output_dir = Path("deployment/onnx_models")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹é…ç½®æ˜ å°„
        self.model_configs = {
            "simplified_airbubble_detector": {
                "class": SimplifiedAirBubbleDetector,
                "checkpoint": "experiments/simplified_airbubble_detector/simplified_airbubble_best.pth",
                "priority": 1,  # æœ€é«˜ä¼˜å…ˆçº§
                "description": "Simplified AirBubble Detector - Champion Model (100% accuracy)",
                "converter_function": self.convert_simplified_airbubble_detector
            },
            "efficientnet_b0": {
                "class": EfficientNetB0,
                "checkpoint": "experiments/experiment_20250802_140818/efficientnet_b0/best_model.pth",
                "priority": 2,
                "description": "EfficientNet-B0 - Historical Baseline (98.14% accuracy)",
                "converter_function": self.convert_efficientnet_b0
            },
            "resnet18_improved": {
                "class": ResNet18Improved,
                "checkpoint": "experiments/experiment_20250802_164948/resnet18_improved/best_model.pth",
                "priority": 3,
                "description": "ResNet18-Improved - High Performance (97.83% accuracy)",
                "converter_function": self.convert_resnet18_improved
            },
            "coatnet": {
                "class": CoAtNet,
                "checkpoint": "experiments/experiment_20250803_032628/coatnet/best_model.pth",
                "priority": 4,
                "description": "CoAtNet - Hybrid Architecture (91.30% accuracy)",
                "converter_function": self.convert_coatnet
            },
            "convnext_tiny": {
                "class": ConvNextTiny,
                "checkpoint": "experiments/experiment_20250802_231639/convnext_tiny/best_model.pth",
                "priority": 5,
                "description": "ConvNeXt Tiny - Modern CNN (89.70% accuracy)",
                "converter_function": self.convert_convnext_tiny
            },
            "vit_tiny": {
                "class": ViTTiny,
                "checkpoint": "experiments/experiment_20250803_020217/vit_tiny/best_model.pth",
                "priority": 6,
                "description": "Vision Transformer Tiny - Attention-based (88.50% accuracy)",
                "converter_function": self.convert_vit_tiny
            },
            "airbubble_hybrid_net": {
                "class": AirBubbleHybridNet,
                "checkpoint": "experiments/experiment_20250803_115344/airbubble_hybrid_net/best_model.pth",
                "priority": 7,
                "description": "AirBubble Hybrid Net - Domain-specific (87.40% accuracy)",
                "converter_function": self.convert_airbubble_hybrid_net
            },
            "mic_mobilenetv3": {
                "class": MICMobileNetV3,
                "checkpoint": "experiments/experiment_20250803_101438/mic_mobilenetv3/best_model.pth",
                "priority": 8,
                "description": "MIC MobileNetV3 - Mobile Optimized (85.20% accuracy)",
                "converter_function": self.convert_mic_mobilenetv3
            },
            "micro_vit": {
                "class": MicroViT,
                "checkpoint": "experiments/experiment_20250803_102845/micro_vit/best_model.pth",
                "priority": 9,
                "description": "Micro ViT - Lightweight Transformer (83.60% accuracy)",
                "converter_function": self.convert_micro_vit
            },
            "enhanced_airbubble_detector": {
                "class": SimplifiedAirBubbleDetector,  # ä½¿ç”¨ç›¸åŒçš„ç±»ï¼Œä½†ä¸åŒçš„æƒé‡
                "checkpoint": "experiments/enhanced_airbubble_detector/best_model.pth",
                "priority": 10,
                "description": "Enhanced AirBubble Detector (52.00% accuracy - overfit)",
                "converter_function": self.convert_enhanced_airbubble_detector
            }
        }
        
    def load_model(self, model_name: str):
        """åŠ è½½PyTorchæ¨¡å‹"""
        try:
            config = self.model_configs[model_name]
            model_class = config["class"]
            checkpoint_path = config["checkpoint"]
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            if model_name == "simplified_airbubble_detector" or model_name == "enhanced_airbubble_detector":
                model = model_class(input_channels=3, num_classes=2)
            else:
                model = model_class(num_classes=2)
            
            # åŠ è½½æƒé‡
            if os.path.exists(checkpoint_path):
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)
                logger.info(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡: {checkpoint_path}")
            else:
                logger.warning(f"âš ï¸ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
                return None
                
            model.eval()
            return model
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½æ¨¡å‹
            if model_name == "simplified_airbubble_detector":
                try:
                    logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½ {model_name}...")
                    # æ£€æŸ¥æ¨¡å‹ç»“æ„æ˜¯å¦åŒ¹é…
                    logger.info(f"æ£€æŸ¥æ¨¡å‹ç»“æ„...")
                    
                    # åŠ è½½æ£€æŸ¥ç‚¹ä»¥æ£€æŸ¥ç»“æ„
                    if os.path.exists(checkpoint_path):
                        checkpoint = torch.load(checkpoint_path, map_location='cpu')
                        if 'model_state_dict' in checkpoint:
                            state_dict = checkpoint['model_state_dict']
                        else:
                            state_dict = checkpoint
                        
                        # æ‰“å°æ¨¡å‹ç»“æ„ä¿¡æ¯
                        logger.info(f"æ£€æŸ¥ç‚¹ä¸­çš„é”®: {state_dict.keys()}")
                        
                        # åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹å®ä¾‹
                        from scripts.train_simplified_airbubble_detector import SimplifiedAirBubbleDetector as TrainSimplifiedAirBubbleDetector
                        model = TrainSimplifiedAirBubbleDetector(input_channels=3, num_classes=2)
                        model.load_state_dict(state_dict)
                        model.eval()
                        logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
                        return model
                except Exception as e2:
                    logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
            return None
    
    # é€šç”¨è½¬æ¢å‡½æ•° - ä½œä¸ºåŸºç¡€å®ç°
    def convert_to_onnx(self, model_name: str, model: torch.nn.Module, opset_version=11):
        """å°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼ - é€šç”¨æ–¹æ³•"""
        try:
            # åˆ›å»ºç¤ºä¾‹è¾“å…¥
            dummy_input = torch.randn(self.input_size)
            
            # ONNXè¾“å‡ºè·¯å¾„
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # è½¬æ¢ä¸ºONNX
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=opset_version,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],  # [batch_size, num_classes]
                "success": True
            }
            
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
            
    def convert_resnet18_improved(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ResNet18Improvedæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ResNetæ˜¯æ ‡å‡†æ¶æ„ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
    
    def convert_coatnet(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢CoAtNetæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # CoAtNetåŒ…å«æ³¨æ„åŠ›æœºåˆ¶ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è½¬æ¢ {model_name}...")
            try:
                # ä½¿ç”¨torch.jit.traceåˆ›å»ºå¯è¿½è¸ªæ¨¡å‹
                dummy_input = torch.randn(self.input_size)
                traced_model = torch.jit.trace(model, dummy_input)
                
                onnx_path = self.output_dir / f"{model_name}.onnx"
                
                # å¯¼å‡ºè¿½è¸ªæ¨¡å‹
                torch.onnx.export(
                    traced_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=12,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                import onnx
                onnx_model = onnx.load(str(onnx_path))
                onnx.checker.check_model(onnx_model)
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
                
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
                
                return {
                    "model_name": model_name,
                    "onnx_path": str(onnx_path),
                    "file_size_mb": round(file_size, 2),
                    "input_shape": list(self.input_size),
                    "output_shape": [1, 2],
                    "success": True
                }
            except Exception as e2:
                logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
                return {
                    "model_name": model_name,
                    "success": False,
                    "error": f"ä¸»è¦é”™è¯¯: {str(e)}; å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(e2)}"
                }
    
    def convert_convnext_tiny(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ConvNextTinyæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ConvNeXtåŒ…å«ç‰¹æ®Šçš„LayerNormå®ç°ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
    
    def convert_vit_tiny(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ViTTinyæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # Vision Transformeréœ€è¦ç‰¹æ®Šå¤„ç†æ³¨æ„åŠ›æœºåˆ¶
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è½¬æ¢ {model_name}...")
            try:
                # ä½¿ç”¨torch.jit.scriptåˆ›å»ºè„šæœ¬æ¨¡å‹
                scripted_model = torch.jit.script(model)
                
                onnx_path = self.output_dir / f"{model_name}.onnx"
                dummy_input = torch.randn(self.input_size)
                
                # å¯¼å‡ºè„šæœ¬æ¨¡å‹
                torch.onnx.export(
                    scripted_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=12,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                import onnx
                onnx_model = onnx.load(str(onnx_path))
                onnx.checker.check_model(onnx_model)
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
                
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
                
                return {
                    "model_name": model_name,
                    "onnx_path": str(onnx_path),
                    "file_size_mb": round(file_size, 2),
                    "input_shape": list(self.input_size),
                    "output_shape": [1, 2],
                    "success": True
                }
            except Exception as e2:
                logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
                return {
                    "model_name": model_name,
                    "success": False,
                    "error": f"ä¸»è¦é”™è¯¯: {str(e)}; å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(e2)}"
                }
    
    def convert_airbubble_hybrid_net(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢AirBubbleHybridNetæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # æ··åˆç½‘ç»œæ¶æ„ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=12)
    
    def convert_mic_mobilenetv3(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢MICMobileNetV3æ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # MobileNetV3å¯èƒ½åŒ…å«ç‰¹æ®Šçš„æ¿€æ´»å‡½æ•°å’ŒSEæ¨¡å—
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ›´å¤šæ“ä½œ
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
    
    def convert_micro_vit(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢MicroViTæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # å¾®å‹ViTï¼Œä½¿ç”¨ä¸ViTç±»ä¼¼çš„æ–¹æ³•
        return self.convert_vit_tiny(model_name, model)
    
    def convert_enhanced_airbubble_detector(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢EnhancedAirBubbleDetectoræ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ä¸SimplifiedAirBubbleDetectorç±»ä¼¼ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
    
    # é’ˆå¯¹æ¯ä¸ªæ¨¡å‹çš„ä¸“ç”¨è½¬æ¢å‡½æ•°
    def convert_simplified_airbubble_detector(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢SimplifiedAirBubbleDetectoræ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ç®€å•æ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†è½¬æ¢æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
    
    def convert_efficientnet_b0(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢EfficientNetB0æ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # EfficientNetå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†æ¿€æ´»å‡½æ•°
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ›´å¤šæ“ä½œ
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è½¬æ¢ {model_name}...")
            try:
                # ä½¿ç”¨torch.jit.traceåˆ›å»ºå¯è¿½è¸ªæ¨¡å‹
                dummy_input = torch.randn(self.input_size)
                traced_model = torch.jit.trace(model, dummy_input)
                
                onnx_path = self.output_dir / f"{model_name}.onnx"
                
                # å¯¼å‡ºè¿½è¸ªæ¨¡å‹
                torch.onnx.export(
                    traced_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=12,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                import onnx
                onnx_model = onnx.load(str(onnx_path))
                onnx.checker.check_model(onnx_model)
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
                
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
                
                return {
                    "model_name": model_name,
                    "onnx_path": str(onnx_path),
                    "file_size_mb": round(file_size, 2),
                    "input_shape": list(self.input_size),
                    "output_shape": [1, 2],
                    "success": True
                }
            except Exception as e2:
                logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
                return {
                    "model_name": model_name,
                    "success": False,
                    "error": f"ä¸»è¦é”™è¯¯: {str(e)}; å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(e2)}"
                }
    
    def convert_resnet18_improved(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ResNet18Improvedæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ResNetæ˜¯æ ‡å‡†æ¶æ„ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
    
    def convert_coatnet(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢CoAtNetæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # CoAtNetåŒ…å«æ³¨æ„åŠ›æœºåˆ¶ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è½¬æ¢ {model_name}...")
            try:
                # ä½¿ç”¨torch.jit.traceåˆ›å»ºå¯è¿½è¸ªæ¨¡å‹
                dummy_input = torch.randn(self.input_size)
                traced_model = torch.jit.trace(model, dummy_input)
                
                onnx_path = self.output_dir / f"{model_name}.onnx"
                
                # å¯¼å‡ºè¿½è¸ªæ¨¡å‹
                torch.onnx.export(
                    traced_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=12,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                import onnx
                onnx_model = onnx.load(str(onnx_path))
                onnx.checker.check_model(onnx_model)
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
                
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
                
                return {
                    "model_name": model_name,
                    "onnx_path": str(onnx_path),
                    "file_size_mb": round(file_size, 2),
                    "input_shape": list(self.input_size),
                    "output_shape": [1, 2],
                    "success": True
                }
            except Exception as e2:
                logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
                return {
                    "model_name": model_name,
                    "success": False,
                    "error": f"ä¸»è¦é”™è¯¯: {str(e)}; å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(e2)}"
                }
    
    def convert_convnext_tiny(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ConvNextTinyæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ConvNeXtåŒ…å«ç‰¹æ®Šçš„LayerNormå®ç°ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
    
    def convert_vit_tiny(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ViTTinyæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # Vision Transformeréœ€è¦ç‰¹æ®Šå¤„ç†æ³¨æ„åŠ›æœºåˆ¶
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è½¬æ¢ {model_name}...")
            try:
                # ä½¿ç”¨torch.jit.scriptåˆ›å»ºè„šæœ¬æ¨¡å‹
                scripted_model = torch.jit.script(model)
                
                onnx_path = self.output_dir / f"{model_name}.onnx"
                dummy_input = torch.randn(self.input_size)
                
                # å¯¼å‡ºè„šæœ¬æ¨¡å‹
                torch.onnx.export(
                    scripted_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=12,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                import onnx
                onnx_model = onnx.load(str(onnx_path))
                onnx.checker.check_model(onnx_model)
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
                
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
                
                return {
                    "model_name": model_name,
                    "onnx_path": str(onnx_path),
                    "file_size_mb": round(file_size, 2),
                    "input_shape": list(self.input_size),
                    "output_shape": [1, 2],
                    "success": True
                }
            except Exception as e2:
                logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
                return {
                    "model_name": model_name,
                    "success": False,
                    "error": f"ä¸»è¦é”™è¯¯: {str(e)}; å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(e2)}"
                }
    
    def convert_airbubble_hybrid_net(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢AirBubbleHybridNetæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # æ··åˆç½‘ç»œæ¶æ„ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=12)
    
    def convert_mic_mobilenetv3(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢MICMobileNetV3æ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # MobileNetV3å¯èƒ½åŒ…å«ç‰¹æ®Šçš„æ¿€æ´»å‡½æ•°å’ŒSEæ¨¡å—
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ›´å¤šæ“ä½œ
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
    
    def convert_micro_vit(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢MicroViTæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # å¾®å‹ViTï¼Œä½¿ç”¨ä¸ViTç±»ä¼¼çš„æ–¹æ³•
        return self.convert_vit_tiny(model_name, model)
    
    def convert_enhanced_airbubble_detector(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢EnhancedAirBubbleDetectoræ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ä¸SimplifiedAirBubbleDetectorç±»ä¼¼ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
        
    # åˆ é™¤é‡å¤çš„load_modelæ–¹æ³•
#!/usr/bin/env python3
"""
æ¨¡å‹ONNXè½¬æ¢è„šæœ¬
å°†æ‰€æœ‰è®­ç»ƒå¥½çš„PyTorchæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼ï¼Œç”¨äºC#é¡¹ç›®éƒ¨ç½²

æ›´æ–°ï¼šå®ç°äº†é’ˆå¯¹æ¯ä¸ªæ¨¡å‹çš„å•ç‹¬è½¬æ¢å‡½æ•°ï¼Œä»¥è§£å†³ä¸åŒæ¨¡å‹æ¶æ„çš„ç‰¹æ®Šéœ€æ±‚
"""

import os
import sys
import torch
import torch.onnx
import json
import logging
import argparse
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æ¨¡å‹ç±»
import torch.nn as nn

# å®šä¹‰SimplifiedAirBubbleDetectorç±»ï¼ˆä»è®­ç»ƒè„šæœ¬å¤åˆ¶ï¼‰
class SimplifiedAirBubbleDetector(nn.Module):
    """ç®€åŒ–ç‰ˆæ°”å­”æ£€æµ‹å™¨ - è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜"""
    
    def __init__(self, input_channels: int = 3, num_classes: int = 2):
        super().__init__()
        
        # å¤§å¹…ç®€åŒ–çš„ç‰¹å¾æå–å™¨ (ç›®æ ‡: <100kå‚æ•°)
        self.features = nn.Sequential(
            # ç¬¬ä¸€å±‚: ä¿æŒåˆ†è¾¨ç‡
            nn.Conv2d(input_channels, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.1),
            
            # ç¬¬äºŒå±‚: è½»å¾®ä¸‹é‡‡æ ·
            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 35x35
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.2),
            
            # ç¬¬ä¸‰å±‚: ç‰¹å¾æå–
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.2),
            
            # ç¬¬å››å±‚: è¿›ä¸€æ­¥ä¸‹é‡‡æ ·
            nn.Conv2d(64, 128, 3, stride=2, padding=1),  # 18x18
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.3),
            
            # å…¨å±€å¹³å‡æ± åŒ–
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # ç®€åŒ–çš„åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        features = self.features(x)
        features = features.view(features.size(0), -1)
        output = self.classifier(features)
        return output

# å¯¼å…¥å…¶ä»–æ¨¡å‹ç±»
try:
    from models.efficientnet_b0 import EfficientNetB0
except ImportError:
    EfficientNetB0 = None

try:
    from models.resnet18_improved import ResNet18Improved
except ImportError:
    ResNet18Improved = None

try:
    from models.coatnet import CoAtNet
except ImportError:
    CoAtNet = None

try:
    from models.convnext_tiny import ConvNextTiny
except ImportError:
    ConvNextTiny = None

try:
    from models.vit_tiny import ViTTiny
except ImportError:
    ViTTiny = None

try:
    from models.airbubble_hybrid_net import AirBubbleHybridNet
except ImportError:
    AirBubbleHybridNet = None

try:
    from models.mic_mobilenetv3 import MICMobileNetV3
except ImportError:
    MICMobileNetV3 = None

try:
    from models.micro_vit import MicroViT
except ImportError:
    MicroViT = None

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('onnx_conversion.log'),
        logging.StreamHandler(sys.stdout)  # ç¡®ä¿è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡º
    ]
)
logger = logging.getLogger(__name__)

class ONNXConverter:
    """ONNXæ¨¡å‹è½¬æ¢å™¨"""
    
    def __init__(self):
        self.input_size = (1, 3, 70, 70)  # MICæµ‹è¯•å›¾åƒå°ºå¯¸
        self.output_dir = Path("deployment/onnx_models")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹é…ç½®æ˜ å°„
        self.model_configs = {
            "simplified_airbubble_detector": {
                "class": SimplifiedAirBubbleDetector,
                "checkpoint": "experiments/simplified_airbubble_detector/simplified_airbubble_best.pth",
                "priority": 1,  # æœ€é«˜ä¼˜å…ˆçº§
                "description": "Simplified AirBubble Detector - Champion Model (100% accuracy)",
                "converter_function": self.convert_simplified_airbubble_detector
            },
            "efficientnet_b0": {
                "class": EfficientNetB0,
                "checkpoint": "experiments/experiment_20250802_140818/efficientnet_b0/best_model.pth",
                "priority": 2,
                "description": "EfficientNet-B0 - Historical Baseline (98.14% accuracy)",
                "converter_function": self.convert_efficientnet_b0
            },
            "resnet18_improved": {
                "class": ResNet18Improved,
                "checkpoint": "experiments/experiment_20250802_164948/resnet18_improved/best_model.pth",
                "priority": 3,
                "description": "ResNet18-Improved - High Performance (97.83% accuracy)",
                "converter_function": self.convert_resnet18_improved
            },
            "coatnet": {
                "class": CoAtNet,
                "checkpoint": "experiments/experiment_20250803_032628/coatnet/best_model.pth",
                "priority": 4,
                "description": "CoAtNet - Hybrid Architecture (91.30% accuracy)",
                "converter_function": self.convert_coatnet
            },
            "convnext_tiny": {
                "class": ConvNextTiny,
                "checkpoint": "experiments/experiment_20250802_231639/convnext_tiny/best_model.pth",
                "priority": 5,
                "description": "ConvNeXt Tiny - Modern CNN (89.70% accuracy)",
                "converter_function": self.convert_convnext_tiny
            },
            "vit_tiny": {
                "class": ViTTiny,
                "checkpoint": "experiments/experiment_20250803_020217/vit_tiny/best_model.pth",
                "priority": 6,
                "description": "Vision Transformer Tiny - Attention-based (88.50% accuracy)",
                "converter_function": self.convert_vit_tiny
            },
            "airbubble_hybrid_net": {
                "class": AirBubbleHybridNet,
                "checkpoint": "experiments/experiment_20250803_115344/airbubble_hybrid_net/best_model.pth",
                "priority": 7,
                "description": "AirBubble Hybrid Net - Domain-specific (87.40% accuracy)",
                "converter_function": self.convert_airbubble_hybrid_net
            },
            "mic_mobilenetv3": {
                "class": MICMobileNetV3,
                "checkpoint": "experiments/experiment_20250803_101438/mic_mobilenetv3/best_model.pth",
                "priority": 8,
                "description": "MIC MobileNetV3 - Mobile Optimized (85.20% accuracy)",
                "converter_function": self.convert_mic_mobilenetv3
            },
            "micro_vit": {
                "class": MicroViT,
                "checkpoint": "experiments/experiment_20250803_102845/micro_vit/best_model.pth",
                "priority": 9,
                "description": "Micro ViT - Lightweight Transformer (83.60% accuracy)",
                "converter_function": self.convert_micro_vit
            },
            "enhanced_airbubble_detector": {
                "class": SimplifiedAirBubbleDetector,  # ä½¿ç”¨ç›¸åŒçš„ç±»ï¼Œä½†ä¸åŒçš„æƒé‡
                "checkpoint": "experiments/enhanced_airbubble_detector/best_model.pth",
                "priority": 10,
                "description": "Enhanced AirBubble Detector (52.00% accuracy - overfit)",
                "converter_function": self.convert_enhanced_airbubble_detector
            }
        }
        
    def load_model(self, model_name: str):
        """åŠ è½½PyTorchæ¨¡å‹"""
        try:
            config = self.model_configs[model_name]
            model_class = config["class"]
            checkpoint_path = config["checkpoint"]
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            if model_name == "simplified_airbubble_detector" or model_name == "enhanced_airbubble_detector":
                model = model_class(input_channels=3, num_classes=2)
            else:
                model = model_class(num_classes=2)
            
            # åŠ è½½æƒé‡
            if os.path.exists(checkpoint_path):
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)
                logger.info(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡: {checkpoint_path}")
            else:
                logger.warning(f"âš ï¸ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
                return None
                
            model.eval()
            return model
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½æ¨¡å‹
            if model_name == "simplified_airbubble_detector":
                try:
                    logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½ {model_name}...")
                    # æ£€æŸ¥æ¨¡å‹ç»“æ„æ˜¯å¦åŒ¹é…
                    logger.info(f"æ£€æŸ¥æ¨¡å‹ç»“æ„...")
                    
                    # åŠ è½½æ£€æŸ¥ç‚¹ä»¥æ£€æŸ¥ç»“æ„
                    if os.path.exists(checkpoint_path):
                        checkpoint = torch.load(checkpoint_path, map_location='cpu')
                        if 'model_state_dict' in checkpoint:
                            state_dict = checkpoint['model_state_dict']
                        else:
                            state_dict = checkpoint
                        
                        # æ‰“å°æ¨¡å‹ç»“æ„ä¿¡æ¯
                        logger.info(f"æ£€æŸ¥ç‚¹ä¸­çš„é”®: {state_dict.keys()}")
                        
                        # åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹å®ä¾‹
                        from scripts.train_simplified_airbubble_detector import SimplifiedAirBubbleDetector as TrainSimplifiedAirBubbleDetector
                        model = TrainSimplifiedAirBubbleDetector(input_channels=3, num_classes=2)
                        model.load_state_dict(state_dict)
                        model.eval()
                        logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
                        return model
                except Exception as e2:
                    logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
            return None
    
    # é€šç”¨è½¬æ¢å‡½æ•° - ä½œä¸ºåŸºç¡€å®ç°
    def convert_to_onnx(self, model_name: str, model: torch.nn.Module, opset_version=11):
        """å°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼ - é€šç”¨æ–¹æ³•"""
        try:
            # åˆ›å»ºç¤ºä¾‹è¾“å…¥
            dummy_input = torch.randn(self.input_size)
            
            # ONNXè¾“å‡ºè·¯å¾„
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # è½¬æ¢ä¸ºONNX
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=opset_version,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],  # [batch_size, num_classes]
                "success": True
            }
            
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
            
    def convert_resnet18_improved(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ResNet18Improvedæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ResNetæ˜¯æ ‡å‡†æ¶æ„ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
    
    def convert_coatnet(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢CoAtNetæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # CoAtNetåŒ…å«æ³¨æ„åŠ›æœºåˆ¶ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è½¬æ¢ {model_name}...")
            try:
                # ä½¿ç”¨torch.jit.traceåˆ›å»ºå¯è¿½è¸ªæ¨¡å‹
                dummy_input = torch.randn(self.input_size)
                traced_model = torch.jit.trace(model, dummy_input)
                
                onnx_path = self.output_dir / f"{model_name}.onnx"
                
                # å¯¼å‡ºè¿½è¸ªæ¨¡å‹
                torch.onnx.export(
                    traced_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=12,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                import onnx
                onnx_model = onnx.load(str(onnx_path))
                onnx.checker.check_model(onnx_model)
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
                
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
                
                return {
                    "model_name": model_name,
                    "onnx_path": str(onnx_path),
                    "file_size_mb": round(file_size, 2),
                    "input_shape": list(self.input_size),
                    "output_shape": [1, 2],
                    "success": True
                }
            except Exception as e2:
                logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
                return {
                    "model_name": model_name,
                    "success": False,
                    "error": f"ä¸»è¦é”™è¯¯: {str(e)}; å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(e2)}"
                }
    
    def convert_convnext_tiny(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ConvNextTinyæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ConvNeXtåŒ…å«ç‰¹æ®Šçš„LayerNormå®ç°ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
    
    def convert_vit_tiny(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ViTTinyæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # Vision Transformeréœ€è¦ç‰¹æ®Šå¤„ç†æ³¨æ„åŠ›æœºåˆ¶
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è½¬æ¢ {model_name}...")
            try:
                # ä½¿ç”¨torch.jit.scriptåˆ›å»ºè„šæœ¬æ¨¡å‹
                scripted_model = torch.jit.script(model)
                
                onnx_path = self.output_dir / f"{model_name}.onnx"
                dummy_input = torch.randn(self.input_size)
                
                # å¯¼å‡ºè„šæœ¬æ¨¡å‹
                torch.onnx.export(
                    scripted_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=12,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                import onnx
                onnx_model = onnx.load(str(onnx_path))
                onnx.checker.check_model(onnx_model)
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
                
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
                
                return {
                    "model_name": model_name,
                    "onnx_path": str(onnx_path),
                    "file_size_mb": round(file_size, 2),
                    "input_shape": list(self.input_size),
                    "output_shape": [1, 2],
                    "success": True
                }
            except Exception as e2:
                logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
                return {
                    "model_name": model_name,
                    "success": False,
                    "error": f"ä¸»è¦é”™è¯¯: {str(e)}; å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(e2)}"
                }
    
    def convert_airbubble_hybrid_net(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢AirBubbleHybridNetæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # æ··åˆç½‘ç»œæ¶æ„ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=12)
    
    def convert_mic_mobilenetv3(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢MICMobileNetV3æ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # MobileNetV3å¯èƒ½åŒ…å«ç‰¹æ®Šçš„æ¿€æ´»å‡½æ•°å’ŒSEæ¨¡å—
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ›´å¤šæ“ä½œ
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
    
    def convert_micro_vit(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢MicroViTæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # å¾®å‹ViTï¼Œä½¿ç”¨ä¸ViTç±»ä¼¼çš„æ–¹æ³•
        return self.convert_vit_tiny(model_name, model)
    
    def convert_enhanced_airbubble_detector(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢EnhancedAirBubbleDetectoræ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ä¸SimplifiedAirBubbleDetectorç±»ä¼¼ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
    
    # é’ˆå¯¹æ¯ä¸ªæ¨¡å‹çš„ä¸“ç”¨è½¬æ¢å‡½æ•°
    def convert_simplified_airbubble_detector(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢SimplifiedAirBubbleDetectoræ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ç®€å•æ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†è½¬æ¢æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
    
    def convert_efficientnet_b0(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢EfficientNetB0æ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # EfficientNetå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†æ¿€æ´»å‡½æ•°
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ›´å¤šæ“ä½œ
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è½¬æ¢ {model_name}...")
            try:
                # ä½¿ç”¨torch.jit.traceåˆ›å»ºå¯è¿½è¸ªæ¨¡å‹
                dummy_input = torch.randn(self.input_size)
                traced_model = torch.jit.trace(model, dummy_input)
                
                onnx_path = self.output_dir / f"{model_name}.onnx"
                
                # å¯¼å‡ºè¿½è¸ªæ¨¡å‹
                torch.onnx.export(
                    traced_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=12,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                import onnx
                onnx_model = onnx.load(str(onnx_path))
                onnx.checker.check_model(onnx_model)
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
                
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
                
                return {
                    "model_name": model_name,
                    "onnx_path": str(onnx_path),
                    "file_size_mb": round(file_size, 2),
                    "input_shape": list(self.input_size),
                    "output_shape": [1, 2],
                    "success": True
                }
            except Exception as e2:
                logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
                return {
                    "model_name": model_name,
                    "success": False,
                    "error": f"ä¸»è¦é”™è¯¯: {str(e)}; å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(e2)}"
                }
    
    def convert_resnet18_improved(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ResNet18Improvedæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ResNetæ˜¯æ ‡å‡†æ¶æ„ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
    
    def convert_coatnet(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢CoAtNetæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # CoAtNetåŒ…å«æ³¨æ„åŠ›æœºåˆ¶ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è½¬æ¢ {model_name}...")
            try:
                # ä½¿ç”¨torch.jit.traceåˆ›å»ºå¯è¿½è¸ªæ¨¡å‹
                dummy_input = torch.randn(self.input_size)
                traced_model = torch.jit.trace(model, dummy_input)
                
                onnx_path = self.output_dir / f"{model_name}.onnx"
                
                # å¯¼å‡ºè¿½è¸ªæ¨¡å‹
                torch.onnx.export(
                    traced_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=12,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                import onnx
                onnx_model = onnx.load(str(onnx_path))
                onnx.checker.check_model(onnx_model)
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
                
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
                
                return {
                    "model_name": model_name,
                    "onnx_path": str(onnx_path),
                    "file_size_mb": round(file_size, 2),
                    "input_shape": list(self.input_size),
                    "output_shape": [1, 2],
                    "success": True
                }
            except Exception as e2:
                logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
                return {
                    "model_name": model_name,
                    "success": False,
                    "error": f"ä¸»è¦é”™è¯¯: {str(e)}; å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(e2)}"
                }
    
    def convert_convnext_tiny(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ConvNextTinyæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ConvNeXtåŒ…å«ç‰¹æ®Šçš„LayerNormå®ç°ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
    
    def convert_vit_tiny(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ViTTinyæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # Vision Transformeréœ€è¦ç‰¹æ®Šå¤„ç†æ³¨æ„åŠ›æœºåˆ¶
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è½¬æ¢ {model_name}...")
            try:
                # ä½¿ç”¨torch.jit.scriptåˆ›å»ºè„šæœ¬æ¨¡å‹
                scripted_model = torch.jit.script(model)
                
                onnx_path = self.output_dir / f"{model_name}.onnx"
                dummy_input = torch.randn(self.input_size)
                
                # å¯¼å‡ºè„šæœ¬æ¨¡å‹
                torch.onnx.export(
                    scripted_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=12,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                import onnx
                onnx_model = onnx.load(str(onnx_path))
                onnx.checker.check_model(onnx_model)
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
                
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
                
                return {
                    "model_name": model_name,
                    "onnx_path": str(onnx_path),
                    "file_size_mb": round(file_size, 2),
                    "input_shape": list(self.input_size),
                    "output_shape": [1, 2],
                    "success": True
                }
            except Exception as e2:
                logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
                return {
                    "model_name": model_name,
                    "success": False,
                    "error": f"ä¸»è¦é”™è¯¯: {str(e)}; å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(e2)}"
                }
    
    def convert_airbubble_hybrid_net(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢AirBubbleHybridNetæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # æ··åˆç½‘ç»œæ¶æ„ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=12)
    
    def convert_mic_mobilenetv3(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢MICMobileNetV3æ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # MobileNetV3å¯èƒ½åŒ…å«ç‰¹æ®Šçš„æ¿€æ´»å‡½æ•°å’ŒSEæ¨¡å—
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ›´å¤šæ“ä½œ
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
    
    def convert_micro_vit(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢MicroViTæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # å¾®å‹ViTï¼Œä½¿ç”¨ä¸ViTç±»ä¼¼çš„æ–¹æ³•
        return self.convert_vit_tiny(model_name, model)
    
    def convert_enhanced_airbubble_detector(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢EnhancedAirBubbleDetectoræ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ä¸SimplifiedAirBubbleDetectorç±»ä¼¼ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
        
#!/usr/bin/env python3
"""
æ¨¡å‹ONNXè½¬æ¢è„šæœ¬
å°†æ‰€æœ‰è®­ç»ƒå¥½çš„PyTorchæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼ï¼Œç”¨äºC#é¡¹ç›®éƒ¨ç½²

æ›´æ–°ï¼šå®ç°äº†é’ˆå¯¹æ¯ä¸ªæ¨¡å‹çš„å•ç‹¬è½¬æ¢å‡½æ•°ï¼Œä»¥è§£å†³ä¸åŒæ¨¡å‹æ¶æ„çš„ç‰¹æ®Šéœ€æ±‚
"""

import os
import sys
import torch
import torch.onnx
import json
import logging
import argparse
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æ¨¡å‹ç±»
import torch.nn as nn

# å®šä¹‰SimplifiedAirBubbleDetectorç±»ï¼ˆä»è®­ç»ƒè„šæœ¬å¤åˆ¶ï¼‰
class SimplifiedAirBubbleDetector(nn.Module):
    """ç®€åŒ–ç‰ˆæ°”å­”æ£€æµ‹å™¨ - è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜"""
    
    def __init__(self, input_channels: int = 3, num_classes: int = 2):
        super().__init__()
        
        # å¤§å¹…ç®€åŒ–çš„ç‰¹å¾æå–å™¨ (ç›®æ ‡: <100kå‚æ•°)
        self.features = nn.Sequential(
            # ç¬¬ä¸€å±‚: ä¿æŒåˆ†è¾¨ç‡
            nn.Conv2d(input_channels, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.1),
            
            # ç¬¬äºŒå±‚: è½»å¾®ä¸‹é‡‡æ ·
            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 35x35
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.2),
            
            # ç¬¬ä¸‰å±‚: ç‰¹å¾æå–
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.2),
            
            # ç¬¬å››å±‚: è¿›ä¸€æ­¥ä¸‹é‡‡æ ·
            nn.Conv2d(64, 128, 3, stride=2, padding=1),  # 18x18
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.3),
            
            # å…¨å±€å¹³å‡æ± åŒ–
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # ç®€åŒ–çš„åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        features = self.features(x)
        features = features.view(features.size(0), -1)
        output = self.classifier(features)
        return output

# å¯¼å…¥å…¶ä»–æ¨¡å‹ç±»
try:
    from models.efficientnet_b0 import EfficientNetB0
except ImportError:
    EfficientNetB0 = None

try:
    from models.resnet18_improved import ResNet18Improved
except ImportError:
    ResNet18Improved = None

try:
    from models.coatnet import CoAtNet
except ImportError:
    CoAtNet = None

try:
    from models.convnext_tiny import ConvNextTiny
except ImportError:
    ConvNextTiny = None

try:
    from models.vit_tiny import ViTTiny
except ImportError:
    ViTTiny = None

try:
    from models.airbubble_hybrid_net import AirBubbleHybridNet
except ImportError:
    AirBubbleHybridNet = None

try:
    from models.mic_mobilenetv3 import MICMobileNetV3
except ImportError:
    MICMobileNetV3 = None

try:
    from models.micro_vit import MicroViT
except ImportError:
    MicroViT = None

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('onnx_conversion.log'),
        logging.StreamHandler(sys.stdout)  # ç¡®ä¿è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡º
    ]
)
logger = logging.getLogger(__name__)

class ONNXConverter:
    """ONNXæ¨¡å‹è½¬æ¢å™¨"""
    
    def __init__(self):
        self.input_size = (1, 3, 70, 70)  # MICæµ‹è¯•å›¾åƒå°ºå¯¸
        self.output_dir = Path("deployment/onnx_models")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹é…ç½®æ˜ å°„
        self.model_configs = {
            "simplified_airbubble_detector": {
                "class": SimplifiedAirBubbleDetector,
                "checkpoint": "experiments/simplified_airbubble_detector/simplified_airbubble_best.pth",
                "priority": 1,  # æœ€é«˜ä¼˜å…ˆçº§
                "description": "Simplified AirBubble Detector - Champion Model (100% accuracy)",
                "converter_function": self.convert_simplified_airbubble_detector
            },
            "efficientnet_b0": {
                "class": EfficientNetB0,
                "checkpoint": "experiments/experiment_20250802_140818/efficientnet_b0/best_model.pth",
                "priority": 2,
                "description": "EfficientNet-B0 - Historical Baseline (98.14% accuracy)",
                "converter_function": self.convert_efficientnet_b0
            },
            "resnet18_improved": {
                "class": ResNet18Improved,
                "checkpoint": "experiments/experiment_20250802_164948/resnet18_improved/best_model.pth",
                "priority": 3,
                "description": "ResNet18-Improved - High Performance (97.83% accuracy)",
                "converter_function": self.convert_resnet18_improved
            },
            "coatnet": {
                "class": CoAtNet,
                "checkpoint": "experiments/experiment_20250803_032628/coatnet/best_model.pth",
                "priority": 4,
                "description": "CoAtNet - Hybrid Architecture (91.30% accuracy)",
                "converter_function": self.convert_coatnet
            },
            "convnext_tiny": {
                "class": ConvNextTiny,
                "checkpoint": "experiments/experiment_20250802_231639/convnext_tiny/best_model.pth",
                "priority": 5,
                "description": "ConvNeXt Tiny - Modern CNN (89.70% accuracy)",
                "converter_function": self.convert_convnext_tiny
            },
            "vit_tiny": {
                "class": ViTTiny,
                "checkpoint": "experiments/experiment_20250803_020217/vit_tiny/best_model.pth",
                "priority": 6,
                "description": "Vision Transformer Tiny - Attention-based (88.50% accuracy)",
                "converter_function": self.convert_vit_tiny
            },
            "airbubble_hybrid_net": {
                "class": AirBubbleHybridNet,
                "checkpoint": "experiments/experiment_20250803_115344/airbubble_hybrid_net/best_model.pth",
                "priority": 7,
                "description": "AirBubble Hybrid Net - Domain-specific (87.40% accuracy)",
                "converter_function": self.convert_airbubble_hybrid_net
            },
            "mic_mobilenetv3": {
                "class": MICMobileNetV3,
                "checkpoint": "experiments/experiment_20250803_101438/mic_mobilenetv3/best_model.pth",
                "priority": 8,
                "description": "MIC MobileNetV3 - Mobile Optimized (85.20% accuracy)",
                "converter_function": self.convert_mic_mobilenetv3
            },
            "micro_vit": {
                "class": MicroViT,
                "checkpoint": "experiments/experiment_20250803_102845/micro_vit/best_model.pth",
                "priority": 9,
                "description": "Micro ViT - Lightweight Transformer (83.60% accuracy)",
                "converter_function": self.convert_micro_vit
            },
            "enhanced_airbubble_detector": {
                "class": SimplifiedAirBubbleDetector,  # ä½¿ç”¨ç›¸åŒçš„ç±»ï¼Œä½†ä¸åŒçš„æƒé‡
                "checkpoint": "experiments/enhanced_airbubble_detector/best_model.pth",
                "priority": 10,
                "description": "Enhanced AirBubble Detector (52.00% accuracy - overfit)",
                "converter_function": self.convert_enhanced_airbubble_detector
            }
        }
        
    def load_model(self, model_name: str):
        """åŠ è½½PyTorchæ¨¡å‹"""
        try:
            config = self.model_configs[model_name]
            model_class = config["class"]
            checkpoint_path = config["checkpoint"]
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            if model_name == "simplified_airbubble_detector" or model_name == "enhanced_airbubble_detector":
                model = model_class(input_channels=3, num_classes=2)
            else:
                model = model_class(num_classes=2)
            
            # åŠ è½½æƒé‡
            if os.path.exists(checkpoint_path):
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)
                logger.info(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡: {checkpoint_path}")
            else:
                logger.warning(f"âš ï¸ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
                return None
                
            model.eval()
            return model
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½æ¨¡å‹
            if model_name == "simplified_airbubble_detector":
                try:
                    logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½ {model_name}...")
                    # æ£€æŸ¥æ¨¡å‹ç»“æ„æ˜¯å¦åŒ¹é…
                    logger.info(f"æ£€æŸ¥æ¨¡å‹ç»“æ„...")
                    
                    # åŠ è½½æ£€æŸ¥ç‚¹ä»¥æ£€æŸ¥ç»“æ„
                    if os.path.exists(checkpoint_path):
                        checkpoint = torch.load(checkpoint_path, map_location='cpu')
                        if 'model_state_dict' in checkpoint:
                            state_dict = checkpoint['model_state_dict']
                        else:
                            state_dict = checkpoint
                        
                        # æ‰“å°æ¨¡å‹ç»“æ„ä¿¡æ¯
                        logger.info(f"æ£€æŸ¥ç‚¹ä¸­çš„é”®: {state_dict.keys()}")
                        
                        # åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹å®ä¾‹
                        from scripts.train_simplified_airbubble_detector import SimplifiedAirBubbleDetector as TrainSimplifiedAirBubbleDetector
                        model = TrainSimplifiedAirBubbleDetector(input_channels=3, num_classes=2)
                        model.load_state_dict(state_dict)
                        model.eval()
                        logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
                        return model
                except Exception as e2:
                    logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
            return None
    
    # é€šç”¨è½¬æ¢å‡½æ•° - ä½œä¸ºåŸºç¡€å®ç°
    def convert_to_onnx(self, model_name: str, model: torch.nn.Module, opset_version=11):
        """å°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼ - é€šç”¨æ–¹æ³•"""
        try:
            # åˆ›å»ºç¤ºä¾‹è¾“å…¥
            dummy_input = torch.randn(self.input_size)
            
            # ONNXè¾“å‡ºè·¯å¾„
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # è½¬æ¢ä¸ºONNX
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=opset_version,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],  # [batch_size, num_classes]
                "success": True
            }
            
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
            
    def convert_resnet18_improved(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ResNet18Improvedæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ResNetæ˜¯æ ‡å‡†æ¶æ„ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
    
    def convert_coatnet(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢CoAtNetæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # CoAtNetåŒ…å«æ³¨æ„åŠ›æœºåˆ¶ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è½¬æ¢ {model_name}...")
            try:
                # ä½¿ç”¨torch.jit.traceåˆ›å»ºå¯è¿½è¸ªæ¨¡å‹
                dummy_input = torch.randn(self.input_size)
                traced_model = torch.jit.trace(model, dummy_input)
                
                onnx_path = self.output_dir / f"{model_name}.onnx"
                
                # å¯¼å‡ºè¿½è¸ªæ¨¡å‹
                torch.onnx.export(
                    traced_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=12,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                import onnx
                onnx_model = onnx.load(str(onnx_path))
                onnx.checker.check_model(onnx_model)
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
                
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
                
                return {
                    "model_name": model_name,
                    "onnx_path": str(onnx_path),
                    "file_size_mb": round(file_size, 2),
                    "input_shape": list(self.input_size),
                    "output_shape": [1, 2],
                    "success": True
                }
            except Exception as e2:
                logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
                return {
                    "model_name": model_name,
                    "success": False,
                    "error": f"ä¸»è¦é”™è¯¯: {str(e)}; å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(e2)}"
                }
    
    def convert_convnext_tiny(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ConvNextTinyæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ConvNeXtåŒ…å«ç‰¹æ®Šçš„LayerNormå®ç°ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
    
    def convert_vit_tiny(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ViTTinyæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # Vision Transformeréœ€è¦ç‰¹æ®Šå¤„ç†æ³¨æ„åŠ›æœºåˆ¶
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è½¬æ¢ {model_name}...")
            try:
                # ä½¿ç”¨torch.jit.scriptåˆ›å»ºè„šæœ¬æ¨¡å‹
                scripted_model = torch.jit.script(model)
                
                onnx_path = self.output_dir / f"{model_name}.onnx"
                dummy_input = torch.randn(self.input_size)
                
                # å¯¼å‡ºè„šæœ¬æ¨¡å‹
                torch.onnx.export(
                    scripted_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=12,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                import onnx
                onnx_model = onnx.load(str(onnx_path))
                onnx.checker.check_model(onnx_model)
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
                
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
                
                return {
                    "model_name": model_name,
                    "onnx_path": str(onnx_path),
                    "file_size_mb": round(file_size, 2),
                    "input_shape": list(self.input_size),
                    "output_shape": [1, 2],
                    "success": True
                }
            except Exception as e2:
                logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
                return {
                    "model_name": model_name,
                    "success": False,
                    "error": f"ä¸»è¦é”™è¯¯: {str(e)}; å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(e2)}"
                }
    
    def convert_airbubble_hybrid_net(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢AirBubbleHybridNetæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # æ··åˆç½‘ç»œæ¶æ„ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=12)
    
    def convert_mic_mobilenetv3(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢MICMobileNetV3æ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # MobileNetV3å¯èƒ½åŒ…å«ç‰¹æ®Šçš„æ¿€æ´»å‡½æ•°å’ŒSEæ¨¡å—
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ›´å¤šæ“ä½œ
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
    
    def convert_micro_vit(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢MicroViTæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # å¾®å‹ViTï¼Œä½¿ç”¨ä¸ViTç±»ä¼¼çš„æ–¹æ³•
        return self.convert_vit_tiny(model_name, model)
    
    def convert_enhanced_airbubble_detector(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢EnhancedAirBubbleDetectoræ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ä¸SimplifiedAirBubbleDetectorç±»ä¼¼ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
    
    # é’ˆå¯¹æ¯ä¸ªæ¨¡å‹çš„ä¸“ç”¨è½¬æ¢å‡½æ•°
    def convert_simplified_airbubble_detector(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢SimplifiedAirBubbleDetectoræ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ç®€å•æ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†è½¬æ¢æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
    
    def convert_efficientnet_b0(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢EfficientNetB0æ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # EfficientNetå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†æ¿€æ´»å‡½æ•°
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ›´å¤šæ“ä½œ
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è½¬æ¢ {model_name}...")
            try:
                # ä½¿ç”¨torch.jit.traceåˆ›å»ºå¯è¿½è¸ªæ¨¡å‹
                dummy_input = torch.randn(self.input_size)
                traced_model = torch.jit.trace(model, dummy_input)
                
                onnx_path = self.output_dir / f"{model_name}.onnx"
                
                # å¯¼å‡ºè¿½è¸ªæ¨¡å‹
                torch.onnx.export(
                    traced_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=12,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                import onnx
                onnx_model = onnx.load(str(onnx_path))
                onnx.checker.check_model(onnx_model)
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
                
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
                
                return {
                    "model_name": model_name,
                    "onnx_path": str(onnx_path),
                    "file_size_mb": round(file_size, 2),
                    "input_shape": list(self.input_size),
                    "output_shape": [1, 2],
                    "success": True
                }
            except Exception as e2:
                logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
                return {
                    "model_name": model_name,
                    "success": False,
                    "error": f"ä¸»è¦é”™è¯¯: {str(e)}; å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(e2)}"
                }
    
    def convert_resnet18_improved(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ResNet18Improvedæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ResNetæ˜¯æ ‡å‡†æ¶æ„ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
    
    def convert_coatnet(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢CoAtNetæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # CoAtNetåŒ…å«æ³¨æ„åŠ›æœºåˆ¶ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è½¬æ¢ {model_name}...")
            try:
                # ä½¿ç”¨torch.jit.traceåˆ›å»ºå¯è¿½è¸ªæ¨¡å‹
                dummy_input = torch.randn(self.input_size)
                traced_model = torch.jit.trace(model, dummy_input)
                
                onnx_path = self.output_dir / f"{model_name}.onnx"
                
                # å¯¼å‡ºè¿½è¸ªæ¨¡å‹
                torch.onnx.export(
                    traced_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=12,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                import onnx
                onnx_model = onnx.load(str(onnx_path))
                onnx.checker.check_model(onnx_model)
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
                
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
                
                return {
                    "model_name": model_name,
                    "onnx_path": str(onnx_path),
                    "file_size_mb": round(file_size, 2),
                    "input_shape": list(self.input_size),
                    "output_shape": [1, 2],
                    "success": True
                }
            except Exception as e2:
                logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
                return {
                    "model_name": model_name,
                    "success": False,
                    "error": f"ä¸»è¦é”™è¯¯: {str(e)}; å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(e2)}"
                }
    
    def convert_convnext_tiny(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ConvNextTinyæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ConvNeXtåŒ…å«ç‰¹æ®Šçš„LayerNormå®ç°ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
    
    def convert_vit_tiny(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢ViTTinyæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # Vision Transformeréœ€è¦ç‰¹æ®Šå¤„ç†æ³¨æ„åŠ›æœºåˆ¶
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬æ”¯æŒæ³¨æ„åŠ›æœºåˆ¶
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•
            logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è½¬æ¢ {model_name}...")
            try:
                # ä½¿ç”¨torch.jit.scriptåˆ›å»ºè„šæœ¬æ¨¡å‹
                scripted_model = torch.jit.script(model)
                
                onnx_path = self.output_dir / f"{model_name}.onnx"
                dummy_input = torch.randn(self.input_size)
                
                # å¯¼å‡ºè„šæœ¬æ¨¡å‹
                torch.onnx.export(
                    scripted_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=12,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                
                # éªŒè¯ONNXæ¨¡å‹
                import onnx
                onnx_model = onnx.load(str(onnx_path))
                onnx.checker.check_model(onnx_model)
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
                
                logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
                
                return {
                    "model_name": model_name,
                    "onnx_path": str(onnx_path),
                    "file_size_mb": round(file_size, 2),
                    "input_shape": list(self.input_size),
                    "output_shape": [1, 2],
                    "success": True
                }
            except Exception as e2:
                logger.error(f"âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥ {model_name}: {str(e2)}")
                return {
                    "model_name": model_name,
                    "success": False,
                    "error": f"ä¸»è¦é”™è¯¯: {str(e)}; å¤‡ç”¨æ–¹æ³•é”™è¯¯: {str(e2)}"
                }
    
    def convert_airbubble_hybrid_net(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢AirBubbleHybridNetæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # æ··åˆç½‘ç»œæ¶æ„ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=12)
    
    def convert_mic_mobilenetv3(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢MICMobileNetV3æ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # MobileNetV3å¯èƒ½åŒ…å«ç‰¹æ®Šçš„æ¿€æ´»å‡½æ•°å’ŒSEæ¨¡å—
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ›´å¤šæ“ä½œ
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
    
    def convert_micro_vit(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢MicroViTæ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # å¾®å‹ViTï¼Œä½¿ç”¨ä¸ViTç±»ä¼¼çš„æ–¹æ³•
        return self.convert_vit_tiny(model_name, model)
    
    def convert_enhanced_airbubble_detector(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢EnhancedAirBubbleDetectoræ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ä¸SimplifiedAirBubbleDetectorç±»ä¼¼ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
        
    def load_model(self, model_name: str):
        """åŠ è½½PyTorchæ¨¡å‹"""
        try:
            config = self.model_configs[model_name]
            model_class = config["class"]
            checkpoint_path = config["checkpoint"]
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            if model_name == "simplified_airbubble_detector" or model_name == "enhanced_airbubble_detector":
                model = model_class(input_channels=3, num_classes=2)
            else:
                model = model_class(num_classes=2)
            
            # åŠ è½½æƒé‡
            if os.path.exists(checkpoint_path):
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)
                logger.info(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡: {checkpoint_path}")
            else:
                logger.warning(f"âš ï¸ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
                return None
                
            model.eval()
            return model
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ {model_name}: {str(e)}")
            return None
    
    # é€šç”¨è½¬æ¢å‡½æ•° - ä½œä¸ºåŸºç¡€å®ç°
    def convert_to_onnx(self, model_name: str, model: torch.nn.Module, opset_version=11):
        """å°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼ - é€šç”¨æ–¹æ³•"""
        try:
            # åˆ›å»ºç¤ºä¾‹è¾“å…¥
            dummy_input = torch.randn(self.input_size)
            
            # ONNXè¾“å‡ºè·¯å¾„
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # è½¬æ¢ä¸ºONNX
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=opset_version,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],  # [batch_size, num_classes]
                "success": True
            }
            
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
    
    # é’ˆå¯¹æ¯ä¸ªæ¨¡å‹çš„ä¸“ç”¨è½¬æ¢å‡½æ•°
    def convert_simplified_airbubble_detector(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢SimplifiedAirBubbleDetectoræ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # ç®€å•æ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†è½¬æ¢æ–¹æ³•
        return self.convert_to_onnx(model_name, model, opset_version=11)
    
    def convert_efficientnet_b0(self, model_name: str, model: torch.nn.Module):
        """è½¬æ¢EfficientNetB0æ¨¡å‹"""
        logger.info(f"ğŸ”„ ä½¿ç”¨ä¸“ç”¨è½¬æ¢å‡½æ•°å¤„ç† {model_name}...")
        # EfficientNetå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†æ¿€æ´»å‡½æ•°
        try:
            dummy_input = torch.randn(self.input_size)
            onnx_path = self.output_dir / f"{model_name}.onnx"
            
            # ä½¿ç”¨æ›´é«˜çš„opsetç‰ˆæœ¬ä»¥æ”¯æŒæ›´å¤šæ“ä½œ
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=12,  # ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯ONNXæ¨¡å‹
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(onnx_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… æˆåŠŸè½¬æ¢ {model_name} -> {onnx_path} ({file_size:.2f} MB)")
            
            return {
                "model_name": model_name,
                "onnx_path": str(onnx_path),
                "file_size_mb": round(file_size, 2),
                "input_shape": list(self.input_size),
                "output_shape": [1, 2],
                "success": True
            }
        except Exception as e:
            logger.error(f"âŒ ONNXè½¬æ¢å¤±è´¥ {model_name}: {str(e)}")
            return {
                "model_name": model_name,
                "success": False,
                "error": str(e)
            }
    
    def test_onnx_model(self, onnx_path: str):
        """æµ‹è¯•ONNXæ¨¡å‹æ¨ç†"""
        try:
            import onnxruntime as ort
            
            # åˆ›å»ºæ¨ç†ä¼šè¯
            session = ort.InferenceSession(onnx_path)
            
            # åˆ›å»ºæµ‹è¯•è¾“å…¥
            test_input = torch.randn(self.input_size).numpy()
            
            # è¿è¡Œæ¨ç†
            outputs = session.run(None, {'input': test_input})
            
            logger.info(f"âœ… ONNXæ¨¡å‹æµ‹è¯•æˆåŠŸ: {onnx_path}")
            logger.info(f"   è¾“å‡ºå½¢çŠ¶: {outputs[0].shape}")
            logger.info(f"   è¾“å‡ºèŒƒå›´: [{outputs[0].min():.4f}, {outputs[0].max():.4f}]")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ONNXæ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}")
            return False
            
    def convert_single_model(self, model_name: str):
        """è½¬æ¢å•ä¸ªæŒ‡å®šçš„æ¨¡å‹"""
        if model_name not in self.model_configs:
            logger.error(f"âŒ æœªçŸ¥æ¨¡å‹: {model_name}")
            logger.info(f"å¯ç”¨æ¨¡å‹: {', '.join(self.model_configs.keys())}")
            return False
            
        logger.info(f"\nğŸ“¦ æ­£åœ¨å¤„ç†æ¨¡å‹: {model_name}")
        logger.info(f"   æè¿°: {self.model_configs[model_name]['description']}")
        
        # åŠ è½½æ¨¡å‹
        model = self.load_model(model_name)
        if model is None:
            return False
        
        # è·å–ä¸“ç”¨è½¬æ¢å‡½æ•°
        converter_function = self.model_configs[model_name].get('converter_function', self.convert_to_onnx)
        
        # è½¬æ¢ä¸ºONNX
        result = converter_function(model_name, model)
        
        # æµ‹è¯•ONNXæ¨¡å‹
        if result["success"]:
            if self.test_onnx_model(result["onnx_path"]):
                logger.info(f"âœ… æ¨¡å‹ {model_name} è½¬æ¢å¹¶æµ‹è¯•æˆåŠŸ!")
                return True
            else:
                logger.error(f"âŒ æ¨¡å‹ {model_name} è½¬æ¢æˆåŠŸä½†æµ‹è¯•å¤±è´¥")
                return False
        else:
            logger.error(f"âŒ æ¨¡å‹ {model_name} è½¬æ¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return False
    
    def generate_model_info(self, conversion_results: list):
        """ç”Ÿæˆæ¨¡å‹ä¿¡æ¯æ–‡ä»¶"""
        model_info = {
            "conversion_date": datetime.now().isoformat(),
            "input_shape": list(self.input_size),
            "output_shape": [1, 2],
            "models": []
        }
        
        for result in conversion_results:
            if result["success"]:
                model_name = result["model_name"]
                config = self.model_configs[model_name]
                
                model_info["models"].append({
                    "name": model_name,
                    "description": config["description"],
                    "priority": config["priority"],
                    "onnx_file": os.path.basename(result["onnx_path"]),
                    "file_size_mb": result["file_size_mb"],
                    "input_shape": result["input_shape"],
                    "output_shape": result["output_shape"]
                })
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        model_info["models"].sort(key=lambda x: x["priority"])
        
        # ä¿å­˜æ¨¡å‹ä¿¡æ¯
        info_path = self.output_dir / "model_info.json"
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(model_info, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜: {info_path}")
        return info_path
    
    def convert_all_models(self):
        """è½¬æ¢æ‰€æœ‰æ¨¡å‹"""
        logger.info("ğŸš€ å¼€å§‹æ‰¹é‡è½¬æ¢æ¨¡å‹ä¸ºONNXæ ¼å¼...")
        
        conversion_results = []
        successful_conversions = 0
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºè½¬æ¢
        sorted_models = sorted(
            self.model_configs.items(),
            key=lambda x: x[1]["priority"]
        )
        
        for model_name, config in sorted_models:
            logger.info(f"\nğŸ“¦ æ­£åœ¨å¤„ç†æ¨¡å‹: {model_name}")
            logger.info(f"   æè¿°: {config['description']}")
            
            # åŠ è½½æ¨¡å‹
            model = self.load_model(model_name)
            if model is None:
                conversion_results.append({
                    "model_name": model_name,
                    "success": False,
                    "error": "Failed to load model"
                })
                continue
            
            # è·å–ä¸“ç”¨è½¬æ¢å‡½æ•°
            converter_function = config.get('converter_function', self.convert_to_onnx)
            
            # è½¬æ¢ä¸ºONNX
            result = converter_function(model_name, model)
            conversion_results.append(result)
            
            # æµ‹è¯•ONNXæ¨¡å‹
            if result["success"]:
                if self.test_onnx_model(result["onnx_path"]):
                    successful_conversions += 1
                else:
                    result["success"] = False
                    result["error"] = "ONNX model test failed"
        
        # ç”Ÿæˆæ¨¡å‹ä¿¡æ¯æ–‡ä»¶
        info_path = self.generate_model_info(conversion_results)
        
        # è¾“å‡ºè½¬æ¢æ€»ç»“
        logger.info(f"\nğŸ‰ ONNXè½¬æ¢å®Œæˆ!")
        logger.info(f"âœ… æˆåŠŸè½¬æ¢: {successful_conversions}/{len(self.model_configs)} ä¸ªæ¨¡å‹")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        logger.info(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯: {info_path}")
        
        return conversion_results

def main():
    """ä¸»å‡½æ•°"""
    try:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        parser = argparse.ArgumentParser(description='å°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼')
        parser.add_argument('--model', type=str, help='è¦è½¬æ¢çš„ç‰¹å®šæ¨¡å‹åç§°ï¼Œä¸æŒ‡å®šåˆ™è½¬æ¢æ‰€æœ‰æ¨¡å‹')
        parser.add_argument('--list', action='store_true', help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹')
        parser.add_argument('--output-dir', type=str, default='deployment/onnx_models', help='ONNXæ¨¡å‹è¾“å‡ºç›®å½•')
        parser.add_argument('--opset', type=int, default=12, help='ONNX opsetç‰ˆæœ¬')
        args = parser.parse_args()
        
        # æ£€æŸ¥ä¾èµ–
        try:
            import onnx
            import onnxruntime
        except ImportError:
            logger.error("âŒ ç¼ºå°‘ä¾èµ–åŒ…ï¼Œè¯·å®‰è£…: pip install onnx onnxruntime")
            return
        
        # åˆ›å»ºè½¬æ¢å™¨
        converter = ONNXConverter()
        
        # å¦‚æœæŒ‡å®šäº†--listå‚æ•°ï¼Œåˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
        if args.list:
            logger.info("\nğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
            for model_name, config in sorted(converter.model_configs.items(), key=lambda x: x[1]["priority"]):
                logger.info(f"   {model_name}: {config['description']}")
            return
        
        # å¦‚æœæŒ‡å®šäº†ç‰¹å®šæ¨¡å‹ï¼Œåªè½¬æ¢è¯¥æ¨¡å‹
        if args.model:
            if args.model not in converter.model_configs:
                logger.error(f"âŒ æœªçŸ¥æ¨¡å‹: {args.model}")
                logger.info(f"å¯ç”¨æ¨¡å‹: {', '.join(converter.model_configs.keys())}")
                return
            
            logger.info(f"ğŸš€ å¼€å§‹è½¬æ¢å•ä¸ªæ¨¡å‹: {args.model}")
            success = converter.convert_single_model(args.model)
            
            if success:
                logger.info(f"\nğŸ‰ æ¨¡å‹ {args.model} è½¬æ¢æˆåŠŸ!")
                logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {converter.output_dir}")
            else:
                logger.error(f"\nâŒ æ¨¡å‹ {args.model} è½¬æ¢å¤±è´¥!")
            
            return
        
        # å¦åˆ™è½¬æ¢æ‰€æœ‰æ¨¡å‹
        results = converter.convert_all_models()
        
        # è¾“å‡ºæˆåŠŸçš„æ¨¡å‹åˆ—è¡¨
        successful_models = [r for r in results if r["success"]]
        if successful_models:
            logger.info("\nğŸ† æˆåŠŸè½¬æ¢çš„æ¨¡å‹:")
            for model in successful_models:
                logger.info(f"   âœ… {model['model_name']} ({model['file_size_mb']} MB)")
        
        # è¾“å‡ºå¤±è´¥çš„æ¨¡å‹åˆ—è¡¨
        failed_models = [r for r in results if not r["success"]]
        if failed_models:
            logger.info("\nâŒ è½¬æ¢å¤±è´¥çš„æ¨¡å‹:")
            for model in failed_models:
                logger.info(f"   âŒ {model['model_name']}: {model.get('error', 'Unknown error')}")
        
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    main()