#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†ç®¡ç†å·¥å…·
æ”¯æŒè¯¯åˆ¤è°ƒæ•´ã€å¢é‡æ•°æ®æ·»åŠ ã€å»é‡ã€é‡æ–°å¹³è¡¡ç­‰åŠŸèƒ½
æ”¯æŒdataset_builder.pyçš„æºæ•°æ®ç»“æ„ï¼ˆcfgæ–‡ä»¶ + å­”ä½å›¾ç‰‡ï¼‰
"""

import os
import shutil
import hashlib
import yaml
import argparse
import json
from pathlib import Path
from collections import defaultdict, Counter
import random
from PIL import Image
import numpy as np
from datetime import datetime

class DatasetManager:
    def __init__(self, dataset_path="bioast_dataset"):
        self.dataset_path = Path(dataset_path)
        self.splits = ['train', 'test', 'val']
        self.classes = ['positive', 'negative']
        
        # å›¾ç‰‡å“ˆå¸Œç¼“å­˜
        self.image_hashes = {}
        self.duplicate_groups = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = self._get_current_stats()
    
    def _get_current_stats(self):
        """è·å–å½“å‰æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = defaultdict(dict)
        total_count = 0
        
        for class_name in self.classes:
            class_total = 0
            for split in self.splits:
                split_path = self.dataset_path / class_name / split
                if split_path.exists():
                    count = len(list(split_path.glob("*.png")))
                    stats[class_name][split] = count
                    class_total += count
                    total_count += count
                else:
                    stats[class_name][split] = 0
            stats[class_name]['total'] = class_total
        
        stats['total'] = total_count
        return dict(stats)
    
    def print_stats(self):
        """æ‰“å°å½“å‰ç»Ÿè®¡ä¿¡æ¯"""
        print("ğŸ“Š å½“å‰æ•°æ®é›†ç»Ÿè®¡:")
        print("=" * 50)
        
        for class_name in self.classes:
            print(f"\n{class_name.upper()}:")
            for split in self.splits:
                count = self.stats[class_name][split]
                print(f"  {split}: {count}")
            print(f"  æ€»è®¡: {self.stats[class_name]['total']}")
        
        total = self.stats['total']
        pos_ratio = self.stats['positive']['total'] / total * 100 if total > 0 else 0
        print(f"\næ€»è®¡: {total}")
        print(f"å¹³è¡¡æ¯”ä¾‹: {pos_ratio:.1f}% positive, {100-pos_ratio:.1f}% negative")
    
    def calculate_image_hash(self, image_path):
        """è®¡ç®—å›¾ç‰‡çš„æ„ŸçŸ¥å“ˆå¸Œ"""
        try:
            with Image.open(image_path) as img:
                # è½¬æ¢ä¸ºç°åº¦å¹¶è°ƒæ•´å¤§å°
                img = img.convert('L').resize((8, 8))
                # è®¡ç®—åƒç´ å¹³å‡å€¼
                pixels = np.array(img)
                avg = pixels.mean()
                # ç”Ÿæˆå“ˆå¸Œ
                hash_bits = pixels > avg
                hash_str = ''.join(['1' if b else '0' for b in hash_bits.flatten()])
                return hash_str
        except Exception:
            return None
    
    def find_duplicates(self):
        """æŸ¥æ‰¾é‡å¤å›¾ç‰‡"""
        print("ğŸ” æ‰«æé‡å¤å›¾ç‰‡...")
        self.image_hashes.clear()
        hash_to_files = defaultdict(list)
        
        # æ‰«ææ‰€æœ‰å›¾ç‰‡
        for class_name in self.classes:
            for split in self.splits:
                split_path = self.dataset_path / class_name / split
                if split_path.exists():
                    for img_file in split_path.glob("*.png"):
                        img_hash = self.calculate_image_hash(img_file)
                        if img_hash:
                            self.image_hashes[str(img_file)] = img_hash
                            hash_to_files[img_hash].append(img_file)
        
        # æ‰¾å‡ºé‡å¤ç»„
        self.duplicate_groups = [files for files in hash_to_files.values() if len(files) > 1]
        
        if self.duplicate_groups:
            print(f"â— å‘ç° {len(self.duplicate_groups)} ç»„é‡å¤å›¾ç‰‡:")
            for i, group in enumerate(self.duplicate_groups, 1):
                print(f"  ç»„ {i}: {len(group)} å¼ å›¾ç‰‡")
                for file_path in group:
                    print(f"    {file_path}")
        else:
            print("âœ… æœªå‘ç°é‡å¤å›¾ç‰‡")
        
        return self.duplicate_groups
    
    def remove_duplicates(self, keep_strategy='first'):
        """ç§»é™¤é‡å¤å›¾ç‰‡"""
        if not self.duplicate_groups:
            self.find_duplicates()
        
        if not self.duplicate_groups:
            print("âœ… æ²¡æœ‰é‡å¤å›¾ç‰‡éœ€è¦å¤„ç†")
            return
        
        removed_count = 0
        for group in self.duplicate_groups:
            if keep_strategy == 'first':
                keep_file = group[0]
                remove_files = group[1:]
            elif keep_strategy == 'train_priority':
                # ä¼˜å…ˆä¿ç•™è®­ç»ƒé›†ä¸­çš„
                train_files = [f for f in group if '/train/' in str(f)]
                if train_files:
                    keep_file = train_files[0]
                    remove_files = [f for f in group if f != keep_file]
                else:
                    keep_file = group[0]
                    remove_files = group[1:]
            else:
                keep_file = group[0]
                remove_files = group[1:]
            
            print(f"ä¿ç•™: {keep_file}")
            for remove_file in remove_files:
                print(f"åˆ é™¤: {remove_file}")
                os.remove(remove_file)
                removed_count += 1
        
        print(f"âœ… åˆ é™¤äº† {removed_count} å¼ é‡å¤å›¾ç‰‡")
        self.stats = self._get_current_stats()
    
    def move_misclassified_samples(self, misclassified_list):
        """ç§»åŠ¨è¯¯åˆ¤æ ·æœ¬
        
        Args:
            misclassified_list: æ ¼å¼ [(src_path, target_class), ...]
        """
        print("ğŸ”„ ç§»åŠ¨è¯¯åˆ¤æ ·æœ¬...")
        moved_count = 0
        
        for src_path, target_class in misclassified_list:
            src_path = Path(src_path)
            if not src_path.exists():
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {src_path}")
                continue
            
            # ç¡®å®šç›®æ ‡è·¯å¾„ - ç§»åŠ¨åˆ°trainç›®å½•ä»¥å¢åŠ è®­ç»ƒæ•°æ®
            target_dir = self.dataset_path / target_class / 'train'
            target_dir.mkdir(parents=True, exist_ok=True)
            target_path = target_dir / src_path.name
            
            # é¿å…è¦†ç›–
            counter = 1
            while target_path.exists():
                stem = src_path.stem
                suffix = src_path.suffix
                target_path = target_dir / f"{stem}_{counter}{suffix}"
                counter += 1
            
            # ç§»åŠ¨æ–‡ä»¶
            shutil.move(str(src_path), str(target_path))
            print(f"âœ… {src_path} â†’ {target_path}")
            moved_count += 1
        
        print(f"âœ… ç§»åŠ¨äº† {moved_count} ä¸ªè¯¯åˆ¤æ ·æœ¬")
        self.stats = self._get_current_stats()
    
    def add_incremental_data(self, source_dirs, auto_split=True, split_ratio=(0.7, 0.2, 0.1)):
        """æ·»åŠ å¢é‡æ•°æ®
        
        Args:
            source_dirs: æºæ•°æ®ç›®å½•åˆ—è¡¨ [(path, class_name), ...]
            auto_split: æ˜¯å¦è‡ªåŠ¨åˆ†å‰²æ•°æ®åˆ°train/test/val
            split_ratio: åˆ†å‰²æ¯”ä¾‹ (train, test, val)
        """
        print("ğŸ“¥ æ·»åŠ å¢é‡æ•°æ®...")
        
        new_files = []
        duplicate_count = 0
        
        for source_dir, class_name in source_dirs:
            source_path = Path(source_dir)
            if not source_path.exists():
                print(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {source_path}")
                continue
            
            print(f"å¤„ç† {class_name} ç±»åˆ«ï¼Œæºç›®å½•: {source_path}")
            
            for img_file in source_path.glob("*.png"):
                # è®¡ç®—æ–°å›¾ç‰‡å“ˆå¸Œ
                new_hash = self.calculate_image_hash(img_file)
                if not new_hash:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                is_duplicate = False
                for existing_path, existing_hash in self.image_hashes.items():
                    if new_hash == existing_hash:
                        print(f"âš ï¸  è·³è¿‡é‡å¤å›¾ç‰‡: {img_file.name} (ä¸ {existing_path} é‡å¤)")
                        duplicate_count += 1
                        is_duplicate = True
                        break
                
                if not is_duplicate:
                    new_files.append((img_file, class_name, new_hash))
        
        if not new_files:
            print("âŒ æ²¡æœ‰æ–°çš„æœ‰æ•ˆå›¾ç‰‡å¯ä»¥æ·»åŠ ")
            return
        
        print(f"ğŸ“‹ å‡†å¤‡æ·»åŠ  {len(new_files)} å¼ æ–°å›¾ç‰‡")
        print(f"ğŸ“‹ è·³è¿‡äº† {duplicate_count} å¼ é‡å¤å›¾ç‰‡")
        
        if auto_split:
            # è‡ªåŠ¨åˆ†å‰²
            random.shuffle(new_files)
            total = len(new_files)
            train_end = int(total * split_ratio[0])
            test_end = train_end + int(total * split_ratio[1])
            
            splits_files = {
                'train': new_files[:train_end],
                'test': new_files[train_end:test_end],
                'val': new_files[test_end:]
            }
        else:
            # å…¨éƒ¨åŠ åˆ°trainï¼ˆç”¨äºè¯¯åˆ¤è°ƒæ•´åçš„å¢å¼ºï¼‰
            splits_files = {'train': new_files, 'test': [], 'val': []}
        
        # å¤åˆ¶æ–‡ä»¶
        added_count = 0
        for split, files in splits_files.items():
            for img_file, class_name, img_hash in files:
                target_dir = self.dataset_path / class_name / split
                target_dir.mkdir(parents=True, exist_ok=True)
                
                # é¿å…æ–‡ä»¶åå†²çª
                target_path = target_dir / img_file.name
                counter = 1
                while target_path.exists():
                    stem = img_file.stem
                    suffix = img_file.suffix
                    target_path = target_dir / f"{stem}_inc{counter}{suffix}"
                    counter += 1
                
                shutil.copy2(img_file, target_path)
                self.image_hashes[str(target_path)] = img_hash
                added_count += 1
                print(f"âœ… æ·»åŠ åˆ° {class_name}/{split}: {target_path.name}")
        
        print(f"âœ… æˆåŠŸæ·»åŠ  {added_count} å¼ æ–°å›¾ç‰‡")
        self.stats = self._get_current_stats()
    
    def rebalance_dataset(self, target_ratio=0.5, method='oversample'):
        """é‡æ–°å¹³è¡¡æ•°æ®é›†
        
        Args:
            target_ratio: ç›®æ ‡æ­£æ ·æœ¬æ¯”ä¾‹
            method: 'oversample', 'undersample', 'mixed'
        """
        print("âš–ï¸ é‡æ–°å¹³è¡¡æ•°æ®é›†...")
        
        pos_count = self.stats['positive']['total']
        neg_count = self.stats['negative']['total']
        total = pos_count + neg_count
        current_ratio = pos_count / total if total > 0 else 0
        
        print(f"å½“å‰æ¯”ä¾‹: {current_ratio:.3f} ({pos_count}pos / {neg_count}neg)")
        print(f"ç›®æ ‡æ¯”ä¾‹: {target_ratio:.3f}")
        
        if abs(current_ratio - target_ratio) < 0.01:
            print("âœ… æ•°æ®é›†å·²ç»å¹³è¡¡")
            return
        
        if method == 'oversample':
            self._oversample_minority_class(target_ratio)
        elif method == 'undersample':
            self._undersample_majority_class(target_ratio)
        else:
            print("âŒ ä¸æ”¯æŒçš„å¹³è¡¡æ–¹æ³•")
    
    def _oversample_minority_class(self, target_ratio):
        """é€šè¿‡è¿‡é‡‡æ ·å¹³è¡¡æ•°æ®é›†"""
        pos_count = self.stats['positive']['total']
        neg_count = self.stats['negative']['total']
        total = pos_count + neg_count
        
        if pos_count / total < target_ratio:
            # éœ€è¦å¢åŠ æ­£æ ·æœ¬
            minority_class = 'positive'
            majority_count = neg_count
        else:
            # éœ€è¦å¢åŠ è´Ÿæ ·æœ¬
            minority_class = 'negative'
            majority_count = pos_count
        
        # è®¡ç®—éœ€è¦çš„æ ·æœ¬æ•°
        target_minority_count = int(majority_count * target_ratio / (1 - target_ratio))
        current_minority_count = self.stats[minority_class]['total']
        need_count = target_minority_count - current_minority_count
        
        if need_count <= 0:
            print("âœ… æ— éœ€è¿‡é‡‡æ ·")
            return
        
        print(f"éœ€è¦ä¸º {minority_class} ç±»åˆ«å¢åŠ  {need_count} ä¸ªæ ·æœ¬")
        
        # æ”¶é›†å¯ç”¨äºå¤åˆ¶çš„å›¾ç‰‡ï¼ˆä¸»è¦ä»trainé›†ï¼‰
        source_files = []
        for split in ['train', 'val', 'test']:  # ä¼˜å…ˆtrain
            split_path = self.dataset_path / minority_class / split
            if split_path.exists():
                source_files.extend(list(split_path.glob("*.png")))
        
        if not source_files:
            print("âŒ æ²¡æœ‰å¯ç”¨äºè¿‡é‡‡æ ·çš„æºæ–‡ä»¶")
            return
        
        # å¤åˆ¶æ–‡ä»¶åˆ°trainç›®å½•
        target_dir = self.dataset_path / minority_class / 'train'
        target_dir.mkdir(parents=True, exist_ok=True)
        
        for i in range(need_count):
            source_file = random.choice(source_files)
            
            # ç”Ÿæˆæ–°æ–‡ä»¶å
            counter = 1
            target_path = target_dir / f"{source_file.stem}_aug{counter}.png"
            while target_path.exists():
                counter += 1
                target_path = target_dir / f"{source_file.stem}_aug{counter}.png"
            
            shutil.copy2(source_file, target_path)
            if i < 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"âœ… å¤åˆ¶: {source_file.name} â†’ {target_path.name}")
        
        print(f"âœ… è¿‡é‡‡æ ·å®Œæˆï¼Œå¢åŠ äº† {need_count} ä¸ªæ ·æœ¬")
        self.stats = self._get_current_stats()
    
    def save_dataset_info(self):
        """ä¿å­˜æ•°æ®é›†ä¿¡æ¯(JSONæ ¼å¼)"""
        report_path = self.dataset_path / "dataset_stats.json"
        
        # å‡†å¤‡æŠ¥å‘Šæ•°æ®
        report_data = {
            "metadata": {
                "source_directory": "å¢é‡æ›´æ–°",
                "output_directory": str(self.dataset_path),
                "sample_count": "N/A",
                "generation_time": datetime.now().isoformat(),
                "tool_version": "dataset_manager.py"
            },
            "summary": {
                "positive_total": self.stats.get('positive', {}).get('total', 0),
                "negative_total": self.stats.get('negative', {}).get('total', 0),
                "total_samples": self.stats.get('total', 0)
            },
            "dataset_splits": {}
        }
        
        # æ·»åŠ å„æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        for subset in self.splits:
            pos_count = self.stats.get('positive', {}).get(subset, 0)
            neg_count = self.stats.get('negative', {}).get(subset, 0)
            total_count = pos_count + neg_count
            
            if total_count > 0:
                pos_ratio = pos_count / total_count
                neg_ratio = neg_count / total_count
                
                report_data["dataset_splits"][subset] = {
                    "positive": {
                        "count": pos_count,
                        "ratio": round(pos_ratio, 4)
                    },
                    "negative": {
                        "count": neg_count,
                        "ratio": round(neg_ratio, 4)
                    },
                    "total": total_count
                }
        
        # å†™å…¥JSONæ–‡ä»¶
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {report_path}")

def main():
    parser = argparse.ArgumentParser(description='æ•°æ®é›†ç®¡ç†å·¥å…·')
    parser.add_argument('--dataset', type=str, default='bioast_dataset', help='æ•°æ®é›†è·¯å¾„')
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # ç»Ÿè®¡å‘½ä»¤
    subparsers.add_parser('stats', help='æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯')
    
    # æŸ¥æ‰¾é‡å¤
    subparsers.add_parser('find-duplicates', help='æŸ¥æ‰¾é‡å¤å›¾ç‰‡')
    
    # åˆ é™¤é‡å¤
    dup_parser = subparsers.add_parser('remove-duplicates', help='åˆ é™¤é‡å¤å›¾ç‰‡')
    dup_parser.add_argument('--strategy', choices=['first', 'train_priority'], default='train_priority', 
                           help='ä¿ç•™ç­–ç•¥')
    
    # ç§»åŠ¨è¯¯åˆ¤æ ·æœ¬
    move_parser = subparsers.add_parser('move-misclassified', help='ç§»åŠ¨è¯¯åˆ¤æ ·æœ¬')
    move_parser.add_argument('--file', type=str, required=True, help='è¯¯åˆ¤æ–‡ä»¶è·¯å¾„')
    move_parser.add_argument('--target-class', choices=['positive', 'negative'], required=True, 
                            help='ç›®æ ‡ç±»åˆ«')
    
    # æ·»åŠ å¢é‡æ•°æ®
    add_parser = subparsers.add_parser('add-data', help='æ·»åŠ å¢é‡æ•°æ®')
    add_parser.add_argument('--source', type=str, required=True, help='æºæ•°æ®ç›®å½•')
    add_parser.add_argument('--class', type=str, choices=['positive', 'negative'], 
                           required=True, dest='class_name', help='æ•°æ®ç±»åˆ«')
    add_parser.add_argument('--no-split', action='store_true', help='ä¸è‡ªåŠ¨åˆ†å‰²ï¼Œå…¨éƒ¨åŠ åˆ°train')
    
    # é‡æ–°å¹³è¡¡
    balance_parser = subparsers.add_parser('rebalance', help='é‡æ–°å¹³è¡¡æ•°æ®é›†')
    balance_parser.add_argument('--ratio', type=float, default=0.5, help='ç›®æ ‡æ­£æ ·æœ¬æ¯”ä¾‹')
    balance_parser.add_argument('--method', choices=['oversample', 'undersample'], 
                               default='oversample', help='å¹³è¡¡æ–¹æ³•')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    manager = DatasetManager(args.dataset)
    
    if args.command == 'stats':
        manager.print_stats()
        
    elif args.command == 'find-duplicates':
        manager.find_duplicates()
        
    elif args.command == 'remove-duplicates':
        manager.remove_duplicates(args.strategy)
        
    elif args.command == 'move-misclassified':
        misclassified_list = [(args.file, args.target_class)]
        manager.move_misclassified_samples(misclassified_list)
        manager.save_dataset_info()
        
    elif args.command == 'add-data':
        source_dirs = [(args.source, args.class_name)]
        manager.add_incremental_data(source_dirs, auto_split=not args.no_split)
        manager.save_dataset_info()
        
    elif args.command == 'rebalance':
        manager.rebalance_dataset(args.ratio, args.method)
        manager.save_dataset_info()

if __name__ == "__main__":
    main()