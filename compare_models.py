#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹å¯¹æ¯”åˆ†æè„šæœ¬

è¿™ä¸ªè„šæœ¬ç”¨äºå¯¹æ¯”åˆ†æå·²è®­ç»ƒçš„æ¨¡å‹ï¼Œç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨ã€‚

ä½¿ç”¨æ–¹æ³•:
    python compare_models.py                           # å¯¹æ¯”æ‰€æœ‰å·²è®­ç»ƒæ¨¡å‹
    python compare_models.py --models model1 model2   # å¯¹æ¯”æŒ‡å®šæ¨¡å‹
    python compare_models.py --top 5                  # å¯¹æ¯”æ€§èƒ½æœ€å¥½çš„5ä¸ªæ¨¡å‹
    python compare_models.py --generate-report        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
"""

import os
import sys
import json
import argparse
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import numpy as np

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

class ModelComparator:
    """
    æ¨¡å‹å¯¹æ¯”åˆ†æå™¨
    """
    
    def __init__(self, output_dir: str = "reports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.checkpoints_dir = Path("checkpoints")
        self.models_data = []
        
        print(f"ğŸ” æ¨¡å‹å¯¹æ¯”åˆ†æå™¨åˆå§‹åŒ–")
        print(f"ğŸ“ æ£€æŸ¥ç‚¹ç›®å½•: {self.checkpoints_dir}")
        print(f"ğŸ“Š è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def scan_trained_models(self) -> List[Dict[str, Any]]:
        """
        æ‰«æå·²è®­ç»ƒçš„æ¨¡å‹
        
        Returns:
            List[Dict]: æ¨¡å‹ä¿¡æ¯åˆ—è¡¨
        """
        models = []
        
        if not self.checkpoints_dir.exists():
            print(f"âŒ æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨: {self.checkpoints_dir}")
            return models
        
        print(f"\nğŸ” æ‰«æå·²è®­ç»ƒæ¨¡å‹...")
        
        for model_dir in self.checkpoints_dir.iterdir():
            if not model_dir.is_dir():
                continue
            
            model_name = model_dir.name
            best_checkpoint = model_dir / "best.pth"
            history_file = model_dir / "training_history.json"
            
            if not best_checkpoint.exists():
                print(f"âš ï¸ {model_name}: ç¼ºå°‘æœ€ä½³æ£€æŸ¥ç‚¹")
                continue
            
            model_info = {
                'name': model_name,
                'checkpoint_path': str(best_checkpoint),
                'model_dir': str(model_dir)
            }
            
            # è¯»å–è®­ç»ƒå†å²
            if history_file.exists():
                try:
                    with open(history_file, 'r', encoding='utf-8') as f:
                        history = json.load(f)
                    
                    model_info.update({
                        'best_val_acc': history.get('best_val_acc', 0),
                        'best_epoch': history.get('best_epoch', 0),
                        'train_losses': history.get('train_losses', []),
                        'train_accuracies': history.get('train_accuracies', []),
                        'val_losses': history.get('val_losses', []),
                        'val_accuracies': history.get('val_accuracies', []),
                        'total_epochs': len(history.get('train_losses', []))
                    })
                    
                    print(f"âœ… {model_name}: æœ€ä½³éªŒè¯å‡†ç¡®ç‡ {history.get('best_val_acc', 0):.2f}%")
                    
                except Exception as e:
                    print(f"âš ï¸ {model_name}: è¯»å–è®­ç»ƒå†å²å¤±è´¥ - {e}")
                    model_info.update({
                        'best_val_acc': 0,
                        'best_epoch': 0,
                        'total_epochs': 0
                    })
            else:
                print(f"âš ï¸ {model_name}: ç¼ºå°‘è®­ç»ƒå†å²æ–‡ä»¶")
                model_info.update({
                    'best_val_acc': 0,
                    'best_epoch': 0,
                    'total_epochs': 0
                })
            
            models.append(model_info)
        
        print(f"\nğŸ“Š æ‰¾åˆ° {len(models)} ä¸ªå·²è®­ç»ƒæ¨¡å‹")
        return models
    
    def filter_models(self, models: List[Dict], model_names: Optional[List[str]] = None, 
                     top_k: Optional[int] = None) -> List[Dict]:
        """
        è¿‡æ»¤æ¨¡å‹
        
        Args:
            models: æ¨¡å‹åˆ—è¡¨
            model_names: æŒ‡å®šçš„æ¨¡å‹åç§°åˆ—è¡¨
            top_k: é€‰æ‹©æ€§èƒ½æœ€å¥½çš„kä¸ªæ¨¡å‹
        
        Returns:
            List[Dict]: è¿‡æ»¤åçš„æ¨¡å‹åˆ—è¡¨
        """
        if model_names:
            # æŒ‰æŒ‡å®šåç§°è¿‡æ»¤
            filtered = [m for m in models if m['name'] in model_names]
            missing = set(model_names) - {m['name'] for m in filtered}
            if missing:
                print(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹: {', '.join(missing)}")
            return filtered
        
        if top_k:
            # æŒ‰æ€§èƒ½æ’åºå¹¶é€‰æ‹©å‰kä¸ª
            sorted_models = sorted(models, key=lambda x: x.get('best_val_acc', 0), reverse=True)
            return sorted_models[:top_k]
        
        return models
    
    def create_comparison_table(self, models: List[Dict]) -> pd.DataFrame:
        """
        åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        
        Args:
            models: æ¨¡å‹åˆ—è¡¨
        
        Returns:
            pd.DataFrame: å¯¹æ¯”è¡¨æ ¼
        """
        data = []
        
        for model in models:
            row = {
                'æ¨¡å‹åç§°': model['name'],
                'æœ€ä½³éªŒè¯å‡†ç¡®ç‡(%)': f"{model.get('best_val_acc', 0):.2f}",
                'æœ€ä½³è½®æ¬¡': model.get('best_epoch', 0),
                'æ€»è®­ç»ƒè½®æ¬¡': model.get('total_epochs', 0),
                'æœ€ç»ˆè®­ç»ƒæŸå¤±': f"{model.get('train_losses', [0])[-1]:.4f}" if model.get('train_losses') else 'N/A',
                'æœ€ç»ˆéªŒè¯æŸå¤±': f"{model.get('val_losses', [0])[-1]:.4f}" if model.get('val_losses') else 'N/A'
            }
            data.append(row)
        
        df = pd.DataFrame(data)
        return df
    
    def plot_training_curves(self, models: List[Dict], save_path: Optional[str] = None):
        """
        ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¯¹æ¯”å›¾
        
        Args:
            models: æ¨¡å‹åˆ—è¡¨
            save_path: ä¿å­˜è·¯å¾„
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('æ¨¡å‹è®­ç»ƒæ›²çº¿å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        # é¢œè‰²æ˜ å°„
        colors = plt.cm.tab10(np.linspace(0, 1, len(models)))
        
        for i, model in enumerate(models):
            model_name = model['name']
            color = colors[i]
            
            train_losses = model.get('train_losses', [])
            val_losses = model.get('val_losses', [])
            train_accs = model.get('train_accuracies', [])
            val_accs = model.get('val_accuracies', [])
            
            if not train_losses:
                continue
            
            epochs = range(1, len(train_losses) + 1)
            
            # è®­ç»ƒæŸå¤±
            axes[0, 0].plot(epochs, train_losses, label=model_name, color=color, linewidth=2)
            
            # éªŒè¯æŸå¤±
            if val_losses:
                axes[0, 1].plot(epochs, val_losses, label=model_name, color=color, linewidth=2)
            
            # è®­ç»ƒå‡†ç¡®ç‡
            if train_accs:
                axes[1, 0].plot(epochs, train_accs, label=model_name, color=color, linewidth=2)
            
            # éªŒè¯å‡†ç¡®ç‡
            if val_accs:
                axes[1, 1].plot(epochs, val_accs, label=model_name, color=color, linewidth=2)
        
        # è®¾ç½®å­å›¾
        axes[0, 0].set_title('è®­ç»ƒæŸå¤±')
        axes[0, 0].set_xlabel('è½®æ¬¡')
        axes[0, 0].set_ylabel('æŸå¤±')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        axes[0, 1].set_title('éªŒè¯æŸå¤±')
        axes[0, 1].set_xlabel('è½®æ¬¡')
        axes[0, 1].set_ylabel('æŸå¤±')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        axes[1, 0].set_title('è®­ç»ƒå‡†ç¡®ç‡')
        axes[1, 0].set_xlabel('è½®æ¬¡')
        axes[1, 0].set_ylabel('å‡†ç¡®ç‡ (%)')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        axes[1, 1].set_title('éªŒè¯å‡†ç¡®ç‡')
        axes[1, 1].set_xlabel('è½®æ¬¡')
        axes[1, 1].set_ylabel('å‡†ç¡®ç‡ (%)')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def plot_performance_comparison(self, models: List[Dict], save_path: Optional[str] = None):
        """
        ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾
        
        Args:
            models: æ¨¡å‹åˆ—è¡¨
            save_path: ä¿å­˜è·¯å¾„
        """
        if not models:
            print("âŒ æ²¡æœ‰æ¨¡å‹æ•°æ®å¯ä¾›å¯¹æ¯”")
            return
        
        # å‡†å¤‡æ•°æ®
        model_names = [model['name'] for model in models]
        accuracies = [model.get('best_val_acc', 0) for model in models]
        
        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('æ¨¡å‹æ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        # æŸ±çŠ¶å›¾
        bars = ax1.bar(model_names, accuracies, color=plt.cm.viridis(np.linspace(0, 1, len(models))))
        ax1.set_title('æœ€ä½³éªŒè¯å‡†ç¡®ç‡å¯¹æ¯”')
        ax1.set_ylabel('å‡†ç¡®ç‡ (%)')
        ax1.set_ylim(0, 100)
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, acc in zip(bars, accuracies):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')
        
        # æ—‹è½¬xè½´æ ‡ç­¾
        ax1.tick_params(axis='x', rotation=45)
        
        # é¥¼å›¾ - æ˜¾ç¤ºç›¸å¯¹æ€§èƒ½
        if len(models) > 1:
            # è®¡ç®—ç›¸å¯¹æ€§èƒ½ï¼ˆå½’ä¸€åŒ–ï¼‰
            total_acc = sum(accuracies)
            if total_acc > 0:
                relative_performance = [acc/total_acc * 100 for acc in accuracies]
                
                wedges, texts, autotexts = ax2.pie(relative_performance, labels=model_names, 
                                                   autopct='%1.1f%%', startangle=90)
                ax2.set_title('ç›¸å¯¹æ€§èƒ½åˆ†å¸ƒ')
                
                # ç¾åŒ–é¥¼å›¾
                for autotext in autotexts:
                    autotext.set_color('white')
                    autotext.set_fontweight('bold')
        else:
            ax2.text(0.5, 0.5, 'éœ€è¦è‡³å°‘2ä¸ªæ¨¡å‹\næ‰èƒ½æ˜¾ç¤ºç›¸å¯¹æ€§èƒ½', 
                    ha='center', va='center', transform=ax2.transAxes, fontsize=12)
            ax2.set_title('ç›¸å¯¹æ€§èƒ½åˆ†å¸ƒ')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def generate_detailed_report(self, models: List[Dict], save_path: Optional[str] = None) -> str:
        """
        ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Š
        
        Args:
            models: æ¨¡å‹åˆ—è¡¨
            save_path: ä¿å­˜è·¯å¾„
        
        Returns:
            str: æŠ¥å‘Šå†…å®¹
        """
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = f"""
# BioAstæ¨¡å‹å¯¹æ¯”åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {timestamp}
**å¯¹æ¯”æ¨¡å‹æ•°é‡**: {len(models)}

## ğŸ“Š æ¨¡å‹æ€§èƒ½æ¦‚è§ˆ

"""
        
        if not models:
            report += "âŒ æ²¡æœ‰æ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹ã€‚\n"
            return report
        
        # æ’åºæ¨¡å‹ï¼ˆæŒ‰æ€§èƒ½ï¼‰
        sorted_models = sorted(models, key=lambda x: x.get('best_val_acc', 0), reverse=True)
        
        # æ€§èƒ½æ’è¡Œæ¦œ
        report += "### ğŸ† æ€§èƒ½æ’è¡Œæ¦œ\n\n"
        report += "| æ’å | æ¨¡å‹åç§° | æœ€ä½³éªŒè¯å‡†ç¡®ç‡ | æœ€ä½³è½®æ¬¡ | æ€»è®­ç»ƒè½®æ¬¡ |\n"
        report += "|------|----------|----------------|----------|------------|\n"
        
        for i, model in enumerate(sorted_models, 1):
            name = model['name']
            acc = model.get('best_val_acc', 0)
            best_epoch = model.get('best_epoch', 0)
            total_epochs = model.get('total_epochs', 0)
            
            medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}"
            report += f"| {medal} | {name} | {acc:.2f}% | {best_epoch} | {total_epochs} |\n"
        
        # ç»Ÿè®¡ä¿¡æ¯
        accuracies = [m.get('best_val_acc', 0) for m in models]
        if accuracies:
            report += f"\n### ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯\n\n"
            report += f"- **æœ€é«˜å‡†ç¡®ç‡**: {max(accuracies):.2f}%\n"
            report += f"- **æœ€ä½å‡†ç¡®ç‡**: {min(accuracies):.2f}%\n"
            report += f"- **å¹³å‡å‡†ç¡®ç‡**: {np.mean(accuracies):.2f}%\n"
            report += f"- **å‡†ç¡®ç‡æ ‡å‡†å·®**: {np.std(accuracies):.2f}%\n"
        
        # è¯¦ç»†åˆ†æ
        report += "\n### ğŸ” è¯¦ç»†åˆ†æ\n\n"
        
        for model in sorted_models:
            name = model['name']
            acc = model.get('best_val_acc', 0)
            best_epoch = model.get('best_epoch', 0)
            total_epochs = model.get('total_epochs', 0)
            
            train_losses = model.get('train_losses', [])
            val_losses = model.get('val_losses', [])
            
            report += f"#### {name}\n\n"
            report += f"- **æœ€ä½³éªŒè¯å‡†ç¡®ç‡**: {acc:.2f}%\n"
            report += f"- **è¾¾åˆ°æœ€ä½³æ€§èƒ½çš„è½®æ¬¡**: {best_epoch}\n"
            report += f"- **æ€»è®­ç»ƒè½®æ¬¡**: {total_epochs}\n"
            
            if train_losses:
                final_train_loss = train_losses[-1]
                report += f"- **æœ€ç»ˆè®­ç»ƒæŸå¤±**: {final_train_loss:.4f}\n"
            
            if val_losses:
                final_val_loss = val_losses[-1]
                report += f"- **æœ€ç»ˆéªŒè¯æŸå¤±**: {final_val_loss:.4f}\n"
            
            # è®­ç»ƒæ•ˆç‡åˆ†æ
            if best_epoch > 0 and total_epochs > 0:
                efficiency = (best_epoch / total_epochs) * 100
                report += f"- **è®­ç»ƒæ•ˆç‡**: {efficiency:.1f}% (åœ¨{efficiency:.1f}%çš„è®­ç»ƒæ—¶é—´å†…è¾¾åˆ°æœ€ä½³æ€§èƒ½)\n"
            
            report += "\n"
        
        # å»ºè®®
        report += "### ğŸ’¡ å»ºè®®\n\n"
        
        if len(models) > 1:
            best_model = sorted_models[0]
            report += f"1. **æ¨èæ¨¡å‹**: {best_model['name']} (å‡†ç¡®ç‡: {best_model.get('best_val_acc', 0):.2f}%)\n"
            
            # åˆ†æè®­ç»ƒæ•ˆç‡
            efficient_models = [m for m in models if m.get('best_epoch', 0) > 0 and m.get('total_epochs', 0) > 0]
            if efficient_models:
                efficiency_scores = [(m['best_epoch'] / m['total_epochs']) for m in efficient_models]
                most_efficient_idx = np.argmin(efficiency_scores)
                most_efficient = efficient_models[most_efficient_idx]
                
                report += f"2. **è®­ç»ƒæ•ˆç‡æœ€é«˜**: {most_efficient['name']} (åœ¨ç¬¬{most_efficient['best_epoch']}è½®è¾¾åˆ°æœ€ä½³æ€§èƒ½)\n"
            
            # æ€§èƒ½å·®å¼‚åˆ†æ
            acc_range = max(accuracies) - min(accuracies)
            if acc_range < 5:
                report += "3. **æ€§èƒ½å·®å¼‚**: å„æ¨¡å‹æ€§èƒ½ç›¸è¿‘ï¼Œå¯ä»¥è€ƒè™‘é€‰æ‹©è®­ç»ƒæ•ˆç‡æ›´é«˜æˆ–å‚æ•°é‡æ›´å°‘çš„æ¨¡å‹\n"
            elif acc_range > 20:
                report += "3. **æ€§èƒ½å·®å¼‚**: å„æ¨¡å‹æ€§èƒ½å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®é€‰æ‹©æ€§èƒ½æœ€å¥½çš„æ¨¡å‹\n"
            else:
                report += "3. **æ€§èƒ½å·®å¼‚**: å„æ¨¡å‹æ€§èƒ½æœ‰ä¸€å®šå·®å¼‚ï¼Œå»ºè®®ç»¼åˆè€ƒè™‘æ€§èƒ½å’Œæ•ˆç‡\n"
        
        report += "\n### ğŸ“ å¤‡æ³¨\n\n"
        report += "- æœ¬æŠ¥å‘ŠåŸºäºè®­ç»ƒè¿‡ç¨‹ä¸­çš„éªŒè¯é›†æ€§èƒ½ç”Ÿæˆ\n"
        report += "- å®é™…éƒ¨ç½²æ—¶å»ºè®®åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¿›ä¸€æ­¥éªŒè¯\n"
        report += "- å¯ä»¥è€ƒè™‘æ¨¡å‹é›†æˆæ¥è¿›ä¸€æ­¥æå‡æ€§èƒ½\n"
        
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {save_path}")
        
        return report
    
    def compare_models(self, model_names: Optional[List[str]] = None, 
                      top_k: Optional[int] = None, 
                      generate_report: bool = False,
                      show_plots: bool = True) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ¨¡å‹å¯¹æ¯”åˆ†æ
        
        Args:
            model_names: æŒ‡å®šè¦å¯¹æ¯”çš„æ¨¡å‹åç§°
            top_k: å¯¹æ¯”æ€§èƒ½æœ€å¥½çš„kä¸ªæ¨¡å‹
            generate_report: æ˜¯å¦ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            show_plots: æ˜¯å¦æ˜¾ç¤ºå›¾è¡¨
        
        Returns:
            Dict: å¯¹æ¯”ç»“æœ
        """
        print(f"\nğŸš€ å¼€å§‹æ¨¡å‹å¯¹æ¯”åˆ†æ")
        
        # æ‰«ææ¨¡å‹
        all_models = self.scan_trained_models()
        
        if not all_models:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹")
            return {'models': [], 'comparison_table': None}
        
        # è¿‡æ»¤æ¨¡å‹
        models_to_compare = self.filter_models(all_models, model_names, top_k)
        
        if not models_to_compare:
            print("âŒ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ¨¡å‹")
            return {'models': [], 'comparison_table': None}
        
        print(f"\nğŸ“Š å¯¹æ¯” {len(models_to_compare)} ä¸ªæ¨¡å‹:")
        for model in models_to_compare:
            print(f"  - {model['name']}: {model.get('best_val_acc', 0):.2f}%")
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_table = self.create_comparison_table(models_to_compare)
        print(f"\nğŸ“‹ æ¨¡å‹å¯¹æ¯”è¡¨æ ¼:")
        print(comparison_table.to_string(index=False))
        
        # ä¿å­˜è¡¨æ ¼
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        table_path = self.output_dir / f"model_comparison_table_{timestamp}.csv"
        comparison_table.to_csv(table_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ“„ å¯¹æ¯”è¡¨æ ¼å·²ä¿å­˜: {table_path}")
        
        results = {
            'models': models_to_compare,
            'comparison_table': comparison_table,
            'table_path': str(table_path)
        }
        
        if show_plots:
            # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
            curves_path = self.output_dir / f"training_curves_{timestamp}.png"
            self.plot_training_curves(models_to_compare, str(curves_path))
            results['curves_path'] = str(curves_path)
            
            # ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”
            performance_path = self.output_dir / f"performance_comparison_{timestamp}.png"
            self.plot_performance_comparison(models_to_compare, str(performance_path))
            results['performance_path'] = str(performance_path)
        
        if generate_report:
            # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            report_path = self.output_dir / f"detailed_comparison_report_{timestamp}.md"
            report_content = self.generate_detailed_report(models_to_compare, str(report_path))
            results['report_path'] = str(report_path)
            results['report_content'] = report_content
        
        print(f"\nâœ… æ¨¡å‹å¯¹æ¯”åˆ†æå®Œæˆ")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        return results

def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description='æ¨¡å‹å¯¹æ¯”åˆ†æå·¥å…·')
    parser.add_argument('--models', nargs='+', help='æŒ‡å®šè¦å¯¹æ¯”çš„æ¨¡å‹åç§°')
    parser.add_argument('--top', type=int, help='å¯¹æ¯”æ€§èƒ½æœ€å¥½çš„kä¸ªæ¨¡å‹')
    parser.add_argument('--generate-report', action='store_true', help='ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š')
    parser.add_argument('--no-plots', action='store_true', help='ä¸æ˜¾ç¤ºå›¾è¡¨')
    parser.add_argument('--output-dir', default='reports', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    print("ğŸ§¬ BioAstæ¨¡å‹å¯¹æ¯”åˆ†æå·¥å…·")
    print("=" * 50)
    
    # åˆ›å»ºå¯¹æ¯”å™¨
    comparator = ModelComparator(output_dir=args.output_dir)
    
    # æ‰§è¡Œå¯¹æ¯”
    results = comparator.compare_models(
        model_names=args.models,
        top_k=args.top,
        generate_report=args.generate_report,
        show_plots=not args.no_plots
    )
    
    if results['models']:
        print(f"\nğŸ¯ å¯¹æ¯”ç»“æœæ‘˜è¦:")
        print(f"  å¯¹æ¯”æ¨¡å‹æ•°: {len(results['models'])}")
        print(f"  è¾“å‡ºæ–‡ä»¶:")
        
        for key, path in results.items():
            if key.endswith('_path') and path:
                print(f"    - {key}: {path}")
    else:
        print("\nâŒ æ²¡æœ‰å¯å¯¹æ¯”çš„æ¨¡å‹")
        print("\nğŸ’¡ æç¤º:")
        print("  1. ç¡®ä¿å·²ç»è®­ç»ƒäº†ä¸€äº›æ¨¡å‹")
        print("  2. æ£€æŸ¥ checkpoints/ ç›®å½•æ˜¯å¦å­˜åœ¨")
        print("  3. ä½¿ç”¨ python train_single_model.py --list_models æŸ¥çœ‹å¯ç”¨æ¨¡å‹")

if __name__ == "__main__":
    main()