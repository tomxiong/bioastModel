#!/usr/bin/env python3
"""
å•ç‹¬è®­ç»ƒæŒ‡å®šæ¨¡å‹çš„è„šæœ¬
æ”¯æŒè®­ç»ƒæ–°å¢çš„æ¨¡å‹ï¼Œè€Œä¸éœ€è¦è®­ç»ƒå…¨éƒ¨æ¨¡å‹
"""

import sys
import os
import time
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from training.dataset import BioastDataset
from training.trainer import ModelTrainer
from training.evaluator import ModelEvaluator
from training.visualizer import TrainingVisualizer

# å¯¼å…¥æ‰€æœ‰æ¨¡å‹
from models.mic_mobilenetv3 import create_mic_mobilenetv3, MODEL_CONFIG as MIC_MOBILENET_CONFIG
from models.micro_vit import create_micro_vit, MODEL_CONFIG as MICRO_VIT_CONFIG
from models.airbubble_hybrid_net import create_airbubble_hybrid_net, MODEL_CONFIG as AIRBUBBLE_CONFIG
from models.coatnet import create_coatnet, MODEL_CONFIG as COATNET_CONFIG
from models.convnext_tiny import create_convnext_tiny, MODEL_CONFIG as CONVNEXT_CONFIG
from models.efficientnet import create_efficientnet_b0, create_efficientnet_b1
from models.resnet_improved import create_resnet18_improved, create_resnet34_improved, create_resnet50_improved
from models.vit_tiny import create_vit_tiny
from models.enhanced_airbubble_detector import create_enhanced_airbubble_detector

# æ–°å¢æ¨¡å‹å¯¼å…¥
from models.efficientnet_v2 import create_efficientnetv2_s, create_efficientnetv2_m
from models.mobilenet_v3 import create_mobilenetv3_large, create_mobilenetv3_small
from models.regnet import create_regnet_x_400mf, create_regnet_y_400mf
from models.densenet import create_densenet121, create_densenet169
from models.shufflenet_v2 import create_shufflenetv2_x0_5, create_shufflenetv2_x1_0
from models.ghostnet import create_ghostnet
from models.mnasnet import create_mnasnet_1_0

class SingleModelTrainer:
    """å•ä¸ªæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model_name: str, config: Dict[str, Any]):
        self.model_name = model_name
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.num_classes = 2
        
        # è®¾ç½®é»˜è®¤é…ç½®
        self.batch_size = config.get('batch_size', 64)
        self.epochs = config.get('epochs', 10)
        self.lr = config.get('lr', 0.001)
        
        print(f"ğŸš€ åˆå§‹åŒ– {model_name} è®­ç»ƒå™¨")
        print(f"ğŸ“± è®¾å¤‡: {self.device}")
        print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {self.epochs}")
        print(f"ğŸ“ˆ å­¦ä¹ ç‡: {self.lr}")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = Path(f'checkpoints/{model_name}')
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # è®­ç»ƒå†å²
        self.train_losses = []
        self.train_accuracies = []
        self.val_losses = []
        self.val_accuracies = []
        self.best_val_acc = 0.0
        self.best_epoch = 0

    def create_model(self) -> nn.Module:
        """åˆ›å»ºæ¨¡å‹"""
        if self.model_name == 'vit_tiny':
            model = create_vit_tiny(num_classes=2, dropout_rate=0.1)
        elif self.model_name == 'micro_vit':
            model = create_micro_vit(num_classes=2)
        elif self.model_name == 'mic_mobilenetv3':
            model = create_mic_mobilenetv3(num_classes=2)
        elif self.model_name == 'airbubble_hybrid_net':
            model = create_airbubble_hybrid_net(num_classes=2)
        elif self.model_name == 'enhanced_airbubble_detector':
            model = create_enhanced_airbubble_detector(num_classes=2)
        elif self.model_name == 'efficientnet_b0':
            model = create_efficientnet_b0(num_classes=2)
        elif self.model_name == 'efficientnet_b1':
            model = create_efficientnet_b1(num_classes=2)
        elif self.model_name == 'resnet18_improved':
            model = create_resnet18_improved(num_classes=2)
        elif self.model_name == 'resnet34_improved':
            model = create_resnet34_improved(num_classes=2)
        elif self.model_name == 'resnet50_improved':
            model = create_resnet50_improved(num_classes=2)
        elif self.model_name == 'coatnet':
            model = create_coatnet(num_classes=2)
        elif self.model_name == 'convnext_tiny':
            model = create_convnext_tiny(num_classes=2)
        # æ–°å¢æ¨¡å‹ - EfficientNet V2ç³»åˆ—
        elif self.model_name == 'efficientnetv2_s':
            model = create_efficientnetv2_s(num_classes=2)
        elif self.model_name == 'efficientnetv2_m':
            model = create_efficientnetv2_m(num_classes=2)
        # æ–°å¢æ¨¡å‹ - MobileNet V3ç³»åˆ—
        elif self.model_name == 'mobilenetv3_large':
            model = create_mobilenetv3_large(num_classes=2)
        elif self.model_name == 'mobilenetv3_small':
            model = create_mobilenetv3_small(num_classes=2)
        # æ–°å¢æ¨¡å‹ - RegNetç³»åˆ—
        elif self.model_name == 'regnet_x_400mf':
            model = create_regnet_x_400mf(num_classes=2)
        elif self.model_name == 'regnet_y_400mf':
            model = create_regnet_y_400mf(num_classes=2)
        # æ–°å¢æ¨¡å‹ - DenseNetç³»åˆ—
        elif self.model_name == 'densenet121':
            model = create_densenet121(num_classes=2)
        elif self.model_name == 'densenet169':
            model = create_densenet169(num_classes=2)
        # æ–°å¢æ¨¡å‹ - è½»é‡çº§æ¨¡å‹
        elif self.model_name == 'shufflenetv2_x0_5':
            model = create_shufflenetv2_x0_5(num_classes=2)
        elif self.model_name == 'shufflenetv2_x1_0':
            model = create_shufflenetv2_x1_0(num_classes=2)
        elif self.model_name == 'ghostnet':
            model = create_ghostnet(num_classes=2)
        elif self.model_name == 'mnasnet_1_0':
            model = create_mnasnet_1_0(num_classes=2)
        else:
            raise ValueError(f"Unknown model: {self.model_name}")
            
        return model.to(self.device)

    def create_data_loaders(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        # æ•°æ®é›†è·¯å¾„
        dataset_dir = Path('bioast_dataset')
        
        if not dataset_dir.exists():
            raise FileNotFoundError("æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·ç¡®ä¿ bioast_dataset ç›®å½•å­˜åœ¨")
        
        # æ•°æ®å˜æ¢
        from torchvision import transforms
        
        train_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=10),
            transforms.ColorJitter(brightness=0.1, contrast=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        val_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = BioastDataset(
            data_dir='bioast_dataset',
            split='train',
            transform=train_transform
        )
        
        val_dataset = BioastDataset(
            data_dir='bioast_dataset',
            split='val',
            transform=val_transform
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=4,
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"ğŸ“Š éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
        
        return train_loader, val_loader

    def create_optimizer_and_scheduler(self, model):
        """åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
        optimizer = optim.AdamW(
            model.parameters(),
            lr=self.lr,
            weight_decay=0.01
        )
        
        scheduler = CosineAnnealingLR(
            optimizer,
            T_max=self.epochs,
            eta_min=1e-6
        )
        
        return optimizer, scheduler

    def train_epoch(self, model, train_loader, optimizer, criterion, epoch: int) -> tuple:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            optimizer.zero_grad()
            output = model(data)
            # Handle models that return dict (like mic_mobilenetv3)
            if isinstance(output, dict):
                output = output['classification']
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
            
            if batch_idx % 20 == 0:
                accuracy = 100. * correct / total
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}, Acc: {accuracy:.2f}%')
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100. * correct / total
        
        return epoch_loss, epoch_acc

    def validate_epoch(self, model, val_loader, criterion) -> tuple:
        """éªŒè¯ä¸€ä¸ªepoch"""
        model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = model(data)
                # Handle models that return dict (like mic_mobilenetv3)
                if isinstance(output, dict):
                    output = output['classification']
                loss = criterion(output, target)
                
                running_loss += loss.item()
                _, predicted = output.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()
        
        epoch_loss = running_loss / len(val_loader)
        epoch_acc = 100. * correct / total
        
        return epoch_loss, epoch_acc

    def save_checkpoint(self, model, optimizer, epoch: int, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_val_acc': self.best_val_acc,
            'train_losses': self.train_losses,
            'train_accuracies': self.train_accuracies,
            'val_losses': self.val_losses,
            'val_accuracies': self.val_accuracies,
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        torch.save(checkpoint, self.save_dir / 'latest.pth')
        
        # ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹
        if is_best:
            torch.save(checkpoint, self.save_dir / 'best.pth')
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")

    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history = {
            'train_losses': self.train_losses,
            'train_accuracies': self.train_accuracies,
            'val_losses': self.val_losses,
            'val_accuracies': self.val_accuracies,
            'best_val_acc': self.best_val_acc,
            'best_epoch': self.best_epoch
        }
        
        with open(self.save_dir / 'training_history.json', 'w') as f:
            json.dump(history, f, indent=2)

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {self.model_name}")
        start_time = time.time()
        
        # åˆ›å»ºæ¨¡å‹
        model = self.create_model()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader = self.create_data_loaders()
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer, scheduler = self.create_optimizer_and_scheduler(model)
        
        # æŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss()
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(1, self.epochs + 1):
            print(f"\nğŸ“… Epoch {epoch}/{self.epochs}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(model, train_loader, optimizer, criterion, epoch)
            
            # éªŒè¯
            val_loss, val_acc = self.validate_epoch(model, val_loader, criterion)
            
            # æ›´æ–°è°ƒåº¦å™¨
            scheduler.step()
            
            # è®°å½•å†å²
            self.train_losses.append(train_loss)
            self.train_accuracies.append(train_acc)
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_acc)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            is_best = val_acc > self.best_val_acc
            if is_best:
                self.best_val_acc = val_acc
                self.best_epoch = epoch
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(model, optimizer, epoch, is_best)
            
            print(f"ğŸ“Š è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%")
            print(f"ğŸ“Š éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
            print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.2f}% (Epoch {self.best_epoch})")
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()
        
        training_time = time.time() - start_time
        print(f"\nâœ… {self.model_name} è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time:.1f}ç§’")
        
        return {
            'model': self.model_name,
            'params': self.config.get('estimated_params', 0),
            'best_acc': self.best_val_acc / 100.0,  # è½¬æ¢ä¸ºå°æ•°
            'training_time': training_time
        }

def get_available_models() -> Dict[str, Dict[str, Any]]:
    """è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹é…ç½®"""
    return {
        # åŸæœ‰æ¨¡å‹
        'vit_tiny': {'estimated_params': 0.5, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        'micro_vit': MICRO_VIT_CONFIG,
        'mic_mobilenetv3': MIC_MOBILENET_CONFIG,
        'airbubble_hybrid_net': AIRBUBBLE_CONFIG,
        'enhanced_airbubble_detector': {'estimated_params': 4.0, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        'efficientnet_b0': {'estimated_params': 5.3, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        'efficientnet_b1': {'estimated_params': 7.8, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        'resnet18_improved': {'estimated_params': 11.2, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        'resnet34_improved': {'estimated_params': 21.3, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        'resnet50_improved': {'estimated_params': 23.5, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        'coatnet': COATNET_CONFIG,
        'convnext_tiny': CONVNEXT_CONFIG,
        
        # æ–°å¢æ¨¡å‹ - EfficientNet V2ç³»åˆ—
        'efficientnetv2_s': {'estimated_params': 21.5, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        'efficientnetv2_m': {'estimated_params': 54.1, 'lr': 0.001, 'batch_size': 32, 'epochs': 10},
        
        # æ–°å¢æ¨¡å‹ - MobileNet V3ç³»åˆ—
        'mobilenetv3_large': {'estimated_params': 5.4, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        'mobilenetv3_small': {'estimated_params': 2.9, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        
        # æ–°å¢æ¨¡å‹ - RegNetç³»åˆ—
        'regnet_x_400mf': {'estimated_params': 5.2, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        'regnet_y_400mf': {'estimated_params': 4.3, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        
        # æ–°å¢æ¨¡å‹ - DenseNetç³»åˆ—
        'densenet121': {'estimated_params': 8.0, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        'densenet169': {'estimated_params': 14.1, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        
        # æ–°å¢æ¨¡å‹ - è½»é‡çº§æ¨¡å‹
        'shufflenetv2_x0_5': {'estimated_params': 1.4, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        'shufflenetv2_x1_0': {'estimated_params': 2.3, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        'ghostnet': {'estimated_params': 5.2, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
        'mnasnet_1_0': {'estimated_params': 4.4, 'lr': 0.001, 'batch_size': 64, 'epochs': 10},
    }

def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒå•ä¸ªæ¨¡å‹')
    parser.add_argument('--model', type=str, help='æ¨¡å‹åç§°')
    parser.add_argument('--epochs', type=int, default=10, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--list_models', action='store_true', help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹')
    
    args = parser.parse_args()
    
    available_models = get_available_models()
    
    if args.list_models:
        print("\nğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
        print("=" * 60)
        for model_name, config in available_models.items():
            params = config.get('estimated_params', 'Unknown')
            print(f"{model_name:<25} | {params:>6}M å‚æ•°")
        return
    
    if not args.model:
        parser.error("--model is required when not using --list_models")
    
    if args.model not in available_models:
        print(f"âŒ é”™è¯¯: æ¨¡å‹ '{args.model}' ä¸å­˜åœ¨")
        print("\nğŸ“‹ å¯ç”¨æ¨¡å‹:")
        for model_name in available_models.keys():
            print(f"  - {model_name}")
        return
    
    # è·å–æ¨¡å‹é…ç½®
    config = available_models[args.model].copy()
    
    # è¦†ç›–å‘½ä»¤è¡Œå‚æ•°
    if args.epochs is not None:
        config['epochs'] = args.epochs
    if args.batch_size is not None:
        config['batch_size'] = args.batch_size
    if args.lr is not None:
        config['lr'] = args.lr
    
    print(f"\nğŸ¯ å‡†å¤‡è®­ç»ƒæ¨¡å‹: {args.model}")
    print(f"ğŸ“Š å‚æ•°é‡: {config.get('estimated_params', 'Unknown')}M")
    print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {config.get('epochs', 10)}")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {config.get('batch_size', 64)}")
    print(f"ğŸ“ˆ å­¦ä¹ ç‡: {config.get('lr', 0.001)}")
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = SingleModelTrainer(args.model, config)
    result = trainer.train()
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    result_file = f'single_model_result_{args.model}_{timestamp}.json'
    with open(result_file, 'w') as f:
        json.dump(result, f, indent=2)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")

if __name__ == "__main__":
    main()