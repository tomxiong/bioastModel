#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†ç®¡ç†å’Œæ‰¹é‡é‡è®­ç»ƒè„šæœ¬

è¿™ä¸ªè„šæœ¬ç”¨äºç®¡ç†æ•°æ®é›†çš„æ›´æ–°å’Œæ¨¡å‹çš„æ‰¹é‡é‡è®­ç»ƒã€‚

ä½¿ç”¨æ–¹æ³•:
    python dataset_manager.py --check                     # æ£€æŸ¥æ•°æ®é›†çŠ¶æ€
    python dataset_manager.py --update-dataset            # æ›´æ–°æ•°æ®é›†
    python dataset_manager.py --retrain-all               # é‡è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    python dataset_manager.py --retrain-best              # é‡è®­ç»ƒæ€§èƒ½æœ€å¥½çš„æ¨¡å‹
    python dataset_manager.py --retrain-models model1 model2  # é‡è®­ç»ƒæŒ‡å®šæ¨¡å‹
    python dataset_manager.py --schedule-retrain          # è®¡åˆ’é‡è®­ç»ƒä»»åŠ¡
"""

import os
import sys
import json
import argparse
import shutil
import hashlib
import subprocess
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

class DatasetManager:
    """
    æ•°æ®é›†ç®¡ç†å™¨
    """
    
    def __init__(self, dataset_dir: str = "data", config_file: str = "dataset_config.json"):
        self.dataset_dir = Path(dataset_dir)
        self.config_file = Path(config_file)
        self.checkpoints_dir = Path("checkpoints")
        self.backup_dir = Path("backups")
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self.dataset_dir.mkdir(exist_ok=True)
        self.backup_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“ æ•°æ®é›†ç›®å½•: {self.dataset_dir}")
        print(f"âš™ï¸ é…ç½®æ–‡ä»¶: {self.config_file}")
        print(f"ğŸ’¾ å¤‡ä»½ç›®å½•: {self.backup_dir}")
    
    def calculate_dataset_hash(self, dataset_path: Path) -> str:
        """
        è®¡ç®—æ•°æ®é›†çš„å“ˆå¸Œå€¼
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„
        
        Returns:
            str: æ•°æ®é›†å“ˆå¸Œå€¼
        """
        hash_md5 = hashlib.md5()
        
        if not dataset_path.exists():
            return ""
        
        # éå†æ‰€æœ‰æ–‡ä»¶è®¡ç®—å“ˆå¸Œ
        for file_path in sorted(dataset_path.rglob("*")):
            if file_path.is_file():
                try:
                    with open(file_path, 'rb') as f:
                        for chunk in iter(lambda: f.read(4096), b""):
                            hash_md5.update(chunk)
                    # åŒæ—¶åŒ…å«æ–‡ä»¶è·¯å¾„ä¿¡æ¯
                    hash_md5.update(str(file_path.relative_to(dataset_path)).encode())
                except Exception as e:
                    print(f"âš ï¸ è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
        
        return hash_md5.hexdigest()
    
    def load_dataset_config(self) -> Dict[str, Any]:
        """
        åŠ è½½æ•°æ®é›†é…ç½®
        
        Returns:
            Dict: æ•°æ®é›†é…ç½®
        """
        if self.config_file.exists():
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        
        # é»˜è®¤é…ç½®
        return {
            'last_update': None,
            'dataset_hash': None,
            'dataset_version': '1.0.0',
            'trained_models': {},
            'retrain_schedule': []
        }
    
    def save_dataset_config(self, config: Dict[str, Any]):
        """
        ä¿å­˜æ•°æ®é›†é…ç½®
        
        Args:
            config: æ•°æ®é›†é…ç½®
        """
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            print(f"âœ… é…ç½®å·²ä¿å­˜: {self.config_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜é…ç½®å¤±è´¥: {e}")
    
    def check_dataset_status(self) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ•°æ®é›†çŠ¶æ€
        
        Returns:
            Dict: æ•°æ®é›†çŠ¶æ€ä¿¡æ¯
        """
        print(f"\nğŸ” æ£€æŸ¥æ•°æ®é›†çŠ¶æ€...")
        
        config = self.load_dataset_config()
        current_hash = self.calculate_dataset_hash(self.dataset_dir)
        
        status = {
            'dataset_exists': self.dataset_dir.exists(),
            'dataset_path': str(self.dataset_dir),
            'current_hash': current_hash,
            'stored_hash': config.get('dataset_hash'),
            'last_update': config.get('last_update'),
            'dataset_version': config.get('dataset_version', '1.0.0'),
            'has_changes': current_hash != config.get('dataset_hash'),
            'trained_models': config.get('trained_models', {})
        }
        
        # ç»Ÿè®¡æ•°æ®é›†ä¿¡æ¯
        if self.dataset_dir.exists():
            train_dir = self.dataset_dir / "train"
            val_dir = self.dataset_dir / "val"
            test_dir = self.dataset_dir / "test"
            
            status['structure'] = {
                'train_exists': train_dir.exists(),
                'val_exists': val_dir.exists(),
                'test_exists': test_dir.exists()
            }
            
            # ç»Ÿè®¡ç±»åˆ«å’Œæ ·æœ¬æ•°
            if train_dir.exists():
                classes = [d.name for d in train_dir.iterdir() if d.is_dir()]
                status['classes'] = classes
                status['num_classes'] = len(classes)
                
                # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
                class_counts = {}
                total_samples = 0
                for class_dir in train_dir.iterdir():
                    if class_dir.is_dir():
                        count = len([f for f in class_dir.iterdir() if f.is_file()])
                        class_counts[class_dir.name] = count
                        total_samples += count
                
                status['class_counts'] = class_counts
                status['total_samples'] = total_samples
        
        # æ‰“å°çŠ¶æ€ä¿¡æ¯
        print(f"ğŸ“Š æ•°æ®é›†çŠ¶æ€æŠ¥å‘Š:")
        print(f"  æ•°æ®é›†è·¯å¾„: {status['dataset_path']}")
        print(f"  æ•°æ®é›†å­˜åœ¨: {'âœ…' if status['dataset_exists'] else 'âŒ'}")
        print(f"  å½“å‰ç‰ˆæœ¬: {status['dataset_version']}")
        print(f"  æœ€åæ›´æ–°: {status['last_update'] or 'æœªçŸ¥'}")
        print(f"  æ•°æ®å˜åŒ–: {'ğŸ”„' if status['has_changes'] else 'âœ… æ— å˜åŒ–'}")
        
        if 'classes' in status:
            print(f"  ç±»åˆ«æ•°é‡: {status['num_classes']}")
            print(f"  æ€»æ ·æœ¬æ•°: {status['total_samples']}")
            print(f"  ç±»åˆ«åˆ†å¸ƒ:")
            for class_name, count in status.get('class_counts', {}).items():
                print(f"    - {class_name}: {count} æ ·æœ¬")
        
        trained_models = status['trained_models']
        if trained_models:
            print(f"  å·²è®­ç»ƒæ¨¡å‹: {len(trained_models)} ä¸ª")
            for model_name, info in trained_models.items():
                dataset_version = info.get('dataset_version', 'æœªçŸ¥')
                needs_retrain = dataset_version != status['dataset_version']
                print(f"    - {model_name}: ç‰ˆæœ¬ {dataset_version} {'ğŸ”„ éœ€è¦é‡è®­ç»ƒ' if needs_retrain else 'âœ…'}")
        
        return status
    
    def update_dataset(self, new_dataset_path: Optional[str] = None) -> bool:
        """
        æ›´æ–°æ•°æ®é›†
        
        Args:
            new_dataset_path: æ–°æ•°æ®é›†è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼‰
        
        Returns:
            bool: æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        print(f"\nğŸ”„ æ›´æ–°æ•°æ®é›†...")
        
        config = self.load_dataset_config()
        
        # å¦‚æœæä¾›äº†æ–°æ•°æ®é›†è·¯å¾„ï¼Œå¤åˆ¶åˆ°ç›®æ ‡ä½ç½®
        if new_dataset_path:
            new_path = Path(new_dataset_path)
            if not new_path.exists():
                print(f"âŒ æ–°æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {new_dataset_path}")
                return False
            
            # å¤‡ä»½å½“å‰æ•°æ®é›†
            if self.dataset_dir.exists():
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                backup_path = self.backup_dir / f"dataset_backup_{timestamp}"
                print(f"ğŸ’¾ å¤‡ä»½å½“å‰æ•°æ®é›†åˆ°: {backup_path}")
                shutil.copytree(self.dataset_dir, backup_path)
            
            # å¤åˆ¶æ–°æ•°æ®é›†
            print(f"ğŸ“ å¤åˆ¶æ–°æ•°æ®é›†ä»: {new_dataset_path}")
            if self.dataset_dir.exists():
                shutil.rmtree(self.dataset_dir)
            shutil.copytree(new_path, self.dataset_dir)
        
        # è®¡ç®—æ–°çš„å“ˆå¸Œå€¼
        new_hash = self.calculate_dataset_hash(self.dataset_dir)
        old_hash = config.get('dataset_hash')
        
        if new_hash == old_hash:
            print(f"âœ… æ•°æ®é›†æ— å˜åŒ–")
            return True
        
        # æ›´æ–°é…ç½®
        config['dataset_hash'] = new_hash
        config['last_update'] = datetime.now().isoformat()
        
        # å¢åŠ ç‰ˆæœ¬å·
        current_version = config.get('dataset_version', '1.0.0')
        version_parts = current_version.split('.')
        if len(version_parts) >= 2:
            version_parts[1] = str(int(version_parts[1]) + 1)
            config['dataset_version'] = '.'.join(version_parts)
        else:
            config['dataset_version'] = '1.1.0'
        
        self.save_dataset_config(config)
        
        print(f"âœ… æ•°æ®é›†æ›´æ–°å®Œæˆ")
        print(f"  æ–°ç‰ˆæœ¬: {config['dataset_version']}")
        print(f"  æ–°å“ˆå¸Œ: {new_hash[:16]}...")
        
        return True
    
    def get_trained_models(self) -> List[Dict[str, Any]]:
        """
        è·å–å·²è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨
        
        Returns:
            List[Dict]: æ¨¡å‹åˆ—è¡¨
        """
        models = []
        
        if not self.checkpoints_dir.exists():
            return models
        
        for model_dir in self.checkpoints_dir.iterdir():
            if not model_dir.is_dir():
                continue
            
            model_name = model_dir.name
            best_checkpoint = model_dir / "best.pth"
            history_file = model_dir / "training_history.json"
            
            if not best_checkpoint.exists():
                continue
            
            model_info = {
                'name': model_name,
                'checkpoint_path': str(best_checkpoint),
                'model_dir': str(model_dir),
                'has_history': history_file.exists()
            }
            
            # è¯»å–è®­ç»ƒå†å²
            if history_file.exists():
                try:
                    with open(history_file, 'r', encoding='utf-8') as f:
                        history = json.load(f)
                    model_info['best_val_acc'] = history.get('best_val_acc', 0)
                    model_info['training_time'] = history.get('training_time', 0)
                except Exception:
                    model_info['best_val_acc'] = 0
                    model_info['training_time'] = 0
            else:
                model_info['best_val_acc'] = 0
                model_info['training_time'] = 0
            
            models.append(model_info)
        
        return models
    
    def retrain_model(self, model_name: str, config_path: Optional[str] = None) -> bool:
        """
        é‡è®­ç»ƒå•ä¸ªæ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
        Returns:
            bool: é‡è®­ç»ƒæ˜¯å¦æˆåŠŸ
        """
        print(f"\nğŸ”„ é‡è®­ç»ƒæ¨¡å‹: {model_name}")
        
        # æ„å»ºè®­ç»ƒå‘½ä»¤
        cmd = ["python", "train_single_model.py", "--model", model_name]
        
        if config_path:
            cmd.extend(["--config", config_path])
        
        # æ·»åŠ é‡è®­ç»ƒæ ‡å¿—
        cmd.append("--retrain")
        
        try:
            print(f"ğŸš€ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # æ‰§è¡Œè®­ç»ƒ
            result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8')
            
            # è®°å½•ç»“æŸæ—¶é—´
            end_time = time.time()
            training_time = end_time - start_time
            
            if result.returncode == 0:
                print(f"âœ… æ¨¡å‹ {model_name} é‡è®­ç»ƒæˆåŠŸ (è€—æ—¶: {training_time:.1f}ç§’)")
                
                # æ›´æ–°é…ç½®ä¸­çš„æ¨¡å‹ä¿¡æ¯
                config = self.load_dataset_config()
                if 'trained_models' not in config:
                    config['trained_models'] = {}
                
                config['trained_models'][model_name] = {
                    'dataset_version': config.get('dataset_version', '1.0.0'),
                    'retrain_time': datetime.now().isoformat(),
                    'training_duration': training_time
                }
                
                self.save_dataset_config(config)
                return True
            else:
                print(f"âŒ æ¨¡å‹ {model_name} é‡è®­ç»ƒå¤±è´¥")
                print(f"é”™è¯¯è¾“å‡º: {result.stderr}")
                return False
        
        except Exception as e:
            print(f"âŒ é‡è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def retrain_models(self, model_names: Optional[List[str]] = None, 
                      retrain_all: bool = False, 
                      retrain_best: bool = False) -> Dict[str, bool]:
        """
        æ‰¹é‡é‡è®­ç»ƒæ¨¡å‹
        
        Args:
            model_names: æŒ‡å®šè¦é‡è®­ç»ƒçš„æ¨¡å‹åç§°åˆ—è¡¨
            retrain_all: æ˜¯å¦é‡è®­ç»ƒæ‰€æœ‰æ¨¡å‹
            retrain_best: æ˜¯å¦åªé‡è®­ç»ƒæ€§èƒ½æœ€å¥½çš„æ¨¡å‹
        
        Returns:
            Dict[str, bool]: æ¯ä¸ªæ¨¡å‹çš„é‡è®­ç»ƒç»“æœ
        """
        print(f"\nğŸ”„ å¼€å§‹æ‰¹é‡é‡è®­ç»ƒ...")
        
        # è·å–è¦é‡è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨
        all_models = self.get_trained_models()
        
        if not all_models:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹")
            return {}
        
        models_to_retrain = []
        
        if retrain_all:
            models_to_retrain = all_models
            print(f"ğŸ“‹ å°†é‡è®­ç»ƒæ‰€æœ‰ {len(models_to_retrain)} ä¸ªæ¨¡å‹")
        elif retrain_best:
            # é€‰æ‹©æ€§èƒ½æœ€å¥½çš„æ¨¡å‹
            best_model = max(all_models, key=lambda x: x.get('best_val_acc', 0))
            models_to_retrain = [best_model]
            print(f"ğŸ† å°†é‡è®­ç»ƒæ€§èƒ½æœ€å¥½çš„æ¨¡å‹: {best_model['name']} (å‡†ç¡®ç‡: {best_model.get('best_val_acc', 0):.2f}%)")
        elif model_names:
            # æŒ‰åç§°è¿‡æ»¤
            model_name_set = set(model_names)
            models_to_retrain = [m for m in all_models if m['name'] in model_name_set]
            
            found_names = {m['name'] for m in models_to_retrain}
            missing_names = model_name_set - found_names
            
            if missing_names:
                print(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹: {', '.join(missing_names)}")
            
            print(f"ğŸ“‹ å°†é‡è®­ç»ƒ {len(models_to_retrain)} ä¸ªæŒ‡å®šæ¨¡å‹")
        else:
            print(f"âŒ è¯·æŒ‡å®šè¦é‡è®­ç»ƒçš„æ¨¡å‹")
            return {}
        
        # æ‰§è¡Œé‡è®­ç»ƒ
        results = {}
        total_models = len(models_to_retrain)
        
        for i, model in enumerate(models_to_retrain, 1):
            model_name = model['name']
            print(f"\nğŸ“Š è¿›åº¦: {i}/{total_models} - é‡è®­ç»ƒ {model_name}")
            
            success = self.retrain_model(model_name)
            results[model_name] = success
            
            if success:
                print(f"âœ… {model_name} é‡è®­ç»ƒæˆåŠŸ")
            else:
                print(f"âŒ {model_name} é‡è®­ç»ƒå¤±è´¥")
        
        # æ±‡æ€»ç»“æœ
        successful = sum(1 for success in results.values() if success)
        failed = total_models - successful
        
        print(f"\nğŸ“Š æ‰¹é‡é‡è®­ç»ƒå®Œæˆ:")
        print(f"  æˆåŠŸ: {successful} ä¸ª")
        print(f"  å¤±è´¥: {failed} ä¸ª")
        
        if failed > 0:
            failed_models = [name for name, success in results.items() if not success]
            print(f"  å¤±è´¥æ¨¡å‹: {', '.join(failed_models)}")
        
        return results
    
    def schedule_retrain(self, schedule_time: Optional[str] = None) -> bool:
        """
        è®¡åˆ’é‡è®­ç»ƒä»»åŠ¡
        
        Args:
            schedule_time: è®¡åˆ’æ‰§è¡Œæ—¶é—´ (æ ¼å¼: YYYY-MM-DD HH:MM)
        
        Returns:
            bool: è®¡åˆ’æ˜¯å¦æˆåŠŸ
        """
        print(f"\nâ° è®¡åˆ’é‡è®­ç»ƒä»»åŠ¡...")
        
        if not schedule_time:
            # é»˜è®¤è®¡åˆ’åœ¨æ˜å¤©åŒä¸€æ—¶é—´
            schedule_time = (datetime.now() + timedelta(days=1)).strftime('%Y-%m-%d %H:%M')
        
        try:
            scheduled_datetime = datetime.strptime(schedule_time, '%Y-%m-%d %H:%M')
        except ValueError:
            print(f"âŒ æ—¶é—´æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD HH:MM æ ¼å¼")
            return False
        
        if scheduled_datetime <= datetime.now():
            print(f"âŒ è®¡åˆ’æ—¶é—´å¿…é¡»åœ¨æœªæ¥")
            return False
        
        config = self.load_dataset_config()
        if 'retrain_schedule' not in config:
            config['retrain_schedule'] = []
        
        # æ·»åŠ è®¡åˆ’ä»»åŠ¡
        task = {
            'scheduled_time': scheduled_datetime.isoformat(),
            'created_time': datetime.now().isoformat(),
            'status': 'pending',
            'task_type': 'retrain_all'
        }
        
        config['retrain_schedule'].append(task)
        self.save_dataset_config(config)
        
        print(f"âœ… é‡è®­ç»ƒä»»åŠ¡å·²è®¡åˆ’")
        print(f"  æ‰§è¡Œæ—¶é—´: {schedule_time}")
        print(f"  ä»»åŠ¡ç±»å‹: é‡è®­ç»ƒæ‰€æœ‰æ¨¡å‹")
        
        return True

def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description='æ•°æ®é›†ç®¡ç†å’Œæ‰¹é‡é‡è®­ç»ƒå·¥å…·')
    parser.add_argument('--check', action='store_true', help='æ£€æŸ¥æ•°æ®é›†çŠ¶æ€')
    parser.add_argument('--update-dataset', help='æ›´æ–°æ•°æ®é›†ï¼ˆå¯é€‰ï¼šæŒ‡å®šæ–°æ•°æ®é›†è·¯å¾„ï¼‰')
    parser.add_argument('--retrain-all', action='store_true', help='é‡è®­ç»ƒæ‰€æœ‰æ¨¡å‹')
    parser.add_argument('--retrain-best', action='store_true', help='é‡è®­ç»ƒæ€§èƒ½æœ€å¥½çš„æ¨¡å‹')
    parser.add_argument('--retrain-models', nargs='+', help='é‡è®­ç»ƒæŒ‡å®šæ¨¡å‹')
    parser.add_argument('--schedule-retrain', help='è®¡åˆ’é‡è®­ç»ƒä»»åŠ¡ (æ ¼å¼: YYYY-MM-DD HH:MM)')
    parser.add_argument('--dataset-dir', default='data', help='æ•°æ®é›†ç›®å½•')
    parser.add_argument('--config-file', default='dataset_config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    print("ğŸ§¬ BioAstæ•°æ®é›†ç®¡ç†å·¥å…·")
    print("=" * 50)
    
    # åˆ›å»ºæ•°æ®é›†ç®¡ç†å™¨
    manager = DatasetManager(
        dataset_dir=args.dataset_dir,
        config_file=args.config_file
    )
    
    # æ‰§è¡Œç›¸åº”æ“ä½œ
    if args.check:
        status = manager.check_dataset_status()
        
    elif args.update_dataset is not None:
        # å¦‚æœæä¾›äº†è·¯å¾„å‚æ•°ï¼Œä½¿ç”¨è¯¥è·¯å¾„ï¼›å¦åˆ™åªæ›´æ–°é…ç½®
        dataset_path = args.update_dataset if args.update_dataset else None
        success = manager.update_dataset(dataset_path)
        if success:
            print(f"\nğŸ’¡ æç¤º: æ•°æ®é›†å·²æ›´æ–°ï¼Œå»ºè®®é‡è®­ç»ƒæ¨¡å‹ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
    
    elif args.retrain_all:
        results = manager.retrain_models(retrain_all=True)
        
    elif args.retrain_best:
        results = manager.retrain_models(retrain_best=True)
        
    elif args.retrain_models:
        results = manager.retrain_models(model_names=args.retrain_models)
        
    elif args.schedule_retrain:
        success = manager.schedule_retrain(args.schedule_retrain)
        
    else:
        # é»˜è®¤æ£€æŸ¥çŠ¶æ€
        print("\nğŸ’¡ æ²¡æœ‰æŒ‡å®šæ“ä½œï¼Œæ‰§è¡ŒçŠ¶æ€æ£€æŸ¥...")
        status = manager.check_dataset_status()
        
        print(f"\nğŸ”§ å¯ç”¨æ“ä½œ:")
        print(f"  --check                    æ£€æŸ¥æ•°æ®é›†çŠ¶æ€")
        print(f"  --update-dataset [path]    æ›´æ–°æ•°æ®é›†")
        print(f"  --retrain-all              é‡è®­ç»ƒæ‰€æœ‰æ¨¡å‹")
        print(f"  --retrain-best             é‡è®­ç»ƒæœ€ä½³æ¨¡å‹")
        print(f"  --retrain-models m1 m2     é‡è®­ç»ƒæŒ‡å®šæ¨¡å‹")
        print(f"  --schedule-retrain time    è®¡åˆ’é‡è®­ç»ƒä»»åŠ¡")

if __name__ == "__main__":
    main()